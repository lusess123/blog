## 20240302 BREAKING: ELON MUSK OPEN AI BOMBSHELL "AGI Achieved Internally", Q-STAR, Lawsuit to DISSOLVE OpenAI
[https://www.youtube.com/watch?v=jPuLfomqS1Q](https://www.youtube.com/watch?v=jPuLfomqS1Q)

![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709385582438-99013e46-58c5-4697-a47b-ef0477a0a3fe.png#averageHue=%23efa79a&clientId=ue4390975-0be6-4&from=paste&height=377&id=u5924e6c8&originHeight=754&originWidth=1556&originalType=binary&ratio=2&rotation=0&showTitle=false&size=219838&status=done&style=none&taskId=ud239cd7f-a2a9-4c4f-9f90-e742f1c2be7&title=&width=778)

- 2000-2010年的早期，一个叫做深度学习的旧算法因为硬件的进步而变得实用。这导致了一场几乎一夜之间的革命，所以这种AI，我们今天谈论的通用目的AI，就是这样开始的。
- 这是从单一目的AI向更多学习型AI、通用目的AI的转变。它之所以不同，是因为这种AI通过训练实例学习每个任务，本质上是自我编程。
- Musk看到AGI的存在性威胁的地方，其他人却将AGI视为利润和权力的来源
- 法庭文件谈到了Q*，OpenAI正在开发一个秘密算法，称为Q*。这是从OpenAI泄露出来的，似乎已经被Sam Altman确认
- OpenAI最初的使命是一个非营利AI实验室的提议，其目标是赶上Google在AGI的竞赛中，但采取不同的方法和使命
- GPT-4的内部设计被保密，除了OpenAI，据信还有微软知道
- GPT-4可视为接近AGI的原型或前身
- GPT的一个限制是它不能回溯，一旦它们开始生成回应，即使结果是一个错误，它们也会继续下去
- 如果这种商业模式有效，它将彻底重新定义风险投资的实践方式；这将允许聪明的投资者基本上使用非营利组织的税前捐赠在该非营利结构下开发某物，然后通过与追求利润最大化的企业伙伴合作来丰富自己
- 马斯克要求美国法院判定Q star 是否是agi
# 20240301 The Illustrated Transformer
[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709296600614-7328ede1-2256-4768-bc14-56c024396f14.png#averageHue=%23fefefd&clientId=udd499d5e-4b0a-4&from=paste&height=402&id=ub0929a81&originHeight=804&originWidth=1436&originalType=binary&ratio=2&rotation=0&showTitle=false&size=145244&status=done&style=none&taskId=ueb788713-fb31-4771-a91b-eaa8ec8d622&title=&width=718)

- 在训练阶段，Transformer 架构通过自注意力机制（Self-Attention Mechanism）允许模型有效地处理序列数据
- 在推理阶段，Transformer 同样展现出其优势。由于其并行处理能力，Transformer 能够快速生成响应或完成任务，这对于需要实时响应的应用来说非常关键。此外，Transformer 的可扩展性也使得它能够通过增加模型的大小来提升性能，这是通过增加层数（depth）或者模型宽度（width）来实现的。
- encoder在结构上完全相同（但它们不共享权重）
- 前馈神经网络（FFNN）在处理序列中每个位置的输出时是独立的，并且对所有位置应用的是相同的网络结构和参数，这使得这一处理过程非常适合并行化
- 编码器的输入首先流经自注意力层，该层可帮助编码器在编码特定单词时查看输入句子中的其他单词
- 解码器具有这两个层，但在它们之间是一个注意力层，可帮助解码器专注于输入句子的相关部分
- Transformer 的一个关键属性，即每个位置的字都流经编码器中自己的路径
- 残差连接（Residual Connection）和层归一化（Layer-Normalization Step）是深度学习中用于改善网络训练效率和效果的两种技术。
- “编码器-解码器注意”层的工作方式与多头自注意力类似，不同之处在于它从其下面的层创建其查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。

