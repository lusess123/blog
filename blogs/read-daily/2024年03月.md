# 20240301 The Illustrated Transformer
[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709296600614-7328ede1-2256-4768-bc14-56c024396f14.png#averageHue=%23fefefd&clientId=udd499d5e-4b0a-4&from=paste&height=402&id=ub0929a81&originHeight=804&originWidth=1436&originalType=binary&ratio=2&rotation=0&showTitle=false&size=145244&status=done&style=none&taskId=ueb788713-fb31-4771-a91b-eaa8ec8d622&title=&width=718)

- 在训练阶段，Transformer 架构通过自注意力机制（Self-Attention Mechanism）允许模型有效地处理序列数据
- 在推理阶段，Transformer 同样展现出其优势。由于其并行处理能力，Transformer 能够快速生成响应或完成任务，这对于需要实时响应的应用来说非常关键。此外，Transformer 的可扩展性也使得它能够通过增加模型的大小来提升性能，这是通过增加层数（depth）或者模型宽度（width）来实现的。
- encoder在结构上完全相同（但它们不共享权重）
- 前馈神经网络（FFNN）在处理序列中每个位置的输出时是独立的，并且对所有位置应用的是相同的网络结构和参数，这使得这一处理过程非常适合并行化
- 编码器的输入首先流经自注意力层，该层可帮助编码器在编码特定单词时查看输入句子中的其他单词
- 解码器具有这两个层，但在它们之间是一个注意力层，可帮助解码器专注于输入句子的相关部分
- Transformer 的一个关键属性，即每个位置的字都流经编码器中自己的路径
- 残差连接（Residual Connection）和层归一化（Layer-Normalization Step）是深度学习中用于改善网络训练效率和效果的两种技术。
- “编码器-解码器注意”层的工作方式与多头自注意力类似，不同之处在于它从其下面的层创建其查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。

