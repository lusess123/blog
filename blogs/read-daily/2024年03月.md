# 20240311《Elon Musk》35 Marrying Talulah
September 2010
- “我可以走一条艰难的道路”
   - “the child within the man.”
- 东方快递
   - Riley喜欢举办有创意的派对，而Musk，尽管在社交上有些笨拙（或许正因为如此），对它们却有一种奇怪的热情。
   - Musk并没有很多稳定而踏实的关系，他的生活中也没有很多稳定而踏实的时期
- 在Musk为数不多的这种关系中，他与Riley的关系是其中之一，他与她共度的岁月——从他们2008年的相遇到他们2016年的第二次离婚——最终成为他一生中相对稳定的最长时期。



# 20240310 OpenAI is INVESTIGATED by Law Firm. The TRUTH about the Board's Firing of Sam Altman is Revealed...
[https://www.youtube.com/watch?v=Y1_EPch_MKw](https://www.youtube.com/watch?v=Y1_EPch_MKw)

- immy apples，可能是Sam Alman的替代账户，他的匿名Twitter账户
- 国际律师事务所WilmerHale调查了2023年11月和12月Sam Altman和Greg Brockman被移除出OpenAI董事会和Sam Altman作为CEO被终止职务的事件
- 事件的根本原因是人际关系和信任的失衡，而非技术或财务问题
- Anthropic的聊天机器人，“Claw”，它被故意延迟以避免推进AI能力的发展速度
- Toner过去曾将自己描述为一名有效的利他主义者
- 有效利他主义（Effective Altruism，简称EA）是一种哲学和社会运动，它强调使用证据和理性来确定如何使用资源以最有效地帮助他人。有效利他主义者认为，在做善事时，不仅要出于好意，更要确保行动的效率和效果，即用有限的资源实现最大的正面影响。
- NIST的工作人员因预期将有效利他主义AI研究员任命到美国AI安全研究所而反抗
- EA，有效利他主义，你知道，它被定义为一个使用证据和理性来确定如何尽可能地帮助他人的智力项目，但它变成了一个类似邪教的高度影响力和富有的追随者团体

# 20240309《Life of PI》Toronto and Pondicherry 5

- 的确，我们所遇之人能够改变我们，有时这种改变如此深刻，以至于我们之后的自我与从前大不相同，连名字也随之改变。举例而言，Simon被称作Peter，Matthew亦名Levi，Nathanael即是Bartholomew，那个不是Iscariot的Judas，改名为Thaddeus，Simeon有时被称为Niger，Saul转而为Paul
- 这似乎是人类自然法则，那些住在海边的人对游泳者持怀疑态度，就像那些住在山区的人对登山者持怀疑态度一样
- 重复不仅对动物的训练很重要，对人类也是如此

# 20240208 The Internet Goes EXTINCT as Gen AI Takes Over | The Dark Forest Internet & Proving Your "Humanness"
[https://www.youtube.com/watch?v=3NN5L-f0cDo&t=1s](https://www.youtube.com/watch?v=3NN5L-f0cDo&t=1s)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709905849832-103eeb98-94ec-4952-86c3-088ccd1b4844.png#averageHue=%23fcfcfc&clientId=u2e46fc49-c8e6-4&from=paste&height=1045&id=u6085b9f5&originHeight=2090&originWidth=1800&originalType=binary&ratio=2&rotation=0&showTitle=false&size=2099912&status=done&style=none&taskId=ub28d88b8-da75-4a0d-95c2-86ab3131ccc&title=&width=900)

- 扩展的暗森林： 生成式AI证明你是网上充斥着生成式AI内容的人类
- 互联网优化已经导致了信息的泛滥，而AI技术的进一步发展可能会加剧这一问题。
- “死互联网理论”，该理论认为大部分的互联网内容是由AI生成的，能够通过人类难以区分机器与人类的反向图灵测试
- 反向图灵测试呢？图灵测试是这样一个想法，即在什么时候人工智能会变得足够智能和高级，以至于能欺骗我们认为它是人类。
- 网络开放性和机器人是无法避免的，几乎没有办法防御这种情况，无论你的防御多么彻底。
- 不过有一种方式真的很有效，那就是收取支付
- Sybil攻击是一种网络安全威胁，其中一个攻击者在网络或系统中创建大量虚假身份（被称为Sybil节点），以试图控制或破坏该网络的功能。这种攻击的名称来自于弗洛拉·瑞塔·谢伯的书《Sybil》，讲述了一个因多重人格障碍而拥有多个身份的女子的故事，象征着一个实体拥有多个虚假身份。
- 一种有效的方法来验证网络用户的人类身份，而不泄露过多的个人信息，以此来增强网络安全和减少虚假信息的传播。"人性证明"被提出作为一种潜在的解决方案，但其实施可能涉及生物识别数据的收集和处理
- 如何在缺少政府颁发的身份证明或中央化身份管理系统的情况下验证一个人的真实性。这是寻找能够提高网络安全和用户隐私保护的验证方法的动因
- UBI 是“无条件基本收入”（Universal Basic Income）的缩写。这是一种社会保障制度，旨在为所有（或几乎所有）公民或居民提供一定数量的无条件的金钱支付，无论个人的其他收入如何，都不需要工作要求或其他条件。UBI 的目的是减少贫困，提高生活标准，促进社会公正和经济平等。它也被看作是对应对自动化、人工智能发展和未来可能导致的工作岗位减少的一种解决方案。
# 20240207 华语母语者最易犯的错误及破解之道
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709823920522-6f858513-ea98-40d7-bf34-f86341b2185b.png#averageHue=%23e9e9e9&clientId=u4542507b-314f-4&from=paste&height=329&id=u04bcc97a&originHeight=658&originWidth=1142&originalType=binary&ratio=2&rotation=0&showTitle=false&size=230596&status=done&style=none&taskId=ued3db9d7-182c-4788-a57c-e25f3725ba4&title=&width=571)
[https://www.youtube.com/watch?v=iZ60WZlZHE4&list=WL&index=2](https://www.youtube.com/watch?v=iZ60WZlZHE4&list=WL&index=2)

- Dark L 和 Light L 音
   - Light L 放在开头，比如 Light, Little , Lead 跟中文的l发音很像
   - Dark L 出现在中间或者末尾，Gril, World, Real,Milk ,Local, 有点模糊的卷起来的声音
- th 音
   - 清辅音，声带不振动，比如， Three , Thought,
   - 浊辅音，声音要带振动，比图， This, That
- Ng
   - Sing, king 鼻腔共鸣
   - n 
- V 
   - 门牙露出来，轻轻放在下嘴唇
- I
- B , D ,G , K , P, T 结尾的词，辅音结尾要短
- 元音
   1. **发音位置和方式**：元音主要通过调整舌位和嘴型来改变发音，而辅音则涉及到气流在口腔或咽喉中被阻碍的具体部位，如唇部、舌尖、喉咙等。
   2. **声音的持续性和清晰度**：元音通常能够持续发音，并且声音较为清晰；辅音的发音则比较短暂，有些辅音（如爆破音）发音非常迅速。
   3. **在单词中的作用**：元音在单词中充当核心音节，是构成音节的必要元素；而辅音主要在词首、词中、词尾位置，起到与元音搭配构词的作用。
# 20240207 Claude 3 "Self-Portrait" Goes Viral | Beats GPT-4 Benchmarks | Why does it appears SELF-AWARE?
[https://www.youtube.com/watch?v=SsbCuWe7WRs](https://www.youtube.com/watch?v=SsbCuWe7WRs)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709812914767-d2e6431a-c7f5-4504-a609-25a0d19131c1.png#averageHue=%23d6795a&clientId=ue70f8b23-bb1a-4&from=paste&height=810&id=u4142833a&originHeight=1620&originWidth=2880&originalType=binary&ratio=2&rotation=0&showTitle=false&size=2617966&status=done&style=none&taskId=u7202e686-9169-4886-adde-078c45e68f8&title=&width=1440)

- 推理能力通常来自于结合两种模型不太可能有训练数据的不同领域。关于GPT-4的这篇研究论文称为“'Sparks of egi'（egi的火花）”
- 大象的预训练更专门化，更适合它们特定的生态位
- 当你要求它们融合两个概念以产生一种新概念或新想法时，假设这个新想法不在它们的训练数据中，一般来说，模型的质量越好，它就越能合成那种想法
- 询问模型是否能够理解两个概念并能够做出超出其训练数据的推理，这是测试这些模型的通用能力的最佳方法之一
- Cloud 3 Opus能够在大量文本中找到一个非常特定的小片段，这是以前未曾见过的行为
- 大模型的"meta-awareness"（元认知能力）是指模型能够对自己的知识、思维过程以及任务的性质有所认识和理解的能力
- TF-IDF是一种用于信息检索和文本挖掘的常用权重计算方法，用于评估一个词语对于一个文件集或一个语料库中的其中一份文件的重要性。
- 所有视觉模型都害怕的东西开始，那就是Costco收据
- 数苹果是一件这些模型做得不好的事情
# 20240206 OpenAI Insider Talks About the Future of AGI + Scaling Laws of Neural Nets
[https://www.youtube.com/watch?v=zeju5OXAVzk&t=53s](https://www.youtube.com/watch?v=zeju5OXAVzk&t=53s)

- 第一个神经网络在50年代被创建，而现代神经网络只是更深，意味着它们包含更多层
- 今天大多数主要的AI技术都源于1950年代的基础研究，结合了一些较小的工程解决方案，如反向传播和Transformer模型
- 最近AI能力的爆炸式增长只有两个原因：规模和数据
- 卡马克（Minecraft的创建者）认为AGI的技术难题在很大程度上已经被解决
- 解决AGI的关键点可能非常简洁，足以被概括在很小的空间内，很多可能被隐藏在过去几十年的各种文本和教科书中
- 人类大脑中有多少参数，突触或参数呢，通常引用的数字是100万亿，超过了银河系中的恒星数量。ChatGPT-4的参数量是1.8万亿个参数，gpt3.5是1750亿。猫有2500亿个突触，狗有5300亿个突触。突触数量通常似乎预示着更高的智力。
- 大象的数量比人类多，但表现出更低的智力，他在某种程度上解释说，数据的质量可能是这些例外的答案。
- AGI 有多少参数量呢，最大可能是1000万亿
- 一个量子计算机的专家正在OpenAI工作，涉及AI的安全性和对齐问题

# 20240305 《西藏度亡经》正文

- 初期中阴明光
- 续发中阴明光
- 实相中阴
   - 业影即时闪现
   - 中阴的6种境界：
      - 本然中阴
      - 梦境中阴
      - 等待中阴
      - 临终中阴
      - 实相中阴（目前处于这个阶段）
         - 喜乐部和持明部诸尊
         - 忿怒部饮血诸尊
      - 投生中阴
# 20240304《Life of PI》Toronto and Pondicherry 4
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709540099056-e8c3fad4-5284-4706-8165-520d160bc034.png#averageHue=%23b3bc76&clientId=uee211344-c369-4&from=paste&height=749&id=u016bb4c4&originHeight=1498&originWidth=1124&originalType=binary&ratio=2&rotation=0&showTitle=false&size=2756681&status=done&style=none&taskId=uec4ac15b-bf4b-44a3-8a74-ee190be838f&title=&width=562)

- But language founders in such seas. Better to picture it in your head if you want to feel it.（但语言在这样的海洋中失去了方向，如果你想感受它，最好在你的脑海中描绘它）
- 一个常见的误解，即野生动物自由即意味着它们快乐
- 实际上在野外，野生动物既不在空间上自由，也不在时间上自由，甚至在个人关系上也不自由
- 动物就是这样，保守，甚至可以说是反动的。微小的变化都可能使它们不安。它们希望事物每天、每月都保持原样。惊喜对它们来说是极其不愉快的。
- 我们不是说：“没有比家更好的地方”吗？这确实是动物所感受到的。动物是有领地意识的。这是了解它们思维的关键。只有熟悉的领地才能让它们实现野外的两个不懈的要求：避开敌人和获取食物和水。
- 甚至有人会主张，如果动物能够以智慧进行选择，它们会选择住在动物园，因为动物园和野外的主要区别在于前者缺乏寄生虫和敌人，食物丰富，而后者则分别缺乏和丰富。
- 动物园不再受人们欢迎。宗教面临同样的问题。它们都被关于自由的某些幻觉所困扰

![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709565068149-2a6ae744-5b18-4f30-ab85-e2069abf482b.png#averageHue=%233f4935&clientId=uf296e90d-dd84-4&from=paste&id=qGiQa&originHeight=1024&originWidth=1024&originalType=binary&ratio=2&rotation=0&showTitle=false&size=1027279&status=done&style=none&taskId=u48c7ddec-3a17-4243-a29f-9c3cdba9524&title=)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709564980951-92e95603-74b6-4ff1-896b-80e151868cae.png#averageHue=%236b8154&clientId=u652d6710-3193-4&from=paste&id=dygRB&originHeight=1024&originWidth=1024&originalType=binary&ratio=2&rotation=0&showTitle=false&size=1084453&status=done&style=none&taskId=u03de54cb-8c4e-48dd-91cc-8dfdca2352e&title=)

# 20240303 Vercel AI SDK
[https://sdk.vercel.ai/docs](https://sdk.vercel.ai/docs)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709469264276-ad07db95-f3c5-45ef-a344-c2973d188710.png#averageHue=%23999999&clientId=u1015d0b0-315e-4&from=paste&height=208&id=u9d131193&originHeight=416&originWidth=800&originalType=binary&ratio=2&rotation=0&showTitle=false&size=144687&status=done&style=none&taskId=u25ab698e-908d-44cc-9613-ca8fa19078b&title=&width=400)

- Vercel AI SDK ：用于构建 AI 驱动的用户界面的开源库
- 基础概念
   - Streaming（流）：流式响应可以在响应变得可用时传输部分响应而不是完整响应
   - 和 Cancellation（取消）
      - Stream Back-pressure （流背压），描述了生产者（producer）产生数据的速率高于消费者（consumer）处理数据的速率时发生的情况。在这种情况下，如果不加以控制，数据可以积累起来，导致内存使用增加，甚至系统崩溃。
      - 使用懒加载可以做到生产和消费一致，比如 生成器的 generato（生成器）r 的 next
   - 提示工程 [https://www.promptingguide.ai/zh](https://www.promptingguide.ai/zh)
   - 响应缓存
   - 工具
   - 生产式UI
      - AIState 和 UIState
      -  嵌套UI流式处理
#  20240303 China's Humanoid Robot JUST SHATTERED World Record! $90,000 Model Begins Deployment Across US and EU
[https://www.youtube.com/watch?v=WWAnJX889j0&t=365s](https://www.youtube.com/watch?v=WWAnJX889j0&t=365s)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709460433072-d8051e0d-4a56-418a-a4c9-9f3a70149097.png#averageHue=%23303a3e&clientId=udd287200-2229-4&from=paste&height=683&id=u7a57cd51&originHeight=1366&originWidth=1568&originalType=binary&ratio=2&rotation=0&showTitle=false&size=235912&status=done&style=none&taskId=ub7b2a858-52c3-4850-b3ad-f641b7dd44d&title=&width=784)

- 人形机器人设定了一个新的世界纪录，它的最大速度达到了每秒3.3米（大约10.8英尺），超过了人类慢跑的速度
- 人形9万美元的机器人H1，在美国，这个价格与高端皮卡车相当
- 四足机器人B2 1600美元
# 20240302 BREAKING: ELON MUSK OPEN AI BOMBSHELL "AGI Achieved Internally", Q-STAR, Lawsuit to DISSOLVE OpenAI
[https://www.youtube.com/watch?v=jPuLfomqS1Q](https://www.youtube.com/watch?v=jPuLfomqS1Q)

![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709385582438-99013e46-58c5-4697-a47b-ef0477a0a3fe.png#averageHue=%23efa79a&clientId=ue4390975-0be6-4&from=paste&height=377&id=u5924e6c8&originHeight=754&originWidth=1556&originalType=binary&ratio=2&rotation=0&showTitle=false&size=219838&status=done&style=none&taskId=ud239cd7f-a2a9-4c4f-9f90-e742f1c2be7&title=&width=778)

- 2000-2010年的早期，一个叫做深度学习的旧算法因为硬件的进步而变得实用。这导致了一场几乎一夜之间的革命，所以这种AI，我们今天谈论的通用目的AI，就是这样开始的。
- 这是从单一目的AI向更多学习型AI、通用目的AI的转变。它之所以不同，是因为这种AI通过训练实例学习每个任务，本质上是自我编程。
- Musk看到AGI的存在性威胁的地方，其他人却将AGI视为利润和权力的来源
- 法庭文件谈到了Q*，OpenAI正在开发一个秘密算法，称为Q*。这是从OpenAI泄露出来的，似乎已经被Sam Altman确认
- OpenAI最初的使命是一个非营利AI实验室的提议，其目标是赶上Google在AGI的竞赛中，但采取不同的方法和使命
- GPT-4的内部设计被保密，除了OpenAI，据信还有微软知道
- GPT-4可视为接近AGI的原型或前身
- GPT的一个限制是它不能回溯，一旦它们开始生成回应，即使结果是一个错误，它们也会继续下去
- 如果这种商业模式有效，它将彻底重新定义风险投资的实践方式；这将允许聪明的投资者基本上使用非营利组织的税前捐赠在该非营利结构下开发某物，然后通过与追求利润最大化的企业伙伴合作来丰富自己
- 马斯克要求美国法院判定Q star 是否是agi
# 20240301 The Illustrated Transformer
[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709296600614-7328ede1-2256-4768-bc14-56c024396f14.png#averageHue=%23fefefd&clientId=udd499d5e-4b0a-4&from=paste&height=402&id=ub0929a81&originHeight=804&originWidth=1436&originalType=binary&ratio=2&rotation=0&showTitle=false&size=145244&status=done&style=none&taskId=ueb788713-fb31-4771-a91b-eaa8ec8d622&title=&width=718)

- 在训练阶段，Transformer 架构通过自注意力机制（Self-Attention Mechanism）允许模型有效地处理序列数据
- 在推理阶段，Transformer 同样展现出其优势。由于其并行处理能力，Transformer 能够快速生成响应或完成任务，这对于需要实时响应的应用来说非常关键。此外，Transformer 的可扩展性也使得它能够通过增加模型的大小来提升性能，这是通过增加层数（depth）或者模型宽度（width）来实现的。
- encoder在结构上完全相同（但它们不共享权重）
- 前馈神经网络（FFNN）在处理序列中每个位置的输出时是独立的，并且对所有位置应用的是相同的网络结构和参数，这使得这一处理过程非常适合并行化
- 编码器的输入首先流经自注意力层，该层可帮助编码器在编码特定单词时查看输入句子中的其他单词
- 解码器具有这两个层，但在它们之间是一个注意力层，可帮助解码器专注于输入句子的相关部分
- Transformer 的一个关键属性，即每个位置的字都流经编码器中自己的路径
- 残差连接（Residual Connection）和层归一化（Layer-Normalization Step）是深度学习中用于改善网络训练效率和效果的两种技术。
- “编码器-解码器注意”层的工作方式与多头自注意力类似，不同之处在于它从其下面的层创建其查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。

