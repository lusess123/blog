# 20240303《Life of PI》Toronto and Pondicherry 4
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709540099056-e8c3fad4-5284-4706-8165-520d160bc034.png#averageHue=%23b3bc76&clientId=uee211344-c369-4&from=paste&height=749&id=u016bb4c4&originHeight=1498&originWidth=1124&originalType=binary&ratio=2&rotation=0&showTitle=false&size=2756681&status=done&style=none&taskId=uec4ac15b-bf4b-44a3-8a74-ee190be838f&title=&width=562)

- But language founders in such seas. Better to picture it in your head if you want to feel it.（但语言在这样的海洋中失去了方向，如果你想感受它，最好在你的脑海中描绘它）
- 一个常见的误解，即野生动物自由即意味着它们快乐
- 实际上在野外，野生动物既不在空间上自由，也不在时间上自由，甚至在个人关系上也不自由
- 动物就是这样，保守，甚至可以说是反动的。微小的变化都可能使它们不安。它们希望事物每天、每月都保持原样。惊喜对它们来说是极其不愉快的。
- 我们不是说：“没有比家更好的地方”吗？这确实是动物所感受到的。动物是有领地意识的。这是了解它们思维的关键。只有熟悉的领地才能让它们实现野外的两个不懈的要求：避开敌人和获取食物和水。
- 甚至有人会主张，如果动物能够以智慧进行选择，它们会选择住在动物园，因为动物园和野外的主要区别在于前者缺乏寄生虫和敌人，食物丰富，而后者则分别缺乏和丰富。
- 动物园不再受人们欢迎。宗教面临同样的问题。它们都被关于自由的某些幻觉所困扰
# 20240303 Vercel AI SDK
[https://sdk.vercel.ai/docs](https://sdk.vercel.ai/docs)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709469264276-ad07db95-f3c5-45ef-a344-c2973d188710.png#averageHue=%23999999&clientId=u1015d0b0-315e-4&from=paste&height=208&id=u9d131193&originHeight=416&originWidth=800&originalType=binary&ratio=2&rotation=0&showTitle=false&size=144687&status=done&style=none&taskId=u25ab698e-908d-44cc-9613-ca8fa19078b&title=&width=400)

- Vercel AI SDK ：用于构建 AI 驱动的用户界面的开源库
- 基础概念
   - Streaming（流）：流式响应可以在响应变得可用时传输部分响应而不是完整响应
   - 和 Cancellation（取消）
      - Stream Back-pressure （流背压），描述了生产者（producer）产生数据的速率高于消费者（consumer）处理数据的速率时发生的情况。在这种情况下，如果不加以控制，数据可以积累起来，导致内存使用增加，甚至系统崩溃。
      - 使用懒加载可以做到生产和消费一致，比如 生成器的 generato（生成器）r 的 next
   - 提示工程 [https://www.promptingguide.ai/zh](https://www.promptingguide.ai/zh)
   - 响应缓存
   - 工具
   - 生产式UI
      - AIState 和 UIState
      -  嵌套UI流式处理
#  20240303 China's Humanoid Robot JUST SHATTERED World Record! $90,000 Model Begins Deployment Across US and EU
[https://www.youtube.com/watch?v=WWAnJX889j0&t=365s](https://www.youtube.com/watch?v=WWAnJX889j0&t=365s)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709460433072-d8051e0d-4a56-418a-a4c9-9f3a70149097.png#averageHue=%23303a3e&clientId=udd287200-2229-4&from=paste&height=683&id=u7a57cd51&originHeight=1366&originWidth=1568&originalType=binary&ratio=2&rotation=0&showTitle=false&size=235912&status=done&style=none&taskId=ub7b2a858-52c3-4850-b3ad-f641b7dd44d&title=&width=784)

- 人形机器人设定了一个新的世界纪录，它的最大速度达到了每秒3.3米（大约10.8英尺），超过了人类慢跑的速度
- 人形9万美元的机器人H1，在美国，这个价格与高端皮卡车相当
- 四足机器人B2 1600美元
# 20240302 BREAKING: ELON MUSK OPEN AI BOMBSHELL "AGI Achieved Internally", Q-STAR, Lawsuit to DISSOLVE OpenAI
[https://www.youtube.com/watch?v=jPuLfomqS1Q](https://www.youtube.com/watch?v=jPuLfomqS1Q)

![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709385582438-99013e46-58c5-4697-a47b-ef0477a0a3fe.png#averageHue=%23efa79a&clientId=ue4390975-0be6-4&from=paste&height=377&id=u5924e6c8&originHeight=754&originWidth=1556&originalType=binary&ratio=2&rotation=0&showTitle=false&size=219838&status=done&style=none&taskId=ud239cd7f-a2a9-4c4f-9f90-e742f1c2be7&title=&width=778)

- 2000-2010年的早期，一个叫做深度学习的旧算法因为硬件的进步而变得实用。这导致了一场几乎一夜之间的革命，所以这种AI，我们今天谈论的通用目的AI，就是这样开始的。
- 这是从单一目的AI向更多学习型AI、通用目的AI的转变。它之所以不同，是因为这种AI通过训练实例学习每个任务，本质上是自我编程。
- Musk看到AGI的存在性威胁的地方，其他人却将AGI视为利润和权力的来源
- 法庭文件谈到了Q*，OpenAI正在开发一个秘密算法，称为Q*。这是从OpenAI泄露出来的，似乎已经被Sam Altman确认
- OpenAI最初的使命是一个非营利AI实验室的提议，其目标是赶上Google在AGI的竞赛中，但采取不同的方法和使命
- GPT-4的内部设计被保密，除了OpenAI，据信还有微软知道
- GPT-4可视为接近AGI的原型或前身
- GPT的一个限制是它不能回溯，一旦它们开始生成回应，即使结果是一个错误，它们也会继续下去
- 如果这种商业模式有效，它将彻底重新定义风险投资的实践方式；这将允许聪明的投资者基本上使用非营利组织的税前捐赠在该非营利结构下开发某物，然后通过与追求利润最大化的企业伙伴合作来丰富自己
- 马斯克要求美国法院判定Q star 是否是agi
# 20240301 The Illustrated Transformer
[https://jalammar.github.io/illustrated-transformer/](https://jalammar.github.io/illustrated-transformer/)
![image.png](https://cdn.nlark.com/yuque/0/2024/png/250863/1709296600614-7328ede1-2256-4768-bc14-56c024396f14.png#averageHue=%23fefefd&clientId=udd499d5e-4b0a-4&from=paste&height=402&id=ub0929a81&originHeight=804&originWidth=1436&originalType=binary&ratio=2&rotation=0&showTitle=false&size=145244&status=done&style=none&taskId=ueb788713-fb31-4771-a91b-eaa8ec8d622&title=&width=718)

- 在训练阶段，Transformer 架构通过自注意力机制（Self-Attention Mechanism）允许模型有效地处理序列数据
- 在推理阶段，Transformer 同样展现出其优势。由于其并行处理能力，Transformer 能够快速生成响应或完成任务，这对于需要实时响应的应用来说非常关键。此外，Transformer 的可扩展性也使得它能够通过增加模型的大小来提升性能，这是通过增加层数（depth）或者模型宽度（width）来实现的。
- encoder在结构上完全相同（但它们不共享权重）
- 前馈神经网络（FFNN）在处理序列中每个位置的输出时是独立的，并且对所有位置应用的是相同的网络结构和参数，这使得这一处理过程非常适合并行化
- 编码器的输入首先流经自注意力层，该层可帮助编码器在编码特定单词时查看输入句子中的其他单词
- 解码器具有这两个层，但在它们之间是一个注意力层，可帮助解码器专注于输入句子的相关部分
- Transformer 的一个关键属性，即每个位置的字都流经编码器中自己的路径
- 残差连接（Residual Connection）和层归一化（Layer-Normalization Step）是深度学习中用于改善网络训练效率和效果的两种技术。
- “编码器-解码器注意”层的工作方式与多头自注意力类似，不同之处在于它从其下面的层创建其查询矩阵，并从编码器堆栈的输出中获取键和值矩阵。

