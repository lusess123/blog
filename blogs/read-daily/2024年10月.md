# 20241017 Let's build GPT: from scratch, in code, spelled out. 4#每日陪读
+ [https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s)
+ 自注意力机制的核心部分，这可能是本视频中最重要的一部分，值得理解。
+ 每一个节点或每一个 token 在每个位置都会发出两个向量——一个查询向量 (query) 和一个键向量 (key)。
+ 这些 tokens 之间关联度的方式，基本上就是通过对键向量和查询向量做点积运算。
+ 查询向量会与所有其他 token 的键向量做点积，这个点积结果现在就变成了一个权重
+ 如果键向量和查询向量有某种程度的对齐，它们的交互会非常强烈，那么我就会从那个特定的 token 中学习到更多信息，而不是其他序列中的 token
+ 权重聚合过程是在这些节点的键向量和查询向量之间以数据依赖的方式进行的
+ 补充几点关于注意力机制的说明：
    - 注意力机制是一种通信机制
    - 注意力机制中没有空间的概念
    - 批次维度中的元素（即独立的样本）永远不会彼此通信。它们始终是独立处理的。
    - 在语言建模的情况下，我们有一种特定的有向图结构，其中未来的token不会与过去的token通信。
+ 注意力被称为“自注意力”的原因是，键、查询和值都来自同一个来源，来自 X。
+ 当我们希望从外部节点源中获取信息时，就会使用交叉注意力
+ 除以头部大小的平方根，他们称之为“缩放注意力”，这是一个重要的归一化步骤
+ 完整的模型中，你通常会有多层自注意力头部，每一层处理输入的不同部分并将其传递到下一层

# 20241016 Let's build GPT: from scratch, in code, spelled out. 3#每日陪读
+ [https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s)
+ 我想与过去通信，最简单的方式就是对所有前面的元素进行平均。
+ 仅仅进行求和或平均是一种非常弱的交互形式，这种通信方式损失极大。我们丢失了大量关于所有 token 空间排列的信息。
+ “bag of words”是指当你对事物进行平均时人们使用的术语
+ self-attention 中的数学技巧：矩阵乘法作为加权聚合——我们可以非常高效地通过矩阵乘法完成这一操作
+ 诀窍是我们能够使用“批处理矩阵乘法”来完成这个聚合，并且这是一个加权聚合，权重是一个（T，T）的矩阵
+ softmax中，我们对每个元素取指数，然后除以它们的总和

# 20241015 Let's build GPT: from scratch, in code, spelled out. 2#每日陪读
+ [https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s)
+ 最简单的神经网络语言建模是Byram
+ PyTorch 的 torch.nn 模块是 PyTorch 中专门用于构建和训练神经网络的核心模块。它提供了许多用于创建神经网络层、损失函数和模型优化的工具
+ 衡量损失或预测质量的一个好方法是使用负对数似然损失（negative log likelihood loss），它在PyTorch中实现为交叉熵（cross-entropy）
+ 如何损失值高于平均水平，说明模型的初始预测并不是非常分散（not super diffuse），它们有一定的熵（entropy）
+ AdamW相比随机梯度下降，是一个更高级且流行的优化器，并且效果非常好
+ nn.embedding 表，它有一个权重，用于存储查找表，这将被移到 GPU 上，以便所有的计算都在 GPU 上进行，从而大大加快速度。
+ 当我们不打算进行反向传播时，告知 PyTorch 也是一个好习惯。



# 20241014 Let's build GPT: from scratch, in code, spelled out. 1 #每日陪读
+ [https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s](https://www.youtube.com/watch?v=kCc8FmEb1nY&t=1221s)
+ **GPT** 是 **Generatively Pre-Trained Transformer** 的缩写
+ 多层感知器等，它真正介绍了语言建模的框架
+ 当人们说“标记化”时，他们指的是根据某个词汇表将原始文本（字符串）转换为一些整数序列。
+ 我们有非常小的词汇表，非常简单的编码和解码函数，但结果是我们得到了非常长的序列
+ 批处理与 **Transformer** 架构相结合，使我们能够通过利用 GPU 并行处理在大数据集上高效训练。
+ 一次性将整个文本输入到Transformer中，在计算上非常昂贵并且难以实现

# 20241013 The spelled-out intro to neural networks and backpropagation: building micrograd（六） #每日陪读
+ [https://www.youtube.com/watch?v=VMj-3S1tku0&t=3634s](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3634s)
+ 在梯度下降中，我们将梯度视为一个指向损失增加方向的向量。
+ 梯度下降法就是不断地进行前向传递、反向传递和参数更新。
+ 学习率的调整是一门微妙的艺术
+ 如果它太低，你的训练会花很长时间才能收敛；但如果它太高，整个过程会变得不稳定，可能还会让损失值“爆炸”。
+ 我们需要在反向传播之前清零梯度的方法
+ 神经网络本质上是一系列矩阵乘法和加法的组合，外加一些非线性函数。
+ PyTorch 的高效性源自它在更大规模上处理张量的能力，能够并行化操作，并针对 GPU 等硬件加速进行优化。
+ 调整参数 的过程就是乘以梯度的负步长，这就是梯度下降的基本原理
+ 我们只需要一团神经网络的东西，然后我们可以让它执行各种任务，这就是神经网络的强大之处。
+ 当你试图让这些神经元处理非常难的问题时，它们会表现出各种非常有趣的涌现特性
+ 学习率是随着迭代次数缩小的，这叫做学习率衰减（learning rate decay）。





# 20241012 The spelled-out intro to neural networks and backpropagation: building micrograd（五） #每日陪读
+ [https://www.youtube.com/watch?v=VMj-3S1tku0&t=3634s](https://www.youtube.com/watch?v=VMj-3S1tku0&t=3634s)
+ 神经网络只是特定类别的数学表达式。
+ 那么什么是神经元层呢？它只是独立评估的一组神经元。
+ 多层感知器（MLP）
+ PyTorch 的一个重要特点是，一切都更加高效。我们可以在所有这些张量上并行执行大量操作。
+ 计算一个单一的数字来衡量神经网络的整体性能
+ “mean squared error loss（均方误差）” 是机器学习中的一种常见损失函数，用于计算预测值与真实值之间的差异平方和。

# 20241011 OpenAI One Step Closer to SELF IMPROVING AI | AI Agents doing AI Research | MLE-bench #ai-web-exam
+ [https://www.youtube.com/watch?v=7_dkSFg2EvY&t=17s](https://www.youtube.com/watch?v=7_dkSFg2EvY&t=17s)
+ 我们似乎正在接近一个神经网络（这些 AI 模型）开始对科学和创新作出越来越多贡献的时代
+ AGI 的变革性影响常见的反对意见之一是认为 AI 很难做到一切。
+ 事实上，我认为我们只需要关注一件事，那就是 AI 何时会在进行 AI 研究方面超过人类。
+ Ashenbrener 认为，通过对未来几年 AI 能力发展的简单推断，到 2027 年底，AI 将轻松达到或超越最优秀人类的水平。
+ AI 研究可以被自动化，而自动化 AI 研究就是启动非凡反馈循环所需的一切。
+ Emily bench，这是一个用于衡量 AI 代理在机器学习工程中表现的基准测试。
+ 能够自主解决我们基准测试中的挑战类型的 AI 代理可能会解锁科学进步的巨大加速。

# 20241010 "GLAD Sam Altman Was FIRED" Geoffrey Hinton | Nobel Prize in Physics Sparks Controversy #每日陪读
+ [https://www.youtube.com/watch?v=MTK3wpzAPwY](https://www.youtube.com/watch?v=MTK3wpzAPwY)
+ 杰弗里·辛顿并不赞同Sam Altman的领导方式。他相信Altman更重视利润而非安全。
+ Hiton 对人工智能的一个重大贡献是神经网络的理念。
+ 大多数顶尖研究人员都认为人工智能将变得比人类更智能。他们在时间尺度上存在分歧。
+ 去年，钓鱼攻击的数量增加了1200%，这是因为这些大型语言模型使得进行钓鱼攻击变得非常容易。
+ 如果你考虑到某种拥有超越整个人类智慧的东西，你真的无法预测它会做什么。
+ **P-1984** 可能是一个指代极权统治与技术控制的比喻，结合了乔治·奥威尔的著名小说《1984》和 AI 的概念。
+ 如果你相信某件事，不要放弃，直到你明白为什么这种信念是错误的。
+ 近60%的人认为天才可能是一种罕见的认知能力，而不仅仅是高智商。
+ synesthesia（联觉）是一种罕见的脑部状态，它会让一些感官不自觉地融合在一起，几乎产生第六感。
+ 天才可能是大脑的某种特定连接方式，你可以指着说，这种奇特的连接方式使得这个人能够完成我们在整个智力谱系中看不到其他人能够完成的认知任务。
+ 做天才级别工作的第一个要求就是不要害怕去做只有天才能做的事情。
+ 智力仅仅是快速、准确地处理信息，还是它还有更深层次的意义？
+ Intelligence killed genius ： [https://guzey.com/intelligence-killed-genius/](https://guzey.com/intelligence-killed-genius/)
+ AI真正令人兴奋的一件事是，随着AI的进展，我们可能会学到更多关于人类大脑的知识。我们可能会更多地了解智力和天才，甚至可能会提出一些我们之前从未听过的新词和新概念。
+ AlphaProt 这一领域的进步可能在未来10到20年内治愈所有疾病
+ 当AI模型在物理学中取得重大进展、突破和创新时，我们可能会把奖项颁给创建该模型的人。

# 20241009 Elon Musk "Do NOT Trust Sam Altman" | SHOCKING INTERVIEW
+ [https://www.youtube.com/watch?v=JP7QN_KM7nY&t=10s](https://www.youtube.com/watch?v=JP7QN_KM7nY&t=10s)
+ **P Doom** 是指 "Probability of Doom"（末日概率），这是一个术语，用来估计全球灭亡或完全毁灭的可能性。
+ 明年，你将能够让 **AI** 制作一部关于某个主题的短片，最迟到明年年底。
+ **OpenAI** 中的 "open" 是马斯克起的名字。
+ 这可能是最大的挑战——如果 **AI** 在所有方面都比你强，你如何找到生活的意义？
+ **AI** 象棋程序远远优于任何人类——超出了人类的理解范围。
+ 有一段时间，最优秀的人类棋手结合最好的计算机可以击败单独的计算机。然后，事情发展到了如果你加入人类，反而会让一切变得更糟的地步。
+ 如果公司只是因为竞争的压力而尽可能快地推出 **AI**，这就不好了。
+ **AI** 的未来光明，但同时也很脆弱。
+ 我能安心入睡并接受这一点的方式之一是…活着见证数字超级智能的到来
+ 我的哲学是，我们应该努力扩展意识的范围和规模。应该努力让人类数量更多、思考更多，甚至可能存在让机器也拥有意识的理由。



# 20241009 The spelled-out intro to neural networks and backpropagation: building micrograd (四)#每日陪读
+ [https://www.youtube.com/watch?v=VMj-3S1tku0&t=4812s](https://www.youtube.com/watch?v=VMj-3S1tku0&t=4812s)
+ 接近梯度被覆盖的问题是，参考链式法则在多变量情况的推广，我们必须累加这些梯度
+ Python 不能处理 `2 * A` 时，它会检查是否 `A` 知道如何乘以 2，这会被重定向到 `rmul` 方法。
+ `e^x` 对 `x` 的导数就是 `e^x`
+ 除法只是一个更强大运算的特殊情况。除法可以转化为乘以 `b` 的负一次方
+ 只要你能够为那个小操作实现前向传播和反向传播，操作的类型和复杂度就无关紧要了。
    - 
    - 

# 20241006 Zuck Announces Meta's NEW AI Model... it's 🔥 up... #每日陪读
+ [https://www.youtube.com/watch?v=Hh152IbXPus](https://www.youtube.com/watch?v=Hh152IbXPus)
+ Movie Gen使用 SAM 2，也就是 Segment Anything Model，来为视频中的对象提取分割掩码
+ 由于 SAM 2 模型是开源的，我认为这意味着其他 AI 视频公司很快也会使用它来创建类似的数据集，并将这一功能加入到他们自己的模型中。
+ Meta 可能会在明年推出有史以来最好的 AI 视频模型。

# 20241006 The spelled-out intro to neural networks and backpropagation: building micrograd（三） #每日陪读
+ [https://www.youtube.com/watch?v=VMj-3S1tku0&t=2436s](https://www.youtube.com/watch?v=VMj-3S1tku0&t=2436s)
+ chain rule 提供了一个清晰的框架来处理变量之间的依赖关系，通过递归的方式，我们能够从局部导数推导出全局导数。
+ chain rule表达为：如果你想求 dZ 对 dX 的导数，那么你需要先求 dZ 对 dY 的导数，再乘以 dY 对 dX 的导数
+ 加法节点的作用可以这样看，它只是传递梯度。
+ 这个反向传播信号，携带了 L 对所有中间节点的导数信息，我们可以把它想象成沿着图向后流动。
+ 反向传播的本质：递归应用chain rule，从最终输出节点向后逐步传递gradients
+ 这些梯度随后被用于更新模型参数，通常使用的方法是梯度下降。
+ 梯度给了我们一些力量，因为我们知道如何影响最终结果，这在训练过程中将非常有用。
+ 唯一重要的是我们知道如何对任何一个函数求导。所以我们拿到一些输入并生成输出，重要的是我们知道如何对其求导。如果你知道输入如何影响输出的局部导数，那这就是你所需要的全部信息。

# 20241005 OpenAI New AI Operating System LAUNCHED | Is "Canvas" the New God-Tier Coder? #每日陪读
+ [https://www.youtube.com/watch?v=sY0PFX83nHs](https://www.youtube.com/watch?v=sY0PFX83nHs)
+ Canvas 的主要思想是，首先，你不需要从零开始
+ 你会在越来越多的 AI 工具中看到这种功能。这确实提升了功能性
+ 目前，还不能直接在这个窗口内运行代码，但这个功能可能很快就会推出。
+ 更像是一个协作工作空间，你们两个人可以在这个 Canvas 上一起工作
+ 代码审查意味着 ChatGPT 会提供内嵌的建议来改进你的代码，这功能做得非常出色。
+ Google Drive 中人们可以在旁边评论的方式类似，ChatGPT 也会在这里提供建议
+ 一种复杂的感受：你会觉得这是在浪费时间，还是觉得自己会变得更好？
+ 它确实改变了开发者处理代码的方式。能够实时获得建议并自动改进代码的功能，是几年前我们无法想象的。
+ 比Anthropic的Cloud更好的地方之一——它会在执行过程中向你展示它的进展。
+ Claude是第一个真正显著提升编码能力的工具
+ 很多人来说，尤其是对那些正在学习的孩子或新手，这彻底改变了游戏规则。
+ 我认为我们还缺少的一点是：首先，我想要将高级语音模式与此结合起来。
+ 我们可能会看到这些智能代理首先在受控的小环境中应用，这些环境要么不容易出错，即使出错也能轻松恢复。
+ 结合高级语音模式、更强的智能代理能力、控制计算机的能力和视觉功能…我们正在非常接近一个不再需要点击或输入的时代。
+ 现在通过这些来回互动的界面，我们正在摸索与这些模型交互的最佳用户界面。





# 20241005 OpenAI Dev Day Sam Altman on AGI, AI Agents, Alignment and Google's Notebook LM（下）
+ [https://www.youtube.com/watch?v=UB47ajBTvs0](https://www.youtube.com/watch?v=UB47ajBTvs0)
+ 我们本可以等更久，真正弄清楚关于版权音乐的分类和过滤器。但我们决定先发布它，同时增加更多功能，不过我想Sam问过我四五次了
+ context windows 和 RAG 的看法：
    - 长上下文的使用率比我预期的要低得多
    - 什么时候我们能够达到可以把你一生中见过的每一条数据都放进去的地步？这显然需要一些研究上的突破。无限上下文将在某个时候实现，可能会在十年内实现
    - 一千万个令牌的快速而准确的上下文，我预计这会在几个月内实现。
+ 坦率地说，这是我们经常问自己的一个问题。这里我觉得开发者可以在其中发挥很大作用，因为存在通用性和特异性之间的权衡。

# OpenAI Dev Day Sam Altman on AGI, AI Agents, Alignment and Google's Notebook LM（上）
+ [https://www.youtube.com/watch?v=UB47ajBTvs0](https://www.youtube.com/watch?v=UB47ajBTvs0)
+ 接下来的两年也会有非常陡峭的进展，再远一些就很难看得那么清楚了
+ 定义真的很重要，定义的重要性意味着我们可能已经非常接近了。
+ Turing test，我曾一直认为这是一个非常清晰的里程碑，你知道，那段模糊时期突然过去了，没人真正关心
+ 你无法选择科学的发展方向
+ 这是我多年来学到的东西，你不能催促突破，但你可以让所有正确的要素到位。
+ O1 显然是我们迄今为止最强大的模型，但它也是迄今为止我们最在线的模型。
+ 你终于具备了推理的能力，能够处理复杂问题，将其分解为更简单的问题并采取行动。2025年将是这一技术真正大规模应用的一年。
+ 代理人是下一个大趋势，它即将到来，这将是另一种突破
+ 如果你真的要让一个代理人开始在你的计算机上进行操作，你对系统的健壮性、可靠性和对齐性将会有非常高的要求。
+ 信任框架是使代理人真正被人们在日常生活中使用的最重要部分之一。
+ 接着下一步的重要进展是让代理能够自主处理更复杂的、多步骤的任务。
+ 你需要为AI模型几乎不能做到的事情进行构建，你知道，可能早期的使用者会去尝试，而其他人还不会完全接受，但这意味着当下一个模型发布时，随着我们不断改进，那些几乎不起作用的应用场景，你将是第一个实现它的人，那将会非常棒
+ 但要找出这个边界非常困难。不过我认为最好的产品将在这个边界上诞生。
+ 无论一项新技术或新的科技浪潮多么酷，都不能免除你必须做出所有艰苦的工作，以建立一家具有持久性的伟大公司。
+ 一个非常常见的现象，就是“我可以做这件令人难以置信的事情，我可以提供这种了不起的服务”，看起来像是一个完整的答案，但它不能免除你遵守任何商业的常规规则。你仍然需要建立一个具有良好战略定位的优秀业务。
+ 有人在 Twitter 上说，现在很酷的做法是把你的 LinkedIn 导出成 PDF，然后交给 Notbook LM，然后你会听到两个播客主持人来回讨论你有多厉害，和你这些年来的所有成就。
+ 客户服务团队可能只需要原本规模的 20%，这都归功于这些技术

# 20241001 The spelled-out intro to neural networks and backpropagation: building micrograd  2 #每日陪读
+ [https://www.youtube.com/watch?v=VMj-3S1tku0&t=1221s](https://www.youtube.com/watch?v=VMj-3S1tku0&t=1221s)
+ 包装函数的作用是为我们提供了一种方法，在Python中输出更好看的表达式，这样我们就不会得到一些难以理解的结果。
+ 梯度检查是指我们在反向传播中推导导数，并得到相对于所有中间结果的导数。
+ 数值梯度就是通过小步长估算导数的过程

