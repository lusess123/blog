# 20250914 The future of agentic coding with Claude Code（上）
+ [https://www.youtube.com/watch?v=iF9iV4xponk](https://www.youtube.com/watch?v=iF9iV4xponk)
+ 在你进行开发时，它已经进入“内循环”（核心迭代环节）。这是过去一年最大的变化：现在写代码时你会用 **agent**，而不是再在 **IDE** 里直接手动改文本。
+ 我们已经看到的趋势是：从“人直接改文本”转向“让模型代为修改文本”。
+ 编码工作自动化尝试难点：
    - 模型本身还不够强；
    - 所谓“**scaffolding**”（搭在模型之上的支架/框架）也不够好。
+ 型在 **agentic coding**（智能体式编码）方面变强很多，这种提升体现在 **3.7**、现在的 **4.0**，以及 **Opus 4.1** 等版本上。
+ **Harness** 也提升很多。显然，这个 **harness** 就是 **Claude Code**：你与模型交互的方式不是“直接用模型”，而是通过这层 **harness**。
+ 可以说 **Claude** 像马，而作为工程师，你要让它朝某个方向前进并对其进行引导，你需要在它周围搭建某种 **scaffolding**（支架/外层系统）来正确“掌舵”。
+ 每天“造模型的人”都会用模型来完成他们的工作。
+ 如果你让模型“自由运行”约 30 分钟，在 **3.5** 上它也许能短时间保持在正确轨道上——可能就一分钟左右。
+ 无论是哪个模型、在测哪种新东西，我就用它来写代码，看看整体“感觉/流程”如何；并没有特定的固定测试。
+ 从某种意义上说，如果你只拿模型做某一件固定的事，你反而会错过一些新能力，比如通过 **MCP** 拉取上下文、读取你的 **Slack** 消息等。
+ 从某种意义上，最好的 **eval**（评测）就是最贴近真实使用的评测；这时“直接用它”反而能给出最佳信号。
+ 我知道针对不同产品我们尝试过各种评测，但对 **Claude Code** 来说，真正有效的是“紧密的反馈闭环”，它几乎比任何“硬编码评测集”都能更快给出信号。
+ 我认为要找到能覆盖软件工程全部复杂性的“合成评测”真的很难。
+ **Claude Code** 的“**dogfooding**（自家人自用、以用促建）循环”是我见过最好的之一。

# 20251012 OpenAI Just SOLVED Hallucinations...（下）
+ [https://www.youtube.com/watch?v=uesNWFP40zw](https://www.youtube.com/watch?v=uesNWFP40zw)
+ 基准测试优化模型可能会助长**hallucinations（幻觉/胡编乱造）**。
+ 我们实际上在强化这些幻觉，因为我们只关注答对，而在不确定答案时乱猜并不会受罚，这种行为反而能提高测试分数
+ 人是在校外、在“社会的狠课”中才学会表达不确定性的价值。
+ 非常自信地说一个百分之百错误的答案——这会消耗你的社会信用
+ “自信地说蠢话”所带来的羞耻感，正是这些模型所缺失的
+ **hallucinations**只是在**base models**中不可避免
+ 如今的训练方式并不能把所有**hallucination**的表现形式都“熨平”。它会提升准确率，但那只是平均意义上的提升；模型猜错的时候依然会错，不过平均分更高，所以我们还会给它“击个掌”。
+ 几乎都是二元评分（要么对要么错），只有极少数不是。并且，只有**wild bench**会对“I don't know”（**IDK**）给予加分，其他统统不会
+ 减少**instruct models**（聊天机器人）的**hallucinations**：
    - 要么惩罚错误答案，要么在模型不确定时，对其说“I don't know”给予加分
    - “不自信”量化为某种数学表征，当其低于某个**threshold**时，就应当让模型说 “I don't know”。
    - 

# 20251011 OpenAI Just SOLVED Hallucinations...（上）
+ [https://www.youtube.com/watch?v=uesNWFP40zw](https://www.youtube.com/watch?v=uesNWFP40zw)
+ 这种事我们都遇到过：我们提问，它给出答案，但却“错得很自信”。你明知道答案不对，但它偏偏坚称自己正确。
+ 答对就会收到“点赞”（正向反馈）；而只要不是正确答案——无论是答错、说 _I don't know_，还是其他说法——都会被判为错误。
+ 这种现象并非源自 **large language models** 内在的缺陷，而是出在训练与评估流程上。换句话说，这不是模型的错，而是我们人类的错
+ 如果错误陈述无法与事实区分开来，那么在 **pre-trained language models** 中，**hallucinations** 就会因自然的统计压力而出现。
+ 模型没有动力去说 _I don't know_
+ **LM** 被训练去具备“自我确定性”。换句话说，它们利用自身的置信度作为奖励信号，通过内部反馈进行 **reinforcement learning**（强化学习）。
+ “是否有效”就是一种基准测试。即模型要学会识别对与错。如果它能正确区分好答案和坏答案，那就是个好模型。

# 20250910 Alfred Wahlforss - Listen Labs
+ [https://www.youtube.com/watch?v=-_gPru8KmIE](https://www.youtube.com/watch?v=-_gPru8KmIE)
+ customer research 这是一个极其庞大的市场，因为每家公司都想“以客户为中心”，并深入理解是什么驱动他们的客户。
+ 一家名叫 **Qualrix** 的公司，他们做的全是枯燥的客户问卷调查。这家公司估值 120 亿
+ 一家 **Fortune 500**（财富 500 强）公司：“你们每年在 **Qualrix** 上花多少钱？” 他们说：“每年 1,000 万美元。这一块的总支出大约是 300 亿美元。
+ 这些公司基本都是管理不善的咨询公司，技术含量低，而且动作非常迟缓。非常适合被 **LLM** 颠覆
+ 我们每月有 2,000 条入站线索，但只能为其中 50 位完成上手/接入
+ “模拟回应”：设想一下，能否基于既有访谈进行外推，并据此用“合成访谈”给出建议

# 20250909 visual storytelling
+ [https://www.vev.design/visual-storytelling/](https://www.vev.design/visual-storytelling/)
+ 最简单来说，visual storytelling（视觉叙事）是“showing, not telling（展示而非叙述）”这种叙事技巧的一个华丽说法。
+ 由于人们对视觉刺激的反应速度远快于单纯的文字，而且反应更具个人化
+ 它是一种长篇幅、有意放慢节奏、并且带有沉浸享受感的内容形式，专为长时间注意力设计
+ 作为高度吸引人的内容形式，它们通常包含一定程度的读者参与和个性化，这是其他数字营销形式根本无法实现的。
+ 矛盾的是，网络同时成为了“短小内容”和“大规模视觉叙事”的栖身之所。挑战在于如何在美学吸引力与可用性、可访问性之间找到平衡。
+ 几种实现方式
    - Scrollytelling Sections
    - Horizontal Scrolling Timelines
    - Video Scroll
    - Before & After Sldier
    - Animated Charts
    - 3D Elements
+ 技巧
    - Illustrated experiences（图文并茂）
    - Animation and interactivity
    - Reader agency（观众参与）
    - 文字作为视觉艺术
+ 最佳实践
    - 从一开始就计划视觉内容设计
    - 保持一致的视觉风格
    - 促进读者参与
    - 使用移动和过渡
    - 不要忽略文本
+ SEO-Friendly
    - 压缩
    - 懒加载
    - visual file meta-data
    - 测试速度和性能

# 20250908 Codex
+ [https://developers.openai.com/codex](https://developers.openai.com/codex)
+ 登录账号或者API key 的方式使用都可以
+ 推荐使用 Codex 与 GPT-5，我们最好的编码模型。默认的推理级别是中等
+ 当你希望 Codex 主动修改代码并准备一个 pull request 时，请使用代码模式。
+ 默认是关闭联网的，Codex预设了访问的白名单
+ 提示词技巧
    - Codex 擅长定位相关代码，但当提示将其搜索范围缩小到几个文件或包时，效率更高
    - 当 Codex 能够验证其工作时，它会生成更高质量的输出
    - 你可以告诉 Codex 如何处理任务或使用它的工具
    - 像人类工程师一样，当复杂的工作被分解成更小、更专注的步骤时，Codex 能更好地处理。
    - 当遇到错误或意外行为时，尝试将详细的日志或错误跟踪粘贴到 Codex 中作为首要的调试步骤
    - 除了针对特定任务外，Codex 经常在开放式任务上给我们带来惊喜
+ AGENTS.md 不包括claude code

# 20250907 Turn boring AI designs to Pro-level with these simple steps(下)
+ [https://www.youtube.com/watch?v=lgqnMzOu2Lw](https://www.youtube.com/watch?v=lgqnMzOu2Lw)
+ Unicorn Studio ：非常**lightweight**（轻量）。它的**performant**（性能表现）也很强
+ Spline
+ 这些编辑功能，让它“像设计工具”一样，但同时拥有**AI 赋能**的能力，而且一切都是**代码层**——不是只停留在 **Figma** 里的**静态设计**
+ 对我这样做了 20 年设计的人来说，也很难一眼看出它是 AI 生成的。**这正是我们要的效果**——你的设计/艺术不该给人“AI 味儿”。
+ 如今，让作品**带有 AI 痕迹**几乎像是一种**冒犯**。能避免就尽量避免。即便大家知道它是 AI 生成的，也务必**保证质量**——不要给人“投机取巧/做得很糙”的感觉，比如“六根手指”“怪异的眼睛”等问题
+ **Spline** 是一款**基于浏览器的 3D 设计与交互工具**，主打“所见即所得”的建模、动画、互动（鼠标/触摸事件）和**网页嵌入**，并支持多人**实时协作**
+ 切换到 **cloud 4**，因为它在**修复类任务**上通常更**stable**（稳定）、更**robust**（可靠）；**创意**阶段已经完成——这一点 **GP5** 很擅长。

# 20250906 Turn boring AI designs to Pro-level with these simple steps（上）
+ [https://www.youtube.com/watch?v=lgqnMzOu2Lw](https://www.youtube.com/watch?v=lgqnMzOu2Lw)
+ **AI** 特别喜欢给的那些渐变色
+ **GPT5** 是一次巨大的飞跃,此前大约只能生成 200 行，如今一次就能生成约 800 行代码，这带来了巨大的差别。你会得到更高的一致性；**image-to-HTML** 转换效果更好；视觉理解能力也更强
+ **GPT5** 生成的那些紫色渐变更少，结构更好，排版（**typography**）等方面也更佳。它对字距（**letter spacing**）和字体（**fonts**）的理解更到位。
+ GPT5在动画方面可能不那么理想，所以有时你需要切换到 **Cloud 4**
+ **Cloud 4** 在结构、**HTML** 与代码层面的稳定性上可能更“聪明”一些；但 **GP5**（即 **GPT5**）显然更有创造力，也更专业，并且视觉理解更强。
+ 如果只是做一个落地页，你不必指定这些细节；但如果你只做单个区块，就完全可以把这些都写清楚。
+ 如果你有明确的品牌基调，强调色（accent color）可以是蓝色——但千万别用紫色，因为 **AI** 很爱自动给紫色。可以用琥珀色（amber）或橙色，这些很流行；翠绿色（emerald）现在也很受欢迎
+ 投影、背景、边框之类的设置，如果拿不准，就直接跳过也没问题。
+ 越来越多设计师在用 _guys_，因为 _inter_ 更像 **AI** 的默认回退字体。如果你想让设计更有一点“oomph”（冲击力/劲儿），_guys_ 是个不错的选择
+ 我发现 **AI** 聚焦单一区块时产出更好：会为该区块生成更多代码与细节；整页截屏反而使每个区块的细节变少，这不是我们想要的
+ **AI** 对“偏方正”的布局不太拿手，除非你明确说明，它仍倾向使用更**圆角**的样式。你可以明确要求“更方正（squareish）”，或“圆角半径设为 0”。
+ 每次“借鉴灵感”时，一个**非常非常重要**的步骤是：打开 **Prompt Builder**，然后在提示里写上诸如“修改文本（change text）”“使用 **lucid icon**”“做成响应式（make responsive）”之类的要求。
+ 当你在既有模板上混改时，风格一致性会非常好。
+ 尽量**减少普通粗体（bold）****的使用。原因是现在屏幕更好了，而 ****AI**** 又很喜欢到处用粗体**
+ **AI** 经常生成**很糟糕的素材**，通常**随机抓自 Unsplashed**（原文如此，常见平台应为 **Unsplash**），而且 **AI** 喜欢反复用同一批素材、同一套头像

# 20250902 OpenAI vs. Deepseek vs. Qwen: Comparing Open Source LLM Architectures（ 下）
+ [https://www.youtube.com/watch?v=raTbhtKZTZA](https://www.youtube.com/watch?v=raTbhtKZTZA)
+ DeepSeek **V3.1** 保留了与 **V3** 相同的核心架构，但提供了更强的推理、更聪明的工具调用以及更高的整体性能
    - 在现代 **LLM** 中，大量算力与内存都被 **KV cache** 占据，因此 **V3** 采用 **MLA**：先把 **keys/values** 压缩到更小的 **latent space**（潜空间）再缓存，推断时再解压使用。
+ **Quen** 的 **MoE** 基座模型只用到五分之一的 **active parameters**（激活参数）就达到了 **dense** 模型的性能。
+ 各家如何扩展 **context length**（上下文长度）,使模型能处理远超原始训练上限的序列。
    - **GPTOSS** 从预训练阶段就应用 **Yarn**，因此权重原生适配 **131k** 上下文；**GPTOSS** “生来”就具备长上下文
    - **Deepsee** 采取分阶段路线——先微调到 **32k**，再训练到 **128k**；**DeepSeek** 通过分步学习获得；**Quen** 则在 **32k** 训练基础上把极限再往外推。
    - **Quen** 也微调到 **32k**，但跳过了后续再训练。**Quen** 在 **inference** 阶段再次应用 **Yarn scaling**——把 **RoPE** 基频放大 **4×**，无需额外再训练就达到 **128k**
+ 各主流模型在后训练与推理阶段都大量依赖 **reinforcement learning (RL)**——更有趣甚至出人意料的是，部分 **RL** 训练所需数据量极小：比如 **Quinn** 仅用 **4,000** 对数据。
+ 各实验室之间数据集差异不透明；显然，幕后进行了大量 **dataset**,这部分工作很可能构成它们的 **moat**（护城河）的一大块，使公司在开源/发布模型时更有底气——外界很难复刻其成果。
+ 不要只盯着 **benchmark** 或“**context size**”等顶层指标；要关注各实验室为达成这些结果所采用的具体方法。

# 20250902 OpenAI vs. Deepseek vs. Qwen: Comparing Open Source LLM Architectures（上）
+ [https://www.youtube.com/watch?v=raTbhtKZTZA](https://www.youtube.com/watch?v=raTbhtKZTZA)
+ **GPT OSS** 采用**专家混合（** **Mixture of Experts, MoE** **）****架构，提供两种规模：****1200 亿参数****和****200 亿参数**。
+ **GPTOSS** 作为**仅解码器（decoder-only）**的 **Transformer** 训练而成，并融入了许多现代 **LLM** 的常见特性
    - **grouped query attention（GQA）**：一种改进的注意力机制，允许多个 **query** 头**共享同一组** **key-value** 对，从而**减少内存**占用并**加速**推理
    - 馈层使用 **swiggloo** 激活函数，相比更简单的 **RLU**，能实现更**细腻**的变换
    - **rotary positional embeddings（RoPE）**，将**位置信息**直接嵌入注意力计算，因而更好地支持**长上下文**。
    - 模型采用**预归一化**的 **RMSNorm**：通过“**均方根**”缩放输入，以获得更**稳定**的训练
+ 亮点
    - **131,000 token** 的上下文窗口，这是在**预训练**中就应用 **YaRN** 缩放实现的，而非在推理时临时扩展。
    - 训练完成后，模型**默认以量化格式**发布，因此足够**轻量**，可部署在**一般硬件**上。这使其可运行在**消费级 GPU**、笔记本或其他**资源受限**设备上。不过，目前**未提供**未量化版本。
+ Qwen3**致密模型**共有**七档**规模，其中包含**6B** 级别——是当代开源权重模型中**较小**的一类
+ **Qwen 3** 的一大差异在于其**控制 Q/K/V 投影尺度**的方法，以在**大规模**下保持注意力**分数稳定**。
+ **思考模式融合**：**Qwen 3** 的关键创新，将“**思考/非思考**”两种风格**统一到一个模型**中，用户可**无缝切换**。尽管该特性在 **Qwen** 首发时颇为独特，但 **GPT-5** 现在也提供了类似的切换。
+ DeepSeek V3  的一大不同在于：它采用与 **GPT OSS**、**Qwen 3** **不同**的注意力机制。

