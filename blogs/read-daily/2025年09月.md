# 20250906 Turn boring AI designs to Pro-level with these simple steps（上）
+ [https://www.youtube.com/watch?v=lgqnMzOu2Lw](https://www.youtube.com/watch?v=lgqnMzOu2Lw)
+ **AI** 特别喜欢给的那些渐变色
+ **GPT5** 是一次巨大的飞跃,此前大约只能生成 200 行，如今一次就能生成约 800 行代码，这带来了巨大的差别。你会得到更高的一致性；**image-to-HTML** 转换效果更好；视觉理解能力也更强
+ **GPT5** 生成的那些紫色渐变更少，结构更好，排版（**typography**）等方面也更佳。它对字距（**letter spacing**）和字体（**fonts**）的理解更到位。
+ GPT5在动画方面可能不那么理想，所以有时你需要切换到 **Cloud 4**
+ **Cloud 4** 在结构、**HTML** 与代码层面的稳定性上可能更“聪明”一些；但 **GP5**（即 **GPT5**）显然更有创造力，也更专业，并且视觉理解更强。
+ 如果只是做一个落地页，你不必指定这些细节；但如果你只做单个区块，就完全可以把这些都写清楚。
+ 如果你有明确的品牌基调，强调色（accent color）可以是蓝色——但千万别用紫色，因为 **AI** 很爱自动给紫色。可以用琥珀色（amber）或橙色，这些很流行；翠绿色（emerald）现在也很受欢迎
+ 投影、背景、边框之类的设置，如果拿不准，就直接跳过也没问题。
+ 越来越多设计师在用 _guys_，因为 _inter_ 更像 **AI** 的默认回退字体。如果你想让设计更有一点“oomph”（冲击力/劲儿），_guys_ 是个不错的选择
+ 我发现 **AI** 聚焦单一区块时产出更好：会为该区块生成更多代码与细节；整页截屏反而使每个区块的细节变少，这不是我们想要的
+ **AI** 对“偏方正”的布局不太拿手，除非你明确说明，它仍倾向使用更**圆角**的样式。你可以明确要求“更方正（squareish）”，或“圆角半径设为 0”。
+ 每次“借鉴灵感”时，一个**非常非常重要**的步骤是：打开 **Prompt Builder**，然后在提示里写上诸如“修改文本（change text）”“使用 **lucid icon**”“做成响应式（make responsive）”之类的要求。
+ 当你在既有模板上混改时，风格一致性会非常好。
+ 尽量**减少普通粗体（bold）****的使用。原因是现在屏幕更好了，而 ****AI**** 又很喜欢到处用粗体**
+ **AI** 经常生成**很糟糕的素材**，通常**随机抓自 Unsplashed**（原文如此，常见平台应为 **Unsplash**），而且 **AI** 喜欢反复用同一批素材、同一套头像

# 20250902 OpenAI vs. Deepseek vs. Qwen: Comparing Open Source LLM Architectures（ 下）
+ [https://www.youtube.com/watch?v=raTbhtKZTZA](https://www.youtube.com/watch?v=raTbhtKZTZA)
+ DeepSeek **V3.1** 保留了与 **V3** 相同的核心架构，但提供了更强的推理、更聪明的工具调用以及更高的整体性能
    - 在现代 **LLM** 中，大量算力与内存都被 **KV cache** 占据，因此 **V3** 采用 **MLA**：先把 **keys/values** 压缩到更小的 **latent space**（潜空间）再缓存，推断时再解压使用。
+ **Quen** 的 **MoE** 基座模型只用到五分之一的 **active parameters**（激活参数）就达到了 **dense** 模型的性能。
+ 各家如何扩展 **context length**（上下文长度）,使模型能处理远超原始训练上限的序列。
    - **GPTOSS** 从预训练阶段就应用 **Yarn**，因此权重原生适配 **131k** 上下文；**GPTOSS** “生来”就具备长上下文
    - **Deepsee** 采取分阶段路线——先微调到 **32k**，再训练到 **128k**；**DeepSeek** 通过分步学习获得；**Quen** 则在 **32k** 训练基础上把极限再往外推。
    - **Quen** 也微调到 **32k**，但跳过了后续再训练。**Quen** 在 **inference** 阶段再次应用 **Yarn scaling**——把 **RoPE** 基频放大 **4×**，无需额外再训练就达到 **128k**
+ 各主流模型在后训练与推理阶段都大量依赖 **reinforcement learning (RL)**——更有趣甚至出人意料的是，部分 **RL** 训练所需数据量极小：比如 **Quinn** 仅用 **4,000** 对数据。
+ 各实验室之间数据集差异不透明；显然，幕后进行了大量 **dataset**,这部分工作很可能构成它们的 **moat**（护城河）的一大块，使公司在开源/发布模型时更有底气——外界很难复刻其成果。
+ 不要只盯着 **benchmark** 或“**context size**”等顶层指标；要关注各实验室为达成这些结果所采用的具体方法。

# 20250902 OpenAI vs. Deepseek vs. Qwen: Comparing Open Source LLM Architectures（上）
+ [https://www.youtube.com/watch?v=raTbhtKZTZA](https://www.youtube.com/watch?v=raTbhtKZTZA)
+ **GPT OSS** 采用**专家混合（** **Mixture of Experts, MoE** **）****架构，提供两种规模：****1200 亿参数****和****200 亿参数**。
+ **GPTOSS** 作为**仅解码器（decoder-only）**的 **Transformer** 训练而成，并融入了许多现代 **LLM** 的常见特性
    - **grouped query attention（GQA）**：一种改进的注意力机制，允许多个 **query** 头**共享同一组** **key-value** 对，从而**减少内存**占用并**加速**推理
    - 馈层使用 **swiggloo** 激活函数，相比更简单的 **RLU**，能实现更**细腻**的变换
    - **rotary positional embeddings（RoPE）**，将**位置信息**直接嵌入注意力计算，因而更好地支持**长上下文**。
    - 模型采用**预归一化**的 **RMSNorm**：通过“**均方根**”缩放输入，以获得更**稳定**的训练
+ 亮点
    - **131,000 token** 的上下文窗口，这是在**预训练**中就应用 **YaRN** 缩放实现的，而非在推理时临时扩展。
    - 训练完成后，模型**默认以量化格式**发布，因此足够**轻量**，可部署在**一般硬件**上。这使其可运行在**消费级 GPU**、笔记本或其他**资源受限**设备上。不过，目前**未提供**未量化版本。
+ Qwen3**致密模型**共有**七档**规模，其中包含**6B** 级别——是当代开源权重模型中**较小**的一类
+ **Qwen 3** 的一大差异在于其**控制 Q/K/V 投影尺度**的方法，以在**大规模**下保持注意力**分数稳定**。
+ **思考模式融合**：**Qwen 3** 的关键创新，将“**思考/非思考**”两种风格**统一到一个模型**中，用户可**无缝切换**。尽管该特性在 **Qwen** 首发时颇为独特，但 **GPT-5** 现在也提供了类似的切换。
+ DeepSeek V3  的一大不同在于：它采用与 **GPT OSS**、**Qwen 3** **不同**的注意力机制。

