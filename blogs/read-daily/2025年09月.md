# 20250902 OpenAI vs. Deepseek vs. Qwen: Comparing Open Source LLM Architectures（上）
+ [https://www.youtube.com/watch?v=raTbhtKZTZA](https://www.youtube.com/watch?v=raTbhtKZTZA)
+ **GPT OSS** 采用**专家混合（** **Mixture of Experts, MoE** **）****架构，提供两种规模：****1200 亿参数****和****200 亿参数**。
+ **GPTOSS** 作为**仅解码器（decoder-only）**的 **Transformer** 训练而成，并融入了许多现代 **LLM** 的常见特性
    - **grouped query attention（GQA）**：一种改进的注意力机制，允许多个 **query** 头**共享同一组** **key-value** 对，从而**减少内存**占用并**加速**推理
    - 馈层使用 **swiggloo** 激活函数，相比更简单的 **RLU**，能实现更**细腻**的变换
    - **rotary positional embeddings（RoPE）**，将**位置信息**直接嵌入注意力计算，因而更好地支持**长上下文**。
    - 模型采用**预归一化**的 **RMSNorm**：通过“**均方根**”缩放输入，以获得更**稳定**的训练
+ 亮点
    - **131,000 token** 的上下文窗口，这是在**预训练**中就应用 **YaRN** 缩放实现的，而非在推理时临时扩展。
    - 训练完成后，模型**默认以量化格式**发布，因此足够**轻量**，可部署在**一般硬件**上。这使其可运行在**消费级 GPU**、笔记本或其他**资源受限**设备上。不过，目前**未提供**未量化版本。
+ Qwen3**致密模型**共有**七档**规模，其中包含**6B** 级别——是当代开源权重模型中**较小**的一类
+ **Qwen 3** 的一大差异在于其**控制 Q/K/V 投影尺度**的方法，以在**大规模**下保持注意力**分数稳定**。
+ **思考模式融合**：**Qwen 3** 的关键创新，将“**思考/非思考**”两种风格**统一到一个模型**中，用户可**无缝切换**。尽管该特性在 **Qwen** 首发时颇为独特，但 **GPT-5** 现在也提供了类似的切换。
+ DeepSeek V3  的一大不同在于：它采用与 **GPT OSS**、**Qwen 3** **不同**的注意力机制。

