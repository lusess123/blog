# 20250331 AI Dev 25 | Justin Uberti: Introduction to the OpenAI Realtime API  3
+ [https://www.youtube.com/watch?v=fHz6s0YmNFo](https://www.youtube.com/watch?v=fHz6s0YmNFo)
+ 很多智能设备都是基于实时操作系统（RTOS）构建的。
+ 我认为问题就变成了：我们该如何桥接实时模型与推理模型？人类能做到这点，因此我们应该期望实时 API 也能做到。
+ 为什么实时 API 无法进行推理？ 因为当你使用推理模型时，它会生成推理用的“token”，这些 token 的生成是需要时间的。
+ 我们需要一种方式让客户端连接 OpenAI，但不暴露 API 密钥。我们用一种叫做“短暂令牌（ephemeral tokens）”的机制来实现这个目标。

# 20250330 AI Dev 25 | Justin Uberti: Introduction to the OpenAI Realtime API  2
+ [https://www.youtube.com/watch?v=fHz6s0YmNFo](https://www.youtube.com/watch?v=fHz6s0YmNFo)
+ 我们的发展方向是建立媒体（例如音频）的直接连接，让其从我们的 API 服务器直接传送至客户端。同时，我们还有一个用于工具调用的通道，当你需要运行某些工具时，API 服务器可以直接连接到你的应用服务器
+ 输入音频缓冲区（input audio buffer），它实时跟踪用户的语音输入，检测用户何时开始和停止讲话。
+ responses API 是单向传输的（从服务器到客户端）；而实时 API 是双向传输的。你可以将事件从客户端流向服务器，也可以从服务器流回客户端。
+ 它确实能够听到不同的声音，但目前还无法识别“这是第一个人说话，这是第二个人说话”，这个功能还没有被训练出来。
+ 它的基础还是一个语言模型（LM），而语言模型的核心同样接受过文本数据训练。其中一部分就涉及到提示（prompting）机制，用于引导响应听起来比纯文本更加流畅自然。
+ 类比一下图像训练（例如CLIP模型），先识别图像内容，再引导模型生成类似的结果。
+ 模型默认行为是等待用户讲话完毕再回应，但开发者可手动控制模型中断行为。
+ 当前典型的交互模式是：用户讲话、用户停止说话，然后智能助理（assistant）才作出回应。

# 20250329 AI Dev 25 | Justin Uberti: Introduction to the OpenAI Realtime API  1
+ [https://www.youtube.com/watch?v=fHz6s0YmNFo](https://www.youtube.com/watch?v=fHz6s0YmNFo)
+ Real-time API 基本上是一种封装方式，用于构建交互式语音代理。
+ Justin Uberti 在加入 openAI 之前在 Google 工作了大约15年，领导创建了 WebRTC 技术
+ Real-time API 专注于低延迟，我们的目标是返回语音数据的时间控制在 500 毫秒以内——最好能接近 300 毫秒，从而实现真正类似人类的语音响应速度。
+ 研究还发现，当对话之间的停顿时间越长……你越会觉得对方在隐瞒些什么。
+ 语音是最原始的用户界面——很长一段时间里，人们就是靠说话来完成事情。在印刷术、网络、互联网出现之前，我们就是靠这种方式与他人互动的。
+ 在Real-time API 之前，大多数现有系统采用的是组件式或串联式模型。
+ 文字转语音组件在生成语音前还需要“向前看”。你不能直接把词元一个个地喂进去，它必须提前看到更多文本，才能恰当地添加情感。
+ Real-time API，它是基于 GPT-4.0 Omni 技术的语音对语音模型。音输入进入模型后，会直接被转换为语言模型的高维空间表示，而不是文本。
+ 语音到语音模型的输出不是纯文本，而是一个文本记录加上一些我们称为 VQ 代码 的内容。这些 VQ 代码随后会被转换为输出语音。这种“输入语音 → 输出语音”的单一前向流程，使得整个过程更快速。因为你只需进行一次处理循环。
+ 你总可以通过减少层数和参数来让模型变快。
+ 它还非常易于引导，因为它是一个深度模型。语音表现可以高度可控，而且能很好地执行指令。
+ GPT-4.0 是一个深度模型，因此即使不明确指定语言，也能处理多种语言。
+ 语音理解始终保持在模型的高维表示中——不会被转换成文本。这使得副语言信息能够被保留。副语言信息包括语调、口音和语音的抑扬顿挫。模型能利用这些信息，而这些信息如果先被转成文本就会丢失。
+ 早期版本的实时 API，音频缓冲、采样率转换、回声消除——全都得开发者自己处理。

# 20250328 This new AI image generator changes everything...（下）
+ [https://www.youtube.com/watch?v=2vW2bIlnA7g](https://www.youtube.com/watch?v=2vW2bIlnA7g)
+ 这个模型不仅能够理解单个环节中的上下文，它还能够理解多个环节之间的上下文关系。
+ 它在保持编辑一致性方面表现非常好。这是在视觉能力方面，AI 模型迈出的巨大一步。
+ 他们给出了非常具体的指令，而这个模型准确无误地完成了任务。
+ 它就是没法生成一杯装满酒的酒杯，不管你怎么提示它，它都不会生成一整杯的酒。
+ 把“平方根”转成数学符号。所以它会把文本形式转换成类似符号形式的表达。  
而现在它具备了“世界知识”——可以把图像和文本之间的知识关联起来。它真的是在看代码，并根据这些代码生成图像。  
+ 多语言文字渲染、编辑的精确性，以及包含小字体的高密度信息——这些情况可能都不是模型最擅长的使用场景。

# 20250327 This new AI image generator changes everything...（上）
+ [https://www.youtube.com/watch?v=2vW2bIlnA7g](https://www.youtube.com/watch?v=2vW2bIlnA7g)
+ 我们将 GPT-4 训练成了一个全能模型，它不仅仅是一个语言模型，还是一个可以处理图像、音频和所有模态的模型。它理解它们，能够生成它们，并且可以无缝地在这些领域之间工作
+ 这将把我们引向将这些更多地做成工具，而不是玩具，供人们使用的方向。这将把我们引向将这些更多地做成工具，而不是玩具，供人们使用的方向。
+ 表情包是我们内部版本中这个模型的主要应用之一。
+ 我们正在提供比通常更高程度的创造性表达和创造自由。所以我们希望模型如果你不想让它冒犯，就不要冒犯，但如果你想让它冒犯，当然在合理范围内，就让人们创造他们需要的，想要的东西。
+ 随着我们的模型越来越强大，它们对世界的了解也在加深。但到目前为止，它们其实只能以文本或代码的形式表达自己。但到目前为止，它们其实只能以文本或代码的形式表达自己。
+ 这些图像比我们之前的图像生成速度慢，但质量简直好得不可思议。我们认为这非常非常值得等待。
+ 我认为一般来说，我们希望这个模型不仅能够生成图像，还能够以正确的方式融入精确的文本，这使得它不仅仅是一个想象力的工具，还可以用于学习和交流。

# 20250326 ARC AGI 2 The $1,000,000 AGI Prize
+ [https://www.youtube.com/watch?v=xvqv10NDGS8](https://www.youtube.com/watch?v=xvqv10NDGS8)
+ ARC AGI 基准测试，拥有全新的一套问题，即便是最聪明的 AI 模型也会被难倒，但人类却能轻松通过。
+ 新的测试在计算资源的投入上有更强的抵抗力，因此单纯投入更多资金和算力是解决不了问题的。
+ 目前，基础语言模型（即非推理模型）得分为零，而推理模型的得分也低于 4%。
+ ARC AGI 2 的核心并不在于让语言模型展示超越人类的技能，而是为了暴露 AI 缺乏的能力，即高效地习得新技能。
+ 这不是数据记忆或某种模式识别的问题。关键问题在于：当你处理这些问题时，你是否能习得新的技能？它考察了一些关键能力，比如符号解释、组合推理和情境规则的应用。
+ 对于智能的评价，他们不再仅仅关注能力和准确性，也同时关注效率。
+ ARC AGI 2 发布后三个月内是否有人能获得70%以上的成绩，人们认为这种可能性为8%。

# 20250325 To Dominate the AI Race, Don’t “Start”a Company | LiveKit, Russ d'Sa
+ [https://www.youtube.com/watch?v=A-IsoneWlzE](https://www.youtube.com/watch?v=A-IsoneWlzE)
+ 为了创办公司而去刻意创办公司，这是我职业生涯中多次犯过的错误。
+ live kit实际上承载了25%的911紧急呼叫系统；也有像Open AI这样的公司，正在使用我们的技术来为ChatGPT提供语音模式支持。
+ live kit是一种基础设施，它能让任何开发者都轻松构建使用实时音视频流的应用。
+ 《doom》和《Wolfenstein》游戏的两位创作者，他们每天晚上都会把soft disk公司的电脑拔掉，装进卡车带回家，然后在晚上用这些机器开发自己的游戏。然后第二天早上再把电脑带回去
+ 你必须坚定地相信你的产品愿景，以及你真正想要打造的东西。
+ 选择一个大市场是有好处的，但更重要的是选对市场。同时，你还需要考虑如何平衡你的策略，在这个市场中逐渐成长，最终建立一个可持续发展的事业。
+ 我从那次经历中学到一个有趣的道理：出问题其实是好事，因为问题的出现恰恰说明你创造的东西是人们真正需要的。
+ 我们所做的一切最终都是为了实现目的，而代码只是达到目的的工具。失败与崩溃实际上是一种必经的考验，它们证明你创造了值得持续改进的东西。
+ 真正致命的并不是竞争对手，而是创始人自己。我们其实并未真正解决任何问题，只是用酷炫的技术打造了一个看似酷炫的解决方案，实际上没有对应的真实需求。
+ live kit 正在解决的问题是互联网本身并非为传输音视频而设计的
+ 人们与AI互动的方式将模仿人与人之间的互动——计算机通过摄像头“看”，通过麦克风“听”，通过扬声器“说”。你需要一个“神经系统”在AI和用户之间传递信号，而live kit正在打造的，正是这种连接组织，帮助用户与AI模型顺畅沟通
+ 我所看到的市场潜力是全球所有的键盘和鼠标，因为它们最终都会被摄像头和麦克风所取代
+ 我给对AI或机器人领域感兴趣的创业者的建议是：这两个领域都是万亿级市场，规模庞大，变化迅速，但目前还处于非常早期阶段。与直觉相反的是，稍微等待一下可能更有好处：先加入一家顶尖的AI公司，学习基础知识，了解行业快速变化，并结识未来的联合创始人。
+ 灵感最终会自然而然地出现，不要刻意强求，但一旦你清晰地意识到机会来了，就要全心全意地去追求。当灵感真正到来时，你自然会知道，此时就应该立即行动，全力以赴去追寻它。

# 20250324 3 VIBE CODING is Eating the World...（下）
# 20250324 3 VIBE CODING is Eating the World...（下）
+ [https://www.youtube.com/watch?v=tmiuiOwf4ac&t=981s](https://www.youtube.com/watch?v=tmiuiOwf4ac&t=981s)
+ 无论如何，我认为资深开发者确实有个强有力的观点：目前的AI还没有达到能够取代经验丰富、花大量时间钻研技术的人的水平。
+ 一开始，AI的表现肯定不如专业人士亲自编码那样好，但随着时间推移，AI会大大减少人们逐字逐句手动编写代码的需求。
+ 数码相机带来的第二个变化是降低了技术门槛：你不再需要掌握一大堆复杂技术，只需拿起相机按一下按钮就行。
+ 从传统数码相机再到手机上的数码相机，拍摄照片的数量呈现爆炸式增长，从过去的数亿张左右，到2007年接近一万亿张。
+ 未来大多数的代码将由那些“时间富裕”的人撰写或生成。因此，多数代码将由孩子、英语非母语学生（ESL）而非专业软件工程师编写，这与视频、照片和其他社交媒体的趋势类似
+ 写作和出版的门槛消失了；但这并不意味着天赋、技能或专业能力上的差距不存在了，而是让更多人有机会参与、练习，并变得更好。  
平庸者——那些才能或努力程度较低的人——经济上会受到影响，收入减少；此外，由于产出更多，产品实际的价格也会降低。  
+ 对于数以百万计的简单任务，如游戏、家庭自动化或我们尚未想到的全新事物，AI将使创作它们变得如同说几句话般简单。

# 20250323 VIBE CODING is Eating the World...（上）
+ [https://www.youtube.com/watch?v=tmiuiOwf4ac](https://www.youtube.com/watch?v=tmiuiOwf4ac)
+ Andre Karpathy 发明了这个术语 Vibe coding，基本理念是，你无需亲自编写代码，而是大量依靠 AI 替你编写代码，你只需顺应 AI 的引导即，可理想情况下，你甚至可以用语音直接说出代码，而非用英文逐字打出来
+ 许多人对此都非常感兴趣，你并不需要编程或软件开发的知识，也无需太多经验，只要你愿意学习并保持灵活的态度即可。学习过程中可能会不太舒服，这完全正常。  
 Peter Levels，在组织2025年的 Vibe coding 游戏开发大赛，每个人都能提交自己用 AI 辅助开发的游戏。Andre Karpathy 和 John Karmac 当评委  
+ 基于3js的“堡垒之夜”与“我的世界”风格的游戏，效果令人赞叹。据说这个例子使用 Cursor 花了37个小时完成。另外一个例子则使用了 wind surf。
+ 如果你从未编程或接触过这些东西，只想尝试一下，最简单的方法可能就是从 Claude 入手。
+ 这种 Vibe coding 方式也遭到了一些批评，有人认为它不是一种严肃的软件开发方法。很多人对这种方式不屑一顾，甚至嘲笑，认为这只是一群不会编码的人在尝试，但他们无法做出真正先进、复杂的东西。
+ 90年代数码相机兴起的情景：当时的数码相机已足够普通消费者使用，使很多新人进入摄影领域。任何人买台相机，随意拍摄即可轻松获取照片，而无需了解胶片运作、相机原理，或如何冲洗挑选胶片。从很多方面来看，这些新人对专业摄影一窍不通，没有专业技能，许多专业摄影师因此瞧不起数码相机，认为它们没什么用。专业人士知道如何挑选正确的胶片、正确冲洗、正确曝光，以精准拍出所需的效果。
+ Vibe coding 和各种AI辅助编程工具似乎会引发类似的情况，我们可能会看到大量新人进入编程领域，尝试开发软件、游戏，以及各种他们能想到的东西。
+ Peter Levels主张编码属于每个人，并暗示那些因Vibe coding而担忧职业安全的人，如果一直持有消极态度，将真的可能面临失业。

# 20250318 OpenAI "Full Code Automation" Coming This Year... （下）
+ [https://www.youtube.com/watch?v=18UWzTnXLjc&t=603s](https://www.youtube.com/watch?v=18UWzTnXLjc&t=603s)
+ Cloud 会给任何特定的提示或项目分配多少算力呢？也就是一种“测试与计算”的概念。例如，输入“思考”、“多想想”或“再深入一些”这样的提示，可能意味着给某个任务或项目分配更多的算力资源。
+ 目前 Claude Code 并不具备真正的视觉功能，无法根据视觉反馈作决策，而 Manus 则使用一种名为 Browser Use 的开源视觉模型，直接与视觉元素交互。
+ OpenAI 发布了名为 Operator 的开源视觉模型，它能进行真实的视觉交互，不只是简单地选择网页元素，因此显得更加人性化。
+ Claude Code 仍需较多的人类指导，而 Manus 可以独立执行复杂任务，不用频繁干预。
+ 网络上确实有许多极为聪明的软件工程师。他们解释了为何自动化软件工程中那些更难的部分是如此困难。他们认为，人工智能编程将会变得一团糟，带来诸多问题和错误。它也可能使人们对编程的理解能力下降。

# 20250317 OpenAI "Full Code Automation"（上）
+ [https://www.youtube.com/watch?v=18UWzTnXLjc](https://www.youtube.com/watch?v=18UWzTnXLjc)
+ Kevin Wheel，是OpenAI的首席产品官，他提到与Anthropic公司的人交流时，对方表示到2027年，前端和后端的功能性代码有99%的可能会自动化
+ Kevin预测，根据竞赛编程的基准测试，今年AI将永久地超越人类程序员，就像计算机在70年前永久超越人类的乘法计算一样。
+ Sam Altman预测，到2025年底将会实现超越第一名的超人类级别的编程模型
+ 我认为下一代这类工具可能类似于Claude Code结合先进的用户体验设计。尽管Manis的评价褒贬不一，但它精准地把握了一些出色的设计方法，很可能会成为未来编码代理的概念蓝图。
+ 首先清晰地定义任务以收集上下文，然后再让模型进行思考，最后再增加计算资源的使用。这与Anthropic最近发布的公告相呼应，即允许用户对每个任务或项目的算力分配拥有更大的控制权。

# 20250513 livekit docs
+ [https://docs.livekit.io/home/](https://docs.livekit.io/home/)
+ livekit是一个开源的多媒体应用平台，支持WebRTC, 服务端的核心代码是go编写的，包含一个AI Agent框架
+ OpenAI, Character.ai, Retell,  Speak 的 实时对话AI产品  都是基于livekit的
+ LiveKit Cloud 提供了Edge optimized， 这个应该是他的核心盈利点
+ 集成了Krisp,用于处理背景噪声，Krisp的最核心产品是 Noise Cancellation（噪音消除）
+ livekit的AI Agent支持两种模式：
    - MultimodelAgent：多模态,端对端，类似openAI的高级语音模式
    - VoicePipeineAgent： STT， LLM， TTS 管道模式，能提供更强大的控制
+ Turn detection （讲话什么时候开始，什么时候结束）  是AI语音聊天的最大挑战之一。livekit 使用（VAD Voice activity detection，语音活动检测 ）和 Turn detection model （livekit自己的开源模型）
+ OpenAI 的 Realtime API 内置了VAD，但是针对VoicePipeineAgent，livekit集成了Silero VAD （由 Silero 开发，基于深度学习的开源VAD模型，[https://github.com/snakers4/silero-vad）](https://github.com/snakers4/silero-vad）)
+ WebRTC通常需要WebSocket作为Signaling Channel(信令通道)

# 20250309 China Releases WORLD'S FIRST AUTONOMOUS AI Agent... Open Source | Manus
+ [https://www.youtube.com/watch?v=CFo1iTd_Cc8](https://www.youtube.com/watch?v=CFo1iTd_Cc8)
+ 对大部分普通人而言，在庞大的代理市场中找到合适的代理并不容易，而 Manus 有效地解决了这个问题
+ Manis 的名字来源于拉丁词汇“manus”（手），象征着知识必须应用于实践的信念。它旨在增强人类的能力，将你的愿景变为现实。
+ 随着这些聊天机器人性能的提升，很快可能会出现这样一种情况：让 AI 聊天机器人为你定制应用程序，比你上网搜索已有软件更快
+ Adobe 软件非常强大，创造了数十亿美元的收入，但大多数用户只使用了其中大约20%的功能。试想一下，如果 AI 助手能让你直接描述你真正需要的那20%的功能，然后专门为你量身打造对应的定制软件，那将会如何？
+ 你完全可以通过自然语言指令来实现上述操作，无需复杂的用户界面。这种方式可能会极大地影响软件即服务（SaaS）公司，以及像谷歌这样的在线信息服务提供商。
+ Manus 或者 Anthropics 的 Cloud coder，二者都能通过 Linux 命令行自主地完成编码、故障排查和软件部署等任务，而完全不需要用户友好的图形界面。
+ 这种类型的自主软件部署似乎已成必然，除非政府对开源软件实施监管；否则，这种技术即将到来，并可能彻底改变整个软件即服务（SaaS）行业。

# 20250302 PT-4.5 Fails. AGI Cancelled. It's all over...
+ [https://www.youtube.com/watch?v=kkZ4-xY7oyU](https://www.youtube.com/watch?v=kkZ4-xY7oyU)
+ GPT 4.5 有粉碎性能基准吗？并没有。相比 GPT 40 它确实有些适度的提升，但远远比不上推理模型，比如 o03 mini high。输入75美元，输出150美元，无疑是目前市面上最昂贵的模型
+ 每增加0.5的版本号，大约对应着10倍的预训练算力。
+ GPT-1 勉强能生成连贯文本；GPT-2 是个混乱的小玩具；GPT-3 比较有趣；而到了 GPT-3.5，则真正引爆了 ChatGPT 的流行时刻
+ GPT-4 正是微软撰写《Sparks of AGI》论文的阶段，意味着我们从此跨越了某种原始通用人工智能（Proto-AGI）的门槛
+ 现在 GPT-4.5 的算力是 GPT-4 的 10 倍，这就是我们目前看到的情况。为什么这很重要？因为如果在不断 10 倍扩展算力 的同时，我们发现它的能力反而下降了，这可能意味着扩展规模已经遇到了瓶颈。
+ 计算能力提升十倍所带来的效果；但你会发现，这种提升非常微妙，也难以解释清楚，你必须深入探究才行——它不是显而易见的，你得真正理解问题本身，以及为何这种差异如此微妙
+ GPT 4.5能够从语境中学习，并创造出在历史背景下正确的歌词段落，效果非常不错。而GPT 40则无法做到这一点，它在上下文学习方面存在困难；与之相比，Claude 3.7 Sonnet模型则表现更好
+ GPT 4.5，这就是我们一直期待的Orion模型。这个模型规模巨大，是一个合成数据工厂。它实际上并不是为大多数人日常使用的各种任务设计的。它的真正作用是生成合成数据，用来训练下一代模型，也就是推理模型。
+ 为什么这个API价格如此昂贵？因为GPT-4曾是所有人的合成数据工厂，每个人都从中提炼知识，以创建自己的模型。对于GPT-4.5，OpenAI可能不想再次出现这种情况，因此如果有人想用它来生成合成数据，就必须支付高昂的费用。
+ 至于扩展能力是否遇到了瓶颈，我认为真正的答案只有当我们看到从这次模型中衍生出的推理模型后才能确定。
+ 4.5确实比4更好，但这种能力的提升究竟会如何转化为我们构建推理模型的效果呢？对我们来说，这些可能只是小的、微妙的改进，但当我们用这些数据进行 **RL**——强化学习——时，对于未来的推理模型系列可能意味着巨大的改进。

