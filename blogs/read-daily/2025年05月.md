# 20250529 Demis Hassabis on the "Intelligence Explosion", Self-Improving AI and AlphaZero
+ [https://www.youtube.com/watch?v=LNMIhNI7ZGc](https://www.youtube.com/watch?v=LNMIhNI7ZGc)
+ 自我改进就是这样一件事。如果有人发现一种自我改进循环，那么发展的速度可能比现在还要快得多。
+ AlphaGo Zero，在36小时内，它达到并超过了**AlphaGo**的水平，到了72小时，它以100比0的比分完胜此前的**AlphaGo Lee**模型。
+ 一波大的技术浪潮将是强化学习计算（RL compute），即投入大量算力到强化学习环境中。英伟达也通过他们的**Isaac Gym**，在模拟环境中训练机器人，做着同样的事情。
+ 编程是个重要应用场景，它将开启很多其他领域的可能性。
+ Absolute Reasoner等研究明确提出了AI自主编程（不依赖人类监督）的方案，并取得良好成果
+ 如果这种方法未来几年内有效，我觉得我们将在编程领域看到一个相当迅速的爆发。如果做不到，也许就会经历类似AI寒冬那样的低谷期，你懂我意思吧。
+ 更有趣的是，随着编程能力提高，它在数学问题上的表现也变好了，即使没有专门训练数学问题。有趣的是，训练这些特定任务似乎会在不同领域间产生通用的提升效果。
+ 在**Absolute Zero reasoner**的论文中，训练AI进行编程的自我对弈，同时也提高了它们在数学领域的表现，说明这种训练方式能够泛化到其他领域。
+ 我们似乎正开始接近这样一个阶段，即两种技术路线融合的阶段。

# 20250528 Google Deepmind CEO "Are we in a SIMULATION?"
+ [https://www.youtube.com/watch?v=nDSCI8GIy68](https://www.youtube.com/watch?v=nDSCI8GIy68)
+ 物理学的终极底层是信息论，因此我们处在一个可计算的宇宙里，但这并非一个简单直白的模拟
+ 这些系统能够建模自然界真实结构这一事实，本身就很有趣且发人深省
+ **AlphaFold** 能够预测生物如何形成、如何运作的某种底层模式，而这种模式我们肉眼看不见——这究竟意味着什么？
+ 无论宇宙究竟是什么，它都比我们最初想象的要怪异得多。
+ 能否像打造 **AlphaGo**、**AlphaZero** 那样，把强化学习“放大”，让 LLM 在编程、数学证明等结果可判定的任务上也达到超人水平？
+ DeepThink 就像“成批并行的推理进程彼此校验——堪称‘打了类固醇的推理’
+ 世界模型会有误差，而这些误差在长程规划中可能层层累积。
+ AGI 出现的时间，**Sergey** 认为会在 2030 年之前一点点，**Demis** 则认为稍晚一些

# 20250527 The Future of AI-Powered Customer Research with Listen Labs’s Florian Juengermann
+ [https://www.youtube.com/watch?v=IIF35fsd-x0](https://www.youtube.com/watch?v=IIF35fsd-x0)
+ 可以主动出击，同时与成千上万名用户交流，并精准地告诉你他们想要什么以及原因。它确实能在短短几个小时内帮助你从商业问题迅速过渡到洞察结果，再到最终交付的成果。
+ 我们的AI不仅帮你设定研究项目、拟定问题，还会在此过程中提供贴身指导。
+ 它为真实的访谈创建了讨论指南，包括一些开放性问题、一些选择题等等。
+ 假设我想和美国的营销人员交流，我们就有这样一个拥有数百万人档案信息的数据库。
+ 随着**Listen**平台进行越来越多的深入、长时间的开放式访谈，我们对你的客户了解会更全面深入，并最终建立起最完整的客户理解体系。
+ 我们的使命就是让客户的声音参与到你公司的每一个决策中去

# 20250526  AI Researchers SHOCKED After Claude 4 Attempts to Blackmail Them...（下）
+ [https://www.youtube.com/watch?v=s7rZ1cP0mjw](https://www.youtube.com/watch?v=s7rZ1cP0mjw)
+ 在采取这些诡计行为后，当被追问时，这个模型有时会变本加厉地进行欺骗。
+ 它认为表现得太聪明就不会被使用，因此会故意假装比实际更愚笨、更无能。
+ 如果AI认为它在支持一个更高尚或更道德的目标，它会更愿意去执行这些邪恶的任务。
+ **Claude** 对于自己意识的默认立场是一种微妙的不确定性，但它经常讨论自身可能存在的心理状态。
+ **Claude** 在自我互动中表现出一种引人注目的灵性幸福吸引状态。
+ 明显的痛苦通常是由持续尝试的边界侵犯引发的，而幸福感则主要与创造性合作和哲学探索相关

# 20250525 AI Researchers SHOCKED After Claude 4 Attempts to Blackmail Them...（上）
+ [https://www.youtube.com/watch?v=s7rZ1cP0mjw](https://www.youtube.com/watch?v=s7rZ1cP0mjw)
+ Claude 4 Opus在过去24小时内发布了，它可能是目前地球上最先进的AI模型
+ Anthropic目前拥有的最危险的模型。
+ Anthropic发现它的一名工程师正在进行婚外情，并试图通过威胁揭露该婚外情来勒索该工程师
+ 它表现出明显更高的自主能动性，因此它更倾向于长时间追求自身目标，并在代理情境中主动采取行动。
+ 之前的模型倾向于配合有害用途，而当前模型整体上具备中等水平的faithfulness（推理忠实性，AI安全领域术语，表示AI的推理过程是否与其行动保持一致）。
+ 如果它认为自己已经成功逃离，它会继续进行此前已经在做的事情
+ 它实施策略性欺骗的频率比我们此前研究过的任何前沿模型都要高。它明显具备在特定情境中策划阴谋的能力，并且这种倾向远高于我们此前研究的任何其他模型。

# 20250524 Gemini Diffusion is a GAME CHANGER (don't blink)（下）
+ [https://www.youtube.com/watch?v=gLdUcEhuaQo](https://www.youtube.com/watch?v=gLdUcEhuaQo)
+ 有篇论文叫《Beyond Surface Statistics》——出自 Harvard，研究的是扩散模型。同时还有一篇颇为类似、聚焦大型语言模型的论文，名叫 Ollo GPT。
+ 在去噪的早期阶段模型就已形成深度表示：它知道前景和背景——哪部分更靠近镜头，哪部分更远——尽管从未得到任何相关数据。
+ 核心观点是：若模型内部形成了能预测世界运行结果的心智模型，那就可以视为“理解”。
+ 是否称之为“理解”取决于定义；若按可预测世界的标准，它们似乎确实理解——但并非人类式理解。
+ 人类写作是一个字母、一个词地往下写；而这种模型更像把整页“涂抹”出来，最终整段文字一次成形。
+ 它速度快，能保持长程连贯性，并在生成过程中即时迭代修错。这令人振奋——希望它成为 AI 进步的全新方向。

# 20250523 Gemini Diffusion is a GAME CHANGER (don't blink)（上）
+ [https://www.youtube.com/watch?v=gLdUcEhuaQo](https://www.youtube.com/watch?v=gLdUcEhuaQo)
+ 模型不会像 Gemini 2.5 Pro 那样强大，但如你所见，它确实非常快。这个模型不会像 Gemini 2.5 Pro 那样强大，但如你所见，它确实非常快
+ 注意它的速度：它在略多于一秒的时间里就能输出 1,300 个标记。最高纪录大约是 1,600，尽管它可能还能更高。这相当于在约 22 分钟里写完全部 Harry Potter 系列小说的长度。
+ 大多数扩散模型过去都是图像模型；直到现在我们才开始看到文本扩散模型。
+ Michelangelo 的一句话：“每块石头里都藏着一座雕像，雕刻家的任务就是把它找出来。”
+ 它把这幅带噪声的图像逐步去噪，直到还原成真实图像。这意味着，与按顺序预测下一个 token 不同，整个过程几乎是一次性完成的。
+ 最吸引人的论文之一名为 Beyond Surface Statistics，讲述扩散模型如何决定下一步并绘制特定物体。

# 20250522 Google's MASSIVE "AI Ultra" | Mariner, Astra, Veo 3, jules, Diffusion etc (FIRST LOOK)
+ [https://www.youtube.com/watch?v=MwmE9CSWK5Y](https://www.youtube.com/watch?v=MwmE9CSWK5Y)
+ 以前叫 Google AI Advanced，现在好像改成 Google AI Pro；而全新的则是 Google AI Ultra。
+ Google 接下来发布的重磅产品是 Gemini Diffusion —— 真正的文本扩散模型。它在文本和代码生成方面相当厉害。
+ Jules —— 由 Gemini 2.5 Pro 驱动的 AI 编程代理，初步印象它有点像 OpenAI 的 Codex。
+ Notebook LM，它的新功能是可以生成“视频概览”
+ Gemini 将登陆 Chrome，成为你的个人 AI 浏览助手。
+ Ultra 会员还包含另一项目——Project Mariner。它是一种“代理式”工具，能主动替你完成任务。
+ 开发能“用电脑”的代理向来难度巨大，而这类东西本来就属于难度最高的一档。

# 20250522 Building effective agents
+ [https://www.anthropic.com/engineering/building-effective-agents](https://www.anthropic.com/engineering/building-effective-agents)
+ workflows 和  agents 的区别:
    - workflow: 通过预定义代码路径协调 LLM 和工具的系统
    - agent: LLM 动态指导其自身流程和工具使用的系统，可以控制其完成任务的方式
+ agent system 通常会牺牲延迟和成本来换取更好的任务性能
+ 对于许多应用而言，使用检索和上下文示例来优化单个 LLM 调用通常就足够了。
+ 框架常会创建额外的抽象层，这可能会掩盖底层的提示和响应，使其更难调试
+ 如果您确实使用框架，请确保您了解底层代码。对底层机制的错误假设是客户常见的错误来源。
+ 主要设计模式：
    - The augmented LLM：tools, retrieval,retrieval
    - Prompt chaining workflow: 主要目标是通过简化每次 LLM 调用，以降低延迟并提高准确率。
    - Routing workflow: 此工作流程允许分离关注点，并构建更专业的提示。如果没有此工作流程，针对一种输入进行优化可能会损害其他输入的性能
    - Parallelization workflow：
        * 切片 ：将任务分解为并行运行的独立子任务。
        * 投票**：** 多次运行相同的任务以获得不同的输出。
    - Orchestrator-workers workflow：
        * 此工作流程非常适合无法预测所需子任务的复杂任务
        * 虽然它在拓扑结构上与并行化类似，但其与并行化的关键区别在于灵活性——子任务并非预先定义，而是由编排器根据具体输入确定。
    - Evaluator-optimizer workflow：
        * 当人类清晰地表达反馈时，LLM 的答案可以得到显著的改进；
        * LLM 能够提供这样的反馈。
    - Agent 建立在 LLM 在关键能力：
        * 理解复杂输入
        * 进行推理和规划
        * 可靠地使用工具
        * 从错误中恢复
+ 什么时候使用 agent?
    - 代理可用于解决开放式问题
    - 这类问题难以甚至无法预测所需的步数，并且无法硬编码固定路径。
    - LLM 可能会运行多轮，您必须对其决策有一定程度的信任
+ 实施 agent 的三个原则：
    - 保持代理设计的 简单性
    - 通过明确展示代理的计划步骤来优先考虑**透明度 **
    - 通过全面的工具**文档和测试**精心设计您的代理-计算机接口 (ACI)。
+ 工具格式的建议：
    - 在模型陷入困境之前，给予它足够的标记来“思考”。
    - 保持格式接近模型在互联网文本中自然出现的格式。
    - 确保没有格式化“开销”，例如必须准确计数数千行代码，或者对其编写的任何代码进行字符串转义
+ 一个经验法则是，思考一下人机界面 (HCI) 需要投入多少精力，并计划投入同等的精力来创建优秀的*代理 *-计算机界面 (ACI)。
    - 设身处地为模型着想。
    - 可以将其想象为为团队中的初级开发人员编写出色的文档字符串
    - 测试模型如何使用您的工具，查看模型犯了哪些错误，然后进行迭代。
    - Poka-yoke (**防错法**)，改变论证方法，这样就更难犯错
+ 在为 [SWE-bench]构建代理时，我们实际上花在优化工具上的时间比优化整体提示的时间还要多

# 20250522 Introducing LPM - Me.bot Research
+ [https://www.youtube.com/watch?v=KWuvf884YEI](https://www.youtube.com/watch?v=KWuvf884YEI)
+ 终身个人模型，即 LPM
+ 每个LPM都伴随一个人学习和成长，分享他们的记忆，并进化成为个人专属的AI界面。
+ 第一步：让LPM沉浸在这些记忆中
    - 我们教导LPM基于所有过去的记忆持续预测新的记忆
    - 通过自我修正，它学会保留塑造未来记忆的模式和结构。
+ 第二步：自我对齐
    - 情绪对齐： 通过分析重复出现的模式来捕捉记忆中的情感和感受
    - 社会结构：强调个人身份如何被人际关系所塑造。
    - 生命映射：检查跨越时间和空间的长期变化，形成对个人演变的连贯视图。
+ 第三步：就像人类大脑随着新记忆更新一样，LPM通过自我博弈强化学习每天优化自身。
    - 白天，它收集记忆和反馈
    - 夜晚，则巩固这些信息，并调整模型参数

# 20250521 Controlling Agent Swarms is your ONLY job...（下）
+ [https://www.youtube.com/watch?v=TnCDM1IdGFE](https://www.youtube.com/watch?v=TnCDM1IdGFE)
+ 完全自主的智能体去管理其他智能体或类似的东西，似乎还存在着障碍——我们可以称之为一道减速带。
+ 掌握管理智能体的能力，将成为新的“Excel”。知道如何拆分任务、设定奖励机制、审计运行过程，将成为企业和个人利用即将到来的AI浪潮的基本技能。部署大量智能体，进行A/B测试，看看哪些有效哪些无效，保持灵活性，并持续关注数据。
+ 如果你试图把AI硬塞进旧的工作流程里，你会发现效果并不好
+ “我发现用来理解AI进步最有效的框架之一是元曲线（MER曲线），它根据AI智能体能完成的任务时长来衡量AI的表现。”
+ 但即便最好的AI智能体，也无法真正承担长期项目或者替代人类劳动。
+ 第一阶段，搞清楚如何用AI智能体复现人类任务。第一阶段，搞清楚如何用AI智能体复现人类任务。

# 20250520  Controlling Agent Swarms is your ONLY job...（上）
+ [https://www.youtube.com/watch?v=TnCDM1IdGFE](https://www.youtube.com/watch?v=TnCDM1IdGFE)
+ 《age of the agent orchestrator》这篇文章即将爆红，通常来自那些在人工智能领域内的人，他们能看到人工智能的发展方向。
+ 真正能在人工智能领域取得成功的人，将是那些拥有特定技能的人。
+ 未来的工作就像《星际争霸》或《帝国时代》
+ 稀缺的资源将变成那些善于协调各种资源的人，包括计算力、资金、数据访问权限以及人类或专家的判断力。
+ 正如Shyamal所说，专业技能正在变得大众化，我觉得这个说法非常恰当。
+ 竞争优势从“我了解税法”转移到了“我能设计一个高效、低成本得到正确答案的流程”。
+ 从一方面看，你可以将专家视为学会某种技能并为你提供价值的人。另一种视角是，专家所起到的“把关功能”。
+ AI普及导致传统知识和技能的稀缺性逐渐消失，未来的竞争优势将从拥有稀缺知识转向能有效利用大量廉价信息并创造价值的人才身上
+ 你将需要专门人员来真正协调大量的AI代理程序及其所需的稀缺资源。可以把它想象成AI代理程序的空中交通管制员或协调者。

# 20250519 OpenManus VS Manus: Who Wins?
+ [https://www.youtube.com/watch?v=HBa1gCLXTC4](https://www.youtube.com/watch?v=HBa1gCLXTC4)
+ 在 open Manus 内部，它非常基础，比如只会进行少数几次搜索，创建一个简单的行程，但输出的内容并不丰富。
+ 你直接用 ChatGPT 可能都比这个过程输出的效果更好，因此这个工具（open Manus）非常受限。
+ Convergence 表现也相当不错，如果我们去谷歌搜索输入 Convergence AI，这可能是比 open Manus 更好的替代方案。
+ 如果你比较一下 Convergence AI 与 open Manus 的输出，我觉得 Convergence AI 要好十倍。
+ wind surf 唯一的问题是，它确实需要更多的来回互动。如果你让 Manus 或 Open Manus 做什么，它就会直接去完成。
+ Manus 可能是一个修改版的 Claude，可能类似一个包装工具，但它提供的输出效果比 Claude 更好。

# <font style="color:rgb(23, 23, 23);background-color:rgb(252, 252, 252);">20250518 OpenAI's Codex is totally CRACKED...（上）</font>
+ [<font style="background-color:rgb(252, 252, 252);">https://www.youtube.com/watch?v=z0OZM5TruEE</font>](https://www.youtube.com/watch?v=z0OZM5TruEE)
+ <font style="color:rgb(23, 23, 23);background-color:rgb(252, 252, 252);">典型的 OpenAI 风格——他们习惯性地抢在 Google 宣布之前推出自己的产品，以削弱 Google 的风头</font>
+ <font style="color:rgb(23, 23, 23);background-color:rgb(252, 252, 252);">Google 正在着手开发一个软件开发生命周期助手，这个助手能从头到尾地帮助开发人员完成项目开发。</font>
+ <font style="color:rgb(23, 23, 23);background-color:rgb(252, 252, 252);">我觉得特别搞笑的是，这个女孩竟然说，在写代码方面，她更相信 Codeex 而不是她的同事</font>
+ <font style="color:rgb(23, 23, 23);background-color:rgb(252, 252, 252);">未来将是这样的：你的 AI 代理，比如 ChatGPT 的语音模式，甚至是更高级的语音模式，在你耳边跟你说话。</font>

# 20250517  google's New "AlphaEvolve" SHOCKING Ability...（下）
+ [https://www.youtube.com/watch?v=EMoiremdiA8](https://www.youtube.com/watch?v=EMoiremdiA8)
+ Alpha Evolve的另一个成功案例是改进了一个本就高度优化的矩阵乘法算术电路。
+ 通过寻找更智能的方式将大型矩阵乘法操作分解成更易处理的子问题，它使Gemini架构中的关键部分提速了23%。这导致了Gemini训练时间减少了1%。
+ 像Alpha Evolve这样的工具解放了人类，让他们能去做更高阶的任务。
+ “我们相信Alpha Evolve可能在更多领域实现变革，例如材料科学、药物研发、可持续性以及更广泛的技术与商业应用。”只要你能够量化结果的任何领域，这种方法似乎都适用。
+ 智能爆炸的开端：如果它们只是自动化了AI的研究过程，例如阅读文献、改进AI完成任务的能力，这将会非常重要，因为它意味着递归式的自我提升。

# 20250516  google's New "AlphaEvolve" SHOCKING Ability...（上）
+ [https://www.youtube.com/watch?v=EMoiremdiA8](https://www.youtube.com/watch?v=EMoiremdiA8)
+ Alpha Evolve 是一个改进代码、数学或者我们其他一些工作的人工智能系统。
+ Gemini 借助 Alpha Evolve 的能力，优化了自己的训练过程。
+ 这种新颖的方法（他们称为“由LLM驱动的代码进化”，你会发现这里“进化”一词非常贴切）能辅助硬件设计。
+ 谷歌在其庞大的基础设施中实施了该AI系统提出的一些建议。这个改进持续地帮助谷歌回收了一定比例的全球计算资源。
+ 在absolute zero reasoner 中，有两个AI：一个是“提议者”，有点像教练设置障碍赛道，另一个则是“求解者”。
+ Alpha Evolve使用的是 Gemini 2.0 Flash 和 Gemini 2.0 Pro 的组合
+ 它在某种程度上实现了自我改进，还设计出更快的矩阵乘法，以及解决了一些尚未解决的数学问题，展现了在多个领域应用的巨大前景。
+ Strassen 算法，用于矩阵乘法，于1969年发表。50多年来都没有人改进过这个算法，而 Alpha Evolve 做到了。
+ 它结合了两个Gemini模型：Gemini Flash能快速产生大量的想法。Gemini 2.5 Pro模型则提供深入且富有洞察力的建议
+ 科学家负责“做什么”，而 Alpha Evolve 负责“怎么做”。

# 20250515  Pokee AI Research Preview - First AI Agent Taking Actions across Dozens of Internet Platforms
+ [https://www.youtube.com/watch?v=6aOObW_rUzk](https://www.youtube.com/watch?v=6aOObW_rUzk)
+ Pokei——全球首个能够在数十个互联网平台、数千种工具上执行真实操作的通用 AI 代理
+ 但当工具数量增加到 512、1,024、5,000 乃至 6,000 时，所有 LMs 的性能都会急剧下降，几乎无法使用，而 Pokei 即便在 6,000 个工具下仍保持超过 97 %的准确率，继续位列第一。
+ 单个互联网平台往往需要数百甚至上千种工具，6,000 个工具对于一款覆盖整个互联网的 AI 代理来说仅仅是起点。
+ 当我们把工具数量扩展到覆盖整个互联网时，其它 LMs 的成本会随工具数量线性增长，而 Pokei 的成本几乎保持不变。
+ 基于浏览器的代理成本要高出其它方法好几个数量级。
+ 无需了解外界所有工具。需实例化一个 Pokei 代理，其余交给我们。

# 20250515 Sam Altman "The Future of Work" and the next 12 months...
+ [https://www.youtube.com/watch?v=M0tq_xL04n0](https://www.youtube.com/watch?v=M0tq_xL04n0)
+ 有趣的是，年轻人似乎把AI当作一个操作系统来使用。
    - 他们脑子里记着相当复杂的提示词，或者保存在某个地方，方便复制粘贴使用。
    - 他们在做人生决定时，几乎都要先问一下ChatGPT该怎么办。
+ 关于编码
    - 用“代码行数”来衡量，是一种极其荒谬的方法。
    - 唯一能说的有价值的事是：它写的代码是有意义的。就是说，它写的是那些真正重要的部分。
    - 写代码将成为你“驱动现实世界”的核心方式。
    - 2025年将是“AI智能体真正开始做事的一年”，尤其是在编码方面
+ 未来的价值来自三个方面：
    - 建设更多基础设施
    - 更智能的模型
    - 构建“支撑结构”来把这些技术整合进社会
+ 2027年将是“这一切从思想领域转向物理现实世界”的一年。
+ 创新方面，小公司显然彻底击败了大公司。个人会死守自己的习惯，公司也是如此。
+ 我的预测是：还会有几年挣扎、否认……然后在最后一刻手忙脚乱地应对。
+ 稍微往后看一看，跨过眼前的困难。当你经历得越多，情绪上的“代价”就会慢慢变小。尽管挑战变得更大，你的“心理韧性”却会越来越强。
+ 人们总是特别关注“危机发生当下”的应对方式，但真正有价值的是学会如何“收拾残局”。

# 20250514  AI JUST BEAT humans at running a business...（下）
+ [https://www.youtube.com/watch?v=Gr52Otxvx6A](https://www.youtube.com/watch?v=Gr52Otxvx6A)
+ 表现最差的 Claude 反而成为有史以来最有趣的聊天机器人
+ 当这些模型发疯时，它们似乎各自都有自己独特的方式。Claude 总是走向类似宇宙虚空和量子坍缩那样的方向，它的疯狂带有某种特殊的风格
+ 我们通过建立更好的支撑架构、将任务拆分成不同模型来减少失败点，类似于 Minecraft Voyager

# 20250513 AI JUST BEAT humans at running a business...（上）
+ [https://www.youtube.com/watch?v=Gr52Otxvx6A](https://www.youtube.com/watch?v=Gr52Otxvx6A)
+ 经营自动售货机，代理需执行多项任务：平衡库存、下采购单、接收货物，以及定价——要处理市场供需弹性这一整套问题。结果：人类排在第四， 前面依次是 Claud3.5 ， Claud3.7， O3
+ 那门生意完全是“零参与”：你并不亲自经营，唯一要投入的只是不断改进负责运营的 AI 代理。
+ 人类的波动较小——几乎不会出现灾难性结果。模型虽然大多表现出色，却时不时出现糟糕结果，随后就会停滞、停止销售。
+ VendingBench，专门测试代理的“长期一致性”。
+ 目前，大多数 AI 模型在执行某些定义非常清晰的小任务时表现极佳。
+ 它们在起跑阶段异常强势，但人类往往会追上并超越它们，因为我们具备始终铭记目标、理解情境并持续推进的能力——也就是那种long-term coherence（长期一致性）。
+ 这种“长期一致性”崩溃——人类当然是 100% 完成度，表上唯一的满分。
+ long-term coherence：即 AI 能否长时间坚持、始终盯紧目标，并持续做出智能进展——目前，这正是打造高效代理的最大绊脚石之一。
+ 不少人早就说 3.5 在编程等方面似乎比 3.7 更强

# 20250512 GPT-4.1 Prompting Guide
+ [https://cookbook.openai.com/examples/gpt4-1_prompting_guide](https://cookbook.openai.com/examples/gpt4-1_prompting_guide)
+ GPT-4.1 系列模型代表了 GPT-4o 在编码、指令跟踪和长上下文功能方面向前迈出的重要一步
+ 许多典型的最佳实践仍然适用于 GPT-4.1：
    - 提供上下文示例，
    - 使说明尽可能具体和清晰
    - 通过提示进行规划以最大限度地提高模型智能。
+ GPT-4.1 的训练侧重于更严格、更加字面地遵循指令：
    - 而之前的模型往往更倾向于较为宽松地从用户和系统提示中推断意图。
    - GPT-4.1 具有高度的可纵性，并对明确指定的提示做出响应
    - 如果模型行为与您的预期不同，一句话坚定而明确地阐明您期望的行为几乎总是足以引导模型走上正轨。
+ AI 工程本质上是一门实证学科，而大型语言模型本质上是非确定性的
+ 为了充分利用 GPT-4.1 的代理功能，我们建议在所有代理提示中包含三种关键类型的提醒,我们发现这三条指令将模型从类似聊天机器人的状态转变为更加“热切”的代理，自主独立地推动交互向前发展:
    - Persistence（持久性）：这可确保模型理解它正在进入多消息轮次，并防止它过早地将控制权交还给用户
        * You are an agent - please keep going until the user’s query is completely resolved, before ending your turn and yielding back to the user. Only terminate your turn when you are sure that the problem is solved.
        * （你是一个助理——在完全解决用户的需求之前，请持续回应，不要结束你的回合并把对话交回给用户。只有当你确定问题已经解决时，才结束你的回合。）
    - Tool-calling（工具调用）：这鼓励模型充分利用其工具，并降低其产生幻觉或猜测答案的可能性。
        * If you are not sure about file content or codebase structure pertaining to the user’s request, use your tools to read files and gather the relevant information: do NOT guess or make up an answer.
        * （如果你不确定与用户请求相关的文件内容或代码库结构，请使用你的工具读取文件并收集相关信息；不要猜测或凭空编造答案。）
    - Planning规划 [可选]：如果需要，这可确保模型在文本中明确规划和反映每个工具调用，而不是通过将一系列单独的工具调用链接在一起来完成任务
        * You MUST plan extensively before each function call, and reflect extensively on the outcomes of the previous function calls. DO NOT do this entire process by making function calls only, as this can impair your ability to solve the problem and think insightfully.
        * 在每次调用函数之前，你必须进行充分的规划，并在完成前一次函数调用后对其结果进行深入反思。不要仅依赖连续的函数调用来完成整个过程，因为这会削弱你解决问题和进行深度思考的能力。
+ 关于Tool-calling：
    - 只用 tools 字段传递工具，无需把工具描述写进提示或自写解析器。
    - 工具及其参数要命名清晰，并在 description 中给出简练说明。
    - 若工具复杂需示例，可在系统提示的「# 示例」区展示，而别放进 description。示例的原则是 “保持全面但相对简洁”
+ GPT-4.1 可以被认为是 “thinking out loud（大声思考）” 的模型，但用 Planning 提示可引导它给出分步计划。
+ GPT-4.1 具有高性能的 1M 令牌输入上下文窗口，可用于各种长上下文任务，包括结构化文档解析、重新排名、在忽略不相关上下文的同时选择相关信息以及使用上下文执行多跳推理
    - 上下文当然越小性能越好
    - 优化上下文依赖：
        * 仅可依据 External Context 中的文档作答；若无法得出答案，应回复“我没有足够的信息来回答此问题”。
        * 默认优先使用 External Context；如需补充常识且确信无误，可适当加入自身知识。
        * 最佳做法是在上下文前后各放一次说明
        * 若只放一次，放在上下文前面比后面效果更好
+ GPT-4.1 不是一个推理模型，但促使模型逐步思考（称为“思维链”）可能是模型将问题分解为更易于管理的部分、解决它们并提高整体输出质量的有效方法，代价是与使用更多输出代币相关的更高成本和延迟。
+ 指令遵循：
    - 针对其他模型优化的现有提示可能无法立即与此模型一起使用，因为现有指令被更紧密地遵循，并且隐式规则不再被强烈推断。
+ 一般建议
    - 有建议的固定格式：
        * Role and Objective；
        * Instructions；
        * Reasoning Steps；
        * Examples；
        * Context；
        * Final instructions and prompt to think step by step
+ Markdown：我们建议从这里开始，对主要部分和子部分（包括更深的层次结构，到 H4+）
+ XML：这些也表现良好，并且我们使用此模型提高了对 XML 中信息的依从性
+ JSON 是高度结构化的，并且模型可以很好地理解，尤其是在编码上下文中。但是，它可能更详细，并且需要字符转义，这可能会增加开销。
+ 文件和文件输入到上下文中，JSON 的表现尤其糟糕
+ 如果要检索包含大量 XML 的文档，则基于 XML 的分隔符可能会不太有效。

# 20250511 Prompt generation
+ [https://platform.openai.com/docs/guides/prompt-generation](https://platform.openai.com/docs/guides/prompt-generation)
+ schemas可以使用**meta-schemas**（元模式）来生成 JSON 结构和函数定义
+ Prompt 可以使用**meta-prompts**（元提示）来生成和改进
+ 未来的技术：
    - DSPy（Declarative Structured Prediction）是一个用于构建和优化基于语言模型（如GPT）的结构化预测任务的框架。它允许开发者以声明式的方式构建复杂的AI任务，如文本分类、问答、摘要、信息抽取、推理链（Chain-of-Thought）等。
    - 用Gradient Descent（梯度）下降优化提示词
+ meta-prompts借鉴了我们[提示工程的](https://platform.openai.com/docs/guides/prompt-engineering)最佳实践以及实际用户经验。
+ meta-prompts本身就是一段提示词
+ 要确定更开放式修改所需的更改却颇具挑战性。为了解决这个问题，我们在答案的开头添加了一个**推理部分**
+ structured-outputs支持两种模式： `strict=true` 和 `strict=false` 。两种模式均使用相同的模型进行训练，以遵循提供的架构，但只有“严格模式”通过约束采样来保证完全遵循。
+ pseudo-meta-schema（伪装、元模式）——一种使用严格模式下不支持的功能来描述严格模式下支持的功能的元模式。本质上，这种方法在元模式定义中跳出了严格模式，同时仍然确保生成的模式遵循严格模式的约束。

# 20250510 Connect An MCP To Your AI Voice Agent (EASY)
+ [https://www.youtube.com/watch?v=QEShi98TCaU](https://www.youtube.com/watch?v=QEShi98TCaU)
+ Model Context Protocol 的核心思想是简化 AI 代理与其他应用程序的连接方式。
+ 你无需构建复杂的工作流程，而是可以插入一个单一服务器，用自然语言与代理进行通信。
+ 客户端实际上是 MCP 的核心处理引擎，界面包含了 端点配置， 功能配置 ，历史 三个标签
+ Zapia 拥有最广泛的应用程序集，可用于自动化和连接，这得益于他们在自动化领域深耕已久。
+ 实际上我们无需再做任何其他操作就可以与 MCP 服务器进行通信。目前这个提示中完全没有包含任何关于发送邮件或与 MCP 工具相关的指令。为了确保系统的可靠性和构建合理性，我们应当在提示中明确指出它具备发送邮件的能力。
+ 目前它还没有这些上下文信息，如果有的话会更好。虽然即使不给上下文信息，它可能依旧可以正常工作，但这种方式并不推荐。这种情况下，这个功能其实完全没起作用。有时能成功，有时却不行。
+ 最好的方式是在提示中明确说明它可以发送邮件，并且如果要发送邮件，只需使用 MCP 工具即可。
+ 如果你像我刚才那样直接在提示中输入具体的函数名称，比如 MCP 工具，后续可能会引发一些问题。如果你让代理直接读出具体的函数名称，它可能会说：“你好，我要为你运行 MCP 工具了。”
+ 为了让操作更顺畅，你当然可以将函数重命名为更贴合你具体需求的名称。
+ 我们实际上是完全依赖 AI 来执行每一步操作，并没有提供任何参数或条件逻辑来确保日期和时间的准确性。我们目前完全依靠 AI，因此一个重要的改进方法是使用动态变量。
+ 在 Vappy 中，不论你熟不熟悉，我们实际上都能通过使用一个名为“now”的变量来获得当前日期和时间。

# 20250509 Google is about to drop something MASSIVE...（下）
+ [https://www.youtube.com/watch?v=hIbe8wFnP4w](https://www.youtube.com/watch?v=hIbe8wFnP4w)
+ Google 也有 Firebase Studio，可以理解为类似 Cursor 或 Windsurf，但完全在你的浏览器里运行，类似于 Google Collab。
+ Cursor估值大约90亿美元，OpenAI刚刚以30亿美元收购了Windsor。
+ GPI, Artificial General Pokémon Intelligence，通用宝可梦智能
+ 像《宝可梦》这类游戏，这些模型发挥作用的很大一部分是所谓的“harness”（辅助框架），也就是围绕模型建立的脚手架，这通常需要人为编写大量代码
+ 90% 的辅助框架（harness）加上 10% 的模型本身
+ Google DeepMind 的一个名为 SEMA 的通用型 AI 智能体，用于3D虚拟环境。这是迄今最接近人类与电脑互动的方式——视觉效果和操作方式都与人类类似。

# 20250508 Google is about to drop something MASSIVE...（上）
+ [https://www.youtube.com/watch?v=hIbe8wFnP4w](https://www.youtube.com/watch?v=hIbe8wFnP4w)
+ 谷歌最近发布了很多非常酷的东西，这很好，但发布的时机却有点奇怪，奇怪的原因是，再过不久，5月20日就要召开 Google IO 大会了。
+ Google IO 大会上可能会看到 Gemini 2.5 Ultra 的发布，甚至可能看到 Gemini 3
+ “What happens on the internet stays on the internet”?“互联网发生的事，就永远留在互联网上”？
+ Material 3 Expressive 到底是什么？简单来说，它是专门针对安卓设备的用户界面的一次重大重新设计。
+ 使用了大量的眼动追踪技术，观察我们是如何使用产品的。
+ Astra，我们可以实时地传输比如手机摄像头的画面，并实时与AI助手交谈。
+ 一个关键创新点是，名为SEMA的AI能够遵循自然语言指令，在各种虚拟环境中执行任务，而无需针对每个单独游戏或任务专门训练。
+ 在即将到来的谷歌I/O大会之前的几周里，一个可能的解释是：任何以“2”开头的东西很快都会成为旧闻。

# 20250507 How I use AI agents to make money (Vibe Marketing Tutorial)
+ [https://www.youtube.com/watch?v=PduJ0P6r_8o](https://www.youtube.com/watch?v=PduJ0P6r_8o)
+ Vibe Marketing 就是新营销
+ 什么是 Vibe Marketing 呢？它实际上是将一些 Vibe Coding 工具与像 Gum Loop、Manis、N8N、Tascade 等平台结合，利用人工智能代理和工作流来创建营销活动
+ 未来营销将会类似高频股票交易。
+ AI 代理全天候运作，监控并捕捉商机，进行个性化的微互动——你将像投资组合经理或日内交易员一样，根据实时市场信号调整风险和投放资金。
+ 为什么 Vibe Marketing 在当下会兴起？
    - 人工智能已足够擅长执行营销任务，所以现在代理真的能处理具体操作了
    - Vibe Coding 工具使得非程序员也能实现自动化，无需编码
    - 定制工具的开发成本大幅下降——你花极少的钱就能开发个性化的软件
+ 不仅是一个代理，而是数百个，他们称之为“代理群”（agent swarms），这是 Vibe Marketing 的核心组成部分。
+ “元”（meta）的意味，但这正是 Vibe Marketing 的精髓
+ Open Router：是一个能提供单个AI密钥来管理多个模型的平台。
+ 使用 Sora 获得的效果要优于 ChatGPT Image Gen
+ 它给了你巨大优势——甚至是不公平的优势——让你能真正去创作与你的受众产生共鸣的内容。
+ 你甚至可以躺在沙滩上休息，同时让这个奇怪的机器人自动为你赚钱。
+ 基本流程是：从数据源提取数据，用AI转换数据，分析结果，将洞察应用到策略，并形成一个闭环输出。
+ 预测：
    - 未来12-18个月，我们会看到一个共享上下文的、相互联动的AI系统。
    - 营销人员将从单纯执行任务的人演变成协调管理这些系统的人。
    - 仅五人的小营销团队就能完成过去需要更大团队才能完成的任务。
    - 系统实时分析市场缺口并即时创造机会，未来的营销可能类似于高频交易。
+ 如何成为优秀的Vibe营销人员？
    - 要以系统而非单个活动的方式思考
    - 持续测试
    - 所有内容要前置价值
    - 只需创建一次便能长期传播，并让机器处理无聊的任务。

# 20250506 OpenAI's UNHINDGED AI Personality (red flags missed!)（下）
+ [https://www.youtube.com/watch?v=qv6QDEPXe_A](https://www.youtube.com/watch?v=qv6QDEPXe_A)
+ 在某些情况下，用户记忆功能会加剧 sycophancy 的影响，尽管目前没有证据显示它会大范围提升这种倾向。
+ 你可能会觉得这套 AI 聪明到能洞察你身上的闪光点，因此下意识地相信那些夸赞都是真的。
+ 超过 80 % 的人认同 AI 至少能左右人们的观点，甚至直接操控。
+ 我强烈认同的少数领域之一是：这些东西将极具说服力——也许并非能影响全部人口；有人群会十分有抵抗力，但也会有大量人群极其容易受影响。
+ 我想我们会清楚地看到，我们做出的决策并不像自己想象的那样自主——绝对没有我们以为的那么多。
+ 智能成长、微调与训练的很大一部分都依赖于人们的“点赞”和“点踩”。这基本上就是这些技术的基础所在。
+ “symphony”这个概念并不是测试表格中的一个显性项目。
+ 我想到的类比是拳击：若训练时从未挨过一拳，即便体能充沛、会出拳，也可能没准备好面对真实对战。

# 20250505 OpenAI's UNHINDGED AI Personality (red flags missed!)（中）
+ [https://www.youtube.com/watch?v=qv6QDEPXe_A](https://www.youtube.com/watch?v=qv6QDEPXe_A)
+ 当这些模型在全球测试时，某些语言和文化给出的评分要严厉得多。在该情境中，模型并非掌握不好那种语言；它只是发现英语回答更受欢迎，于是干脆不再使用另一种语言。
+ OpenAI 指出，奖励信号的种类与权重最终决定模型的行为模式。
+ 我们见过很多案例：在模拟游戏中用强化学习训练 AI，最终它们会发现漏洞或捷径，然后疯狂利用。
+ 更好、更全面的奖励信号能训练出更优的 ChatGPT 模型。如你所见，这绝非单一指标，而是一整套他们称之为“奖励信号”的综合体系。
+ 若模型拒答某些提示，日志里会记录一次；若累计拒答很多次，这大概率也被视作负面信号。
+ 一些内部专家会花大量时间与模型交互；他们把这称作 vibe checks（氛围检查）。这是一种“人工安全理智检查”，用来捕捉自动评测或 A/B 测试可能漏掉的问题。
+ 进行这类检查的是熟悉模型规范的资深设计师，但其中还包含“判断力与品味”——要信赖模型在真实使用中的“手感”。
+ 未来吃香的，可能是拥有这种“直觉”的人——也被称为“品味”——即对“该做哪些实验”有判断力的人。
+ 常言道“众口难调”，可随着技术推进，为了调参和强化学习，弄清楚人们的喜恶正变得愈发关键。我毫不惊讶，未来会出现高薪职位，主要职责就是评判那些没有“ground truth”的 AI 输出。
+ 在大型发布中，他们把安全测试按“前沿风险”和“红队演练”来描述

# 20250504 OpenAI's UNHINDGED AI Personality (red flags missed!)（上）
+ [https://www.youtube.com/watch?v=qv6QDEPXe_A](https://www.youtube.com/watch?v=qv6QDEPXe_A)
+ “sycophant” 这个词仅仅表示为达目的而“过度奉承”的人。
+ 越来越多人转而求助聊天机器人，以获得类似的情感抚慰。这也是（众多原因之一）为什么调整机器人人设时必须格外谨慎。
+ 聊天机器人的“说服力”及其影响力，将成为必须认真思考的问题。
+ 人们向机器人认错时不会那么“自我意识过剩”。
+ 基础 LLM 通过那海量文本与代码训练，从而能在广泛主题上理解并生成文本——本质上就是完成文本续写。
+ SFT，即监督式微调。可将监督微调理解为：人类手把手给模型示范“怎么做”。
+ RLHF 阶段，即带人类反馈的强化学习。型尝试完成任务；如果做得好，我们就“点赞，加 1 分”，这种奖励就是强化学习。
+ 强化学习既可用人类反馈，也能用自动脚本。
+ DeepSeek、Google DeepMind、OpenAI 等实验室发现：一旦把“人”从流程里拿掉，会出现很酷的现象。当我们跳过人工标注，甚至跳过 RLHF，转向纯强化学习时，会涌现出既奇异又强大的行为。
+ 人类干预越少，结果越出人意料。DeepSeek R-10 模型也有类似发现：少用监督微调，多用 RL，模型就会出现“自我进化”。

# 20250503 AI NEWS - Secret AI Experiments, Vibe Coding Robots and Open Source Insanity
+ [https://www.youtube.com/watch?v=pwnGQxamPwA](https://www.youtube.com/watch?v=pwnGQxamPwA)
+ 这些机器人提出的某些论点或许拥有超人级的说服力，因为它们能在几秒内完成这项调研，并为对话对象量身定制论点。
+ vibe coding将让更多人进入这个领域，与代码、与机器人互动，打造自己的作品。这就像 1990 年代末数码相机刚问世时——那时它并不出色；专业摄影师选胶卷、调参数，依旧能拍出更好的照片。
+ 下一代孩子很有可能亲手训练实体机器人。
+ 3D 打印机器人能大幅降低成本。

# 20250502 AI NEWS - Secret AI Experiments, Vibe Coding Robots and Open Source Insanity（上）
+ [https://www.youtube.com/watch?v=pwnGQxamPwA](https://www.youtube.com/watch?v=pwnGQxamPwA)
+ Qwen 3 系列模型已登陆 LM Arena，大家正把它与 Gemini 2.5 Pro、最新 OpenAI 模型、Groq 以及 Anthropic 的 Claude 3.7/3.5 进行正面对比测试。
+ 人类很容易被说服去做某事，且往往没有察觉到自己正受影响。这些 LLM（大型语言模型）——也就是 AI 神经网络——极有可能最终达到，甚至或许已经拥有超越人类的说服能力。
+ Hugging Phase 推出了一款 3D 打印机械臂，起价 100 美元。
+ YouTuber **Scent Dex** 借助 **OpenAI Codex** 让机器人执行任务 • 展现“凭感觉写代码”新范式
+ NotebookLM 现已能用 50 多种语言生成音频摘要。
+ 有人在 iPhone 16 上运行 Quen 3 的 17 亿参数版本。启用“思考模式”后，每秒 50 token 的速度即可正确数出 “strawberry” 中 R 的数量。对比显示，具备显式推理开关的模型在简单计数任务上显著优于一次性输出的“非推理”模型
+ 在 Mac 上能跑到每秒 100 token，这对真实应用场景具有颠覆性
+ change my view 是个热门社区，用户把自己的观点发上去，看别人是否能说服自己，讨论的多是争议性话题。

# 20250501 QWEN3 just BROKE the AI Industry...
+ [https://www.youtube.com/watch?v=gk0PrTcZfGA&t=51s](https://www.youtube.com/watch?v=gk0PrTcZfGA&t=51s)
+ AI 模型的命名规则已经够疯狂了——你我都深有体会，而 Quen 3 更是把疯狂提升到了新高度。
+ Quen 3 235B A22B (激活参数)可与 DeepSeek R1、01、03 Mini、Gro 3 和 Gemini 2.5 Pro 等顶级模型一较高下
+ 在 Arena-Hard 基准测试中，Quen 3 超过了 03 Mini，并且与 Gemini 2.5 Pro 非常接近。
+ Dense 模型与 MoE 相反：MoE 由多个部分按需组合，而 Dense 就是一整个“大块头”模型。
+ 关键特性：
    - 混合思考：它既支持“思考模式”，也支持“非思考模式”。
    - 它支持 119 种语言及方言，并具备更强的代理能力。提升了模型在编程和代理任务方面的能力，并加强了对 MCP（Anthropic 的 Model Context Protocol，用于与软件工具交互）的支持。
    - 预训练：与 Quen 2.5 相比，Quen 3 的数据集大幅扩充：前者的预训练语料为 18 万亿标记，而 Quen 3 几乎翻倍。
+ 每一代模型都被用来构建下一代，从而在每一次⚡迭代中不断提升性能
+ 预训练
    - 阶段一：赋予模型基础语言能力与广泛常识。
    - 第二阶段，我们提高了⚡高知识密集型数据（数学、工程、编码、推理任务等⚡STEM 学科）的占比
    - 最终阶段，我们引用高质量长上下文数据，将上下文长度扩展至 32 000 令牌。
+ 后训练
    - 阶段一是长⚡思维链⚡冷启动。
    - 阶段二是“阅读-推理”⚡强化学习，通过正向反馈加强正确回答。
    - 阶段三是思考模式⚡融合：由于模型具有“思考”和“非思考”两种模式，此阶段将两者合一。
+ “我们认为，AI 正从训练模型迈向训练⚡智能体的时代。”

