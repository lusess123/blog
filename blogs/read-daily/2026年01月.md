# 20260103 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（中）
+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 要得到更好的模型，你总得在某个地方付出代价。你要么需要生成更好的数据、在数据上花更多时间，要么需要在训练上花时间，要么需要在推理上花时间。
+ **RAG** 的好处是它基本能用，但任何更好的方案都会更贵。
+ LLM有固定的容量：每个参数可以存储 3.6 比特，所以10亿参数能存储4GB。存储无关知识会占用宝贵空间
+ 微调：
    - 会完美记忆训练数据，前缀微调的方式可以做到0损失，做到125倍的压缩比
    - 会丧失泛化能力（无法写诗）
    - 会覆盖原来就有的知识
    - 只能复述原文
+ 合成数据：
    - 他们拿一个小数据集，生成一个非常大的、更多样化的数据集，这个数据集能代表他们关心的内容。
    - 即使你没有很多数据，如果你愿意生成一个描述你现有数据的大型合成数据集，你实际上可以在上面训练模型，而且效果很好。
    - **Self-Edit**（自适应语言模型），缩写是 **SEAL**。他们让模型决定生成什么数据来让自己变得更好。在一些受限场景下，这实际上是有效的。这相当诡异。
    - 我们想把东西训练进权重。我们可以生成大型合成数据集来描述非常小的数据集，而且效果不错
+ 当更新整个参数的时候会发生**catastrophic forgetting**（灾难性遗忘）：这是一个存在很久的问题，但没人真正知道怎么解决。这非常非常难。
+ 我们有这个巨大的模型，我们在上面添加一小部分来控制它，只训练那些参数。这样我们保留了模型中的大部分信息
    - **Full fine-tuning**：更新所有参数（容易遗忘）
    - **Prefix Tuning**：只训练 KV 缓存
    - **Memory Layers**：大型查找表
    - **MoE 扩展**：在混合专家模型上添加专家
    - **LoRA**：低秩适配，只训练小矩阵
+ what properties we want？
    - 轻量级
    - Resistant to forgetting (抗遗忘性)
    - High capacity (高容量)
    - 快速推理
+ **Thinking Machines** 的新训练 API。它基本上建立在这个想法上：只要你使用 **LoRA** 并批量处理 **LoRA**，你就可以为每个人提供一个模型。
+ **LoRA 的权衡**："学得少，忘得也少"——表达能力较弱但保留更多原有知识
+ **RL vs SFT**：强化学习可能更适合参数高效方法，SFT 可能需要全量微调



# 20260102 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（上）
<!-- 这是一张图片，ocr 内容为：CONTEXT ROT:HOW INCREASING INPUT TOKENS IMPACTS LLM PERFORMANCE REPEATED WORDS-PERFORMANCE BY INPUT LENGTH(TOKENS) CLAUDE SONNET 4 1.0 GPT-4.1 QWEN3-32B GEMINI 2.5 FLASH 0.9 AVERAGE NORMALIZED LEVENSHTEIN SCORE 0.8 0.7 0.6 0.5 102 104 103 INPUT LENGTH(TOKENS) -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767334806619-7a11663c-78f8-411c-9d30-d0a45eb5b312.png)

+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 那些更小众的、或者我称之为"长尾"的任务，对 ChatGPT 来说真的很难
+ 如何把知识注入到模型的参数里？
    - full context
    - RAG
    - weights：非常新、目前还没人做的
+ "context rot（上下文腐烂）"：上下文加东西时性能数量级的下降，：
    - 1000 token 上下文时，每秒能输出 1 万 token；128k token 上下文时，每秒只能输出 130 个
    - 问题出在一个叫"self attention"的小模块上
    - 所有输入的词都需要互相"看"对方，这造成了二次方的依赖关系。4 个 token，矩阵有 16 个元素；12 个 token，就有 144 个。
+ 计算量和模型性能之间存在固有的权衡：让模型在输入那么多 token 时"不崩溃"和让模型真正能在大量 token 间正确推理，这是两码事
+ 一旦1 万 token，模型基本就不工作了。虽然没崩溃——输出的东西语法正确、看起来有意义——但实际上没解决问题。
+ context rot方面，claude 表现最好，但评测并不好，但实际使用上是真的好
+ MiniMax M2 选择了全注意力机制
+ 向量数据库的问题：
    - 使用向量数据库没有安全优势，而且它们在大规模运行时很难。
    - 不具有适应性。所以在嵌入的时候需要加入上下文，有额外的token消耗
    - 有一些关系是无法被固定维度的向量捕获的，你必须进行推理才能回答所有可能的任务。
    - 需要跨文档"潜在推理"的问题无法解决
    - 那些文档中隐含但没有明确回答的问题，都无法被当前技术解决
+ agentic search ：它是一个可以抓取信息的模型，连续做一堆查询然后回复。并非RAG

# 20260101 How Claude Code Works - Jared Zoneraich, PromptLayer 下
+ [https://www.youtube.com/watch?v=RFKCzGlAU6Q](https://www.youtube.com/watch?v=RFKCzGlAU6Q)
+ skill:一个可扩展的system prompt
+ 统一差异格式（**Unified diff**）是一项agent的标准。
+ 未来的范式
    - 将推理模型作为一种工具来使用，
    - 我觉得我们可以从“to-do lists”中学到很多
    - Skills
+ AI therapist problem，这类问题没有“全局最大值”（即唯一最优解）
+ Codex
    - 与 **Claude Code** 非常相似，采用相同的主循环架构。有趣的是，它的核心是用 **Rust** 编写的，并且是开源的。
    - 它更多是由事件驱动的；在并发线程、提交队列和事件输出方面做了更多工作。
+ AMP
    - 愿景：我们如何构建一个能在“对智能体最友好”的环境中运行的助手？
    - 上下文管理：相比 压缩，移交（**handoff**）开启一个新线程，并把必要的信息传给它，速度很快。压缩是「有损压缩」，移交是「精准打包」
+ Cursor
    - “界面优先”而非“命令行优先”的
    - 新模型 **Composer** 经过了蒸馏。他们拥有核心数据。
+ 基准测试（**Benchmarks**）几乎没用。基准测试已经变成了营销手段。
+ 有一种可能性是我们将开始在更高的抽象层级构建代理，并且只是依赖 **Claude Code** 和这些其他代理来做很多框架和编排工作。
+ 不同的视角在代理中很重要。但不同的视角很重要，因为有不同的方法来解决问题，没有一种比另一种更好，你可能想要一个混合专家代理。
+ headless  Claude Code 的强大之处，可以自动化很多工作流程

