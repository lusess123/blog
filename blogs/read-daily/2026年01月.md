# 20250110 How Tolan builds voice-first AI with GPT-5.1
[https://openai.com/index/tolan/](https://openai.com/index/tolan/)

<!-- 这是一张图片，ocr 内容为：'WHAT TRIPS DOES THE USER HAVE COMING UP?" EMBED QUESTIONS USER:'I M SO "WHAT IS THE USER DOING THE WEEK OF 2025-11-02?" &QUERY MEMORY EXCITED FOR MY TRIP SYNTHESIZE QUESTIONS THIS WEEKEND" VECTOR DB "WHAT DOES THE USER LIKE TO DO?"  TOLAN: "I KNOW. MERGE RESULTS CAMPING WITH STEVEN SETS WITH MEAN IN YOSEMITE IS GOING RECIPROCAL RANK TO BE EPIC!" USER:"FOR OUR NEXT STORE:"EVAN HOPES TO COMBINE,EDIT, CLUSTER MEMORIES BY COMPRESS TRIP,I REALLY HOPE REFLECT GO TO ICELAND WITH REFINE MEMORIES EMBEDDING(KNN) WE GO TO ICELAND* HIS FRIEND STEVEN'' WITHIN  CLUSTER -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767999394394-154365da-c13e-44a2-903d-3f62a4f0ea8d.png)

+ Tolan 随着时间的推移，它会从对话中学习。
+ 语音 AI 提高了延迟和上下文管理的门槛，但也比文本实现了更开放、探索性的互动。团队将精力集中在两个关键杠杆上：记忆和角色设计。
+ OpenAI GPT-5.1 和 Responses API 的推出将语音启动时间缩短了 0.7 秒以上
+ 上下文处理：与许多在多轮对话中缓存提示的智能体不同，Tolan 每一轮都从头重建其上下文窗口.这种架构使 Tolan 能够实时适应突然的主题转变，这是自然语音交互的基本要求。
    - 最近消息的摘要
    - 角色卡片
    - 向量检索的记忆
    - 语气指导
    - 实时应用信号
+ 缓存提示根本行不通.实时重建方法既是技术密集型的，也是 Tolan 成功的基础
+ Tolan 构建了一个记忆系统，不仅保留事实和偏好，还保留情感上的"氛围"信号——这些线索有助于引导 Tolan 该如何回应
    - text-embedding-3-large 模型嵌入
    - Turbopuffer ：一个高速向量数据库，支持低于 50 毫秒的查询时间
    - 系统根据聊天信息生成“多个提问”来触发记忆召回（通过生成多角度的子问题来提高召回率）
    - 每晚压缩任务，移除低价值或冗余的条目来解决记忆矛盾
+ 科幻作家撰写角色支架来进行个性管理
+ 一个并行系统监控对话的情感基调（tenor），并动态调整 Tolan 的表达方式
+ GPT-5.1 的过渡是一个转折点，分层的提示指令——语气支架、记忆注入、性格特征——被更忠实地遵循了。不需要变通方案
+ 记忆召回失误下降了 30%（基于产品内的挫败信号），在 GPT-5.1 驱动的角色上线后，次日用户留存率上升了超过 20%。
+ 通过 summary threshold（摘要阈值）来创建聊天摘要
+ “用户回复后” 再做 提取，反思（kNN），压缩

# 20250109 Automating Large Scale Refactors with Parallel Agents - Robert Brennan, AllHands
[https://www.youtube.com/watch?v=rcsliSIy_YU](https://www.youtube.com/watch?v=rcsliSIy_YU)

<!-- 这是一张图片，ocr 内容为：EVOLUTION OF AI-DRIVEN DEVELOPMENT PLUGINS CLOUD AGENTS ORCHESTRATION IDES LOCAL AGENTS OPENHANDS CLOUD SDK CLI CODEX CLINE GEMINI JULES W WINDSURF CLAUDE GITHUB CURSOR DEVIN CODE COPILOT AGENTIC TACTICAL MEDIAN DEVELOPER EARLY ADOPTER -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767917963431-96ad5e8d-5217-41e7-b4e1-316b9650b9e4.png)

+ 有大量繁琐的工作涉及技术债务、代码维护和代码现代化。这些任务非常适合自动化.但它们往往规模太大，不适合单次一把梭
+ OpenDev：我们意识到，这不应该在黑盒子里发生。如果我们的工作将要改变，我们希望这种改变是由软件开发社区驱动的，我们希望在这种改变中拥有发言权。
+ 即使你今天把大语言模型冻结住、不让它们再进步，未来两三年软件工程这份工作仍然会发生巨大变化——因为我们还在摸索如何将这项技术落地。
+ Evolution of AI-Driven Development：
    - **Plugins → IDEs → Local Agents → Cloud Agents → Orchestration**
    - **Tactica** -》 **Median Developer** -〉**Early Adopter** -》Agentic
+ agent 创造 agent。我认为这是当前能力的最前沿。人们刚开始试验这个，刚开始在规模化层面看到成功，但确实有一些任务非常适合这种工作流。它有潜力真正自动化掉每个当代软件公司底下堆积如山的技术债务。
+ 对于 **Open Hands**，我们一开始主要做云 agent，后来稍微退了一步，构建了类似 **Cloud Code** 的本地 CLI，以便在开发者当前所在的位置与他们会面
+ 采用了编排的顶尖 1% 工程师，他们的生产力获得了巨大提升，能够处理其他团队根本顾不上的大量技术债务积压。通常是那些高度可重复、高度可自动化的任务。
    - 复整个代码库中的 CVE
    - 给 Python 3 代码库添加类型注解
    - 把 Java 单体应用拆分成微服务
    - 旧版 Java 迁移到新版 Java
    - **Spark 2** 任务迁移到 **Spark 3**
    - 把整个前端从 **Redux** 迁移到了 **Zustand**
+ 什么这些任务不能one shot it（一把梭）？
    - Agent Problems：
        * 上下文窗口有限
        * 偷懒
        * 缺乏领域知识
        * 错误会逐步累积
    - 人的问题
        * 难以传达隐性经验
        * 难以把大任务拆解成清晰步骤
        * **需要中途审查/阶段性同步（check-in）**
        * 完成”的标准不够明确（DoD 不清晰）
+ 我想说清楚：我们并不期望每个开发者都去做 agent 编排。在熟悉的环境中、在 IDE 旁边本地运行 **Cloud Code**，至少在未来几年会是常见的工作流。你不会看到所有软件工程都有 3000% 的生产力提升，大概只会得到大家报告的那 20% 的提升
+ 我认为这真的很难做对。这就是需要大量思考的地方——如何分解任务，以便我能验证每个单独的步骤，以便我能真正自动化整个过程，而不是最后得到一堆乱糟糟的代码
+ 如果你刚开始尝试这个，我建议把自己限制在大约 3 到 5 个并发 agent。我发现超过这个数量，你的大脑就会崩溃。但对于那些真正大规模采用编排的人，我们看到他们同时运行数百甚至数千个 agent

# 20260108  Ralph Mode for Deep Agents: Running an Agent Forever
[https://www.youtube.com/watch?v=yi4XNKcUS8Q](https://www.youtube.com/watch?v=yi4XNKcUS8Q)

<!-- 这是一张图片，ocr 内容为：RALPH MODE FOR DEEP AGENTS RALPH LOOP TASK "BUILD A PYTHON COURSE" ITERATION 1 HIGERT TEMIREG ITERATION 2 DEEPAGENT ITERATION 3 CTRL+C TO STOP OR MAX_ITERS REACHED FILESYSTEM (WORK PERSISTS) -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767829489126-9e1a18ac-cbce-42ff-88f1-4aaaa555307f.png)

+ Ralph loop：一个 agent 只靠一个简单的 **while loop** 加上 **file system**，究竟能做到什么
+ 文件系统就像“工作日志 + 记忆仓库”，保存每一轮迭代的产出和进展。
+ 现在想让 agent 做“长时间运行”的工作，最简单的方法之一，就是强制它一轮又一轮地继续跑这个循环。
+ agent 非常非常擅长用 **file system**。而文件系统之所以好，是因为如果再配合 **Git** 这种“历史记录”工具，它们就能把之前做过的工作一路追踪、记下来。
+ 归根结底，你基本上就是在一个不断更新的文件系统上，用同一个提示词一遍遍地循环运行 **Ralph**。
+ **Ralph** 会先创建一个待办事项列表（to-do list）。这是因为我们是基于 **deep agents** 构建的，而 **deep agents** 内置了这种原语，也就是我们的待办事项列表工具
+ 在这些长时间运行的 **agent** 任务中，上下文管理仍然很难做。
    - 压缩、摘要、卸载等手段都是为了减轻“上下文腐烂”（context rot）的问题
    - 如果我们重启循环，让 **Ralph** 看到“嘿，这些是大概做出的变更，这是我现在开始的地方”，那么我们就拥有了一个相对干净的上下文窗口，它可以继续在某种循环中工作。

# 20260107 Build a Prompt Learning Loop - SallyAnn DeLucia & Fuad Ali, Arize
[https://www.youtube.com/watch?v=SbcQYbrvAfI](https://www.youtube.com/watch?v=SbcQYbrvAfI)

<!-- 这是一张图片，ocr 内容为：1 OTHER ISSUE I'D LIKE TO MENTION DOMAIN EXPERTS TECHNICALUSERS AL ENGINEER DATA AL PRODUCT SUBJECT MATTER SCIENTIST MANAGER EXPERTS DEVELOPER RESPONSIBILITIES RESPONSIBILITIES DOMAIN PROMPT ENGINEERING CODE/AUTOMATION TRACK AND RUN EVALS PIPELINES/FRAMEWORKS APPLICATION PERFORMANCE/COSTS ENSURE PRODUCT SUCCESS -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767744561294-f353fb8d-a6ef-42dc-bc9a-c058af701d19.png)

+ agent会失败，很多时候不是模型弱，而是环境和指令弱。
    - 适应性与自我学习：系统没有从环境中沉淀/学习出稳定的“系统级指令”。
    - 确定性 vs 非确定性失衡：要么不做规划，要么规划太死板，缺少可控的灵活度。
    - 下文工程不到位：缺工具、缺工具使用指引、缺关键上下文（尤其是数据被预先裁剪后导致信息不全）。
    - 技术侧往往更擅长工程实现，却不一定懂“什么回答才对/什么体验才好”；
    - 业务侧最懂“什么才叫对”，却可能不擅长写代码、看 trace、调 pipeline。
+ RL 还要改权重
+ meta prompting 　只有分数，没有为什么和怎么改

# 20250106 AI Periodic Table Explained: Mapping LLMs, RAG & AI Agent Frameworks
[https://www.youtube.com/watch?v=ESBMgZHzfG0](https://www.youtube.com/watch?v=ESBMgZHzfG0)

<!-- 这是一张图片，ocr 内容为：15 62 54 61CKG. LETAIEML REACIVE VALID. MODILS LOWL P EM PAIMITIES PROMPTS EMBEDDINGS LLM LOU 2 FC 19 VX WU COMOSIT LONS FUNCEION GUORDRAILS RAG MULEIMODAL VECEOR CALL LOW了 BE FW FT AG WS DEPLOYMENT FRAMEWORY AGENT FINETUNE LED -KEAN SMALL LOU G MA TH LS SNISUAWE MULTI-AGONE SYNTHETIC THINKING INEE REE. -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767655054093-e099c2a1-54ab-4937-b44c-7fffdf39af0b.png)

+ Primitives
    - **Prompt** — 你给 AI 的指令
    - **Embeddings** — 把文本转成能比较相似度的数字向量
    - **LLM** — 能理解和生成语言的大模型
+ Compositions
    - **Function Calling** — LLM 调用外部工具获取真实数据
    - **Vector Database** — 存储和检索向量的数据库
    - **RAG** — 先检索相关文档，再让 LLM 生成答案
    - **Guardrails** — 防止 AI 输出不当内容的安全过滤器
    - **Multi-modal** — 能处理文本、图像、音频的模型
+ Deployment
    - **Agents** — 能自主规划、执行、迭代直到完成目标的 AI
    - **Fine Tuning** — 用特定数据训练模型，把知识烘焙进权重
    - **Framework** — 搭建 AI 系统的开发平台（如 LangChain）
    - **Red Teaming** — 模拟攻击来测试 AI 的安全漏洞
    - **Small Models** — 蒸馏后的轻量模型，快且便宜
+ Emerging
    - **Multi-agent** — 多个 AI 协作分工解决复杂问题
    - **Synthetic Data** — 用 AI 生成训练数据来训练 AI
    - **Interpretability** — 理解模型为什么这样做（打开黑盒）
    - **Thinking Models** — 会花时间推理的模型（如 o1）
+ **生产级 RAG**：Embeddings → Vector DB → RAG → Prompt → LLM + Guardrails
+ **Agentic Loop**：Agents ⟲ Function Calling → Framework
+ 下次有人向你推销一个炫酷的新 AI 功能、新 AI 产品、甚至 AI 创业点子时，试着把它映射到这张表上。他们在用哪些元素？他们在运行什么反应？他们是否缺少安全元素？他们是否过度设计了编排？他们是否在用思考模型，而小模型其实就能胜任？

# 20260105 The Tree That Thinks
<!-- 这是一张图片，ocr 内容为：WISDOM OFTEN HIDES IN UNGLAMOROUS PLACES. -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767618495495-1f17c907-1287-4744-80c9-f917a1531256.png)

+ [https://www.youtube.com/watch?v=JlJgacKxrwY](https://www.youtube.com/watch?v=JlJgacKxrwY)
+ 信息不断回到自身，不断放大、不断回响，形成一场巨大的自我强化噪音风暴，直到整个系统彻底崩溃。
+ 对任何思维系统的最大威胁，不是缺乏脑力或数据，而是无法从自身失控的反馈中挣脱出来。它真的会被自己的想法淹死。
+ 循环感觉像是一个巨大的「原来如此」时刻，好像一切都连上了，但它实际上是一种智识上的失忆症——系统已经完全忘记了原始想法是从哪里来的，只是在无尽地重新发现自己的回声。
+ 想要一切与一切同时连接，这其中有一种不成熟。真正的成长、真正的成熟是关于做出选择。是关于理解结构实际上给你自由，而不是拿走它。
+ 任何智能系统的终极目标：扎根于清晰的目标，向丰富复杂的意义世界分支延伸，最重要的是，有纪律为自己创造结构，这样它就不会成为自己聪明才智的受害者。

# 20260104 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（下）
+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 想把信息注入模型：
    - RL
    - SFT
    - LoRA
    - Prefix Tuning
    - 记忆层
+ 当你在规模上做这件事时，什么最高效？我不太确定，但我认为 **Prefix Tuning** 是一个很好的候选，因为 KV 缓存现在非常常用。
+ 记忆层。这是另一种将数据注入模型的方法，我认为很好。这就像在 MLP 上添加一个专家，但这个专家只是一个巨大的可微分查找表。记忆层很酷的一点是它是可控的
+ 现在有很多相互矛盾的证据。有人认为 **LoRA** 好，有人认为 **Prefix Tuning** 好。这些人认为记忆层好。我真的不确定，但我认为会是其中之一
+ RL 的作用是调整模型的行为模式（比如让它更礼貌、更谨慎），而不是往模型里灌入大量新的事实性知识
+ **RL vs SFT 参数需求**：RL 因为学习信号稀疏（只有 0/1），比 SFT 需要少 1000 倍的参数
+ 在很长一段时间内，基本上没有你只训练模型而从不做 **RAG** 的场景。我认为你会两者都做。也许如果你有大量文档，也许每天你做一次大训练，然后每次服务时你也做 **RAG**
+ RAG的根本问题在于chunking（分块）：即使retrieval（检索）做的足够好，也只是找到需要的一部分信息，无法真正在所有内容上进行推理。在极限情况下，有些类型的数据无论你怎么分块，你永远不会得到你需要的一切。
+ **LoRA**，也许你给每个用户训练几兆字节。在现实的短期内，可能更像是每天更新一次
+ 当你开始看到对数据集更稀疏的更新，或者有一些新数据进来但不是很多且相当频繁时，你可能想转向更接近 **Deep Research** 的推理时方法。
+ 做的好的话，提示词是隐含的。在理想情况下根本不需要提示词。把提示词压缩进权重，产生一个不需要提示词就能给出相同输出的模型，然后推理成本更低
+ 一旦你开始对模型进行多次更新，特别是当你有相互矛盾的信息时你怎么办。我认为最优的合成数据策略是在训练过程中以某种方式解决这个问题
+ 联邦学习：我认为它很难的一个问题是现在模型太大了，网络成本太高
+ 创建一个什么都不知道的模型真的很难。所以我更倾向于专业化的模型，擅长你关心的东西但不擅长其他东西，而不是倡导一个什么都不擅长的模型。
+ **时间元素**：如何处理不同时间的信息是未解决的研究问题

# 20260103 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（中）
+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 要得到更好的模型，你总得在某个地方付出代价。你要么需要生成更好的数据、在数据上花更多时间，要么需要在训练上花时间，要么需要在推理上花时间。
+ **RAG** 的好处是它基本能用，但任何更好的方案都会更贵。
+ LLM有固定的容量：每个参数可以存储 3.6 比特，所以10亿参数能存储4GB。存储无关知识会占用宝贵空间
+ 微调：
    - 会完美记忆训练数据，前缀微调的方式可以做到0损失，做到125倍的压缩比
    - 会丧失泛化能力（无法写诗）
    - 会覆盖原来就有的知识
    - 只能复述原文
+ 合成数据：
    - 他们拿一个小数据集，生成一个非常大的、更多样化的数据集，这个数据集能代表他们关心的内容。
    - 即使你没有很多数据，如果你愿意生成一个描述你现有数据的大型合成数据集，你实际上可以在上面训练模型，而且效果很好。
    - **Self-Edit**（自适应语言模型），缩写是 **SEAL**。他们让模型决定生成什么数据来让自己变得更好。在一些受限场景下，这实际上是有效的。这相当诡异。
    - 我们想把东西训练进权重。我们可以生成大型合成数据集来描述非常小的数据集，而且效果不错
+ 当更新整个参数的时候会发生**catastrophic forgetting**（灾难性遗忘）：这是一个存在很久的问题，但没人真正知道怎么解决。这非常非常难。
+ 我们有这个巨大的模型，我们在上面添加一小部分来控制它，只训练那些参数。这样我们保留了模型中的大部分信息
    - **Full fine-tuning**：更新所有参数（容易遗忘）
    - **Prefix Tuning**：只训练 KV 缓存
    - **Memory Layers**：大型查找表
    - **MoE 扩展**：在混合专家模型上添加专家
    - **LoRA**：低秩适配，只训练小矩阵
+ what properties we want？
    - 轻量级
    - Resistant to forgetting (抗遗忘性)
    - High capacity (高容量)
    - 快速推理
+ **Thinking Machines** 的新训练 API。它基本上建立在这个想法上：只要你使用 **LoRA** 并批量处理 **LoRA**，你就可以为每个人提供一个模型。
+ **LoRA 的权衡**："学得少，忘得也少"——表达能力较弱但保留更多原有知识
+ **RL vs SFT**：强化学习可能更适合参数高效方法，SFT 可能需要全量微调



# 20260102 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（上）
<!-- 这是一张图片，ocr 内容为：CONTEXT ROT:HOW INCREASING INPUT TOKENS IMPACTS LLM PERFORMANCE REPEATED WORDS-PERFORMANCE BY INPUT LENGTH(TOKENS) CLAUDE SONNET 4 1.0 GPT-4.1 QWEN3-32B GEMINI 2.5 FLASH 0.9 AVERAGE NORMALIZED LEVENSHTEIN SCORE 0.8 0.7 0.6 0.5 102 104 103 INPUT LENGTH(TOKENS) -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767334806619-7a11663c-78f8-411c-9d30-d0a45eb5b312.png)

+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 那些更小众的、或者我称之为"长尾"的任务，对 ChatGPT 来说真的很难
+ 如何把知识注入到模型的参数里？
    - full context
    - RAG
    - weights：非常新、目前还没人做的
+ "context rot（上下文腐烂）"：上下文加东西时性能数量级的下降，：
    - 1000 token 上下文时，每秒能输出 1 万 token；128k token 上下文时，每秒只能输出 130 个
    - 问题出在一个叫"self attention"的小模块上
    - 所有输入的词都需要互相"看"对方，这造成了二次方的依赖关系。4 个 token，矩阵有 16 个元素；12 个 token，就有 144 个。
+ 计算量和模型性能之间存在固有的权衡：让模型在输入那么多 token 时"不崩溃"和让模型真正能在大量 token 间正确推理，这是两码事
+ 一旦1 万 token，模型基本就不工作了。虽然没崩溃——输出的东西语法正确、看起来有意义——但实际上没解决问题。
+ context rot方面，claude 表现最好，但评测并不好，但实际使用上是真的好
+ MiniMax M2 选择了全注意力机制
+ 向量数据库的问题：
    - 使用向量数据库没有安全优势，而且它们在大规模运行时很难。
    - 不具有适应性。所以在嵌入的时候需要加入上下文，有额外的token消耗
    - 有一些关系是无法被固定维度的向量捕获的，你必须进行推理才能回答所有可能的任务。
    - 需要跨文档"潜在推理"的问题无法解决
    - 那些文档中隐含但没有明确回答的问题，都无法被当前技术解决
+ agentic search ：它是一个可以抓取信息的模型，连续做一堆查询然后回复。并非RAG

# 20260101 How Claude Code Works - Jared Zoneraich, PromptLayer 下
+ [https://www.youtube.com/watch?v=RFKCzGlAU6Q](https://www.youtube.com/watch?v=RFKCzGlAU6Q)
+ skill:一个可扩展的system prompt
+ 统一差异格式（**Unified diff**）是一项agent的标准。
+ 未来的范式
    - 将推理模型作为一种工具来使用，
    - 我觉得我们可以从“to-do lists”中学到很多
    - Skills
+ AI therapist problem，这类问题没有“全局最大值”（即唯一最优解）
+ Codex
    - 与 **Claude Code** 非常相似，采用相同的主循环架构。有趣的是，它的核心是用 **Rust** 编写的，并且是开源的。
    - 它更多是由事件驱动的；在并发线程、提交队列和事件输出方面做了更多工作。
+ AMP
    - 愿景：我们如何构建一个能在“对智能体最友好”的环境中运行的助手？
    - 上下文管理：相比 压缩，移交（**handoff**）开启一个新线程，并把必要的信息传给它，速度很快。压缩是「有损压缩」，移交是「精准打包」
+ Cursor
    - “界面优先”而非“命令行优先”的
    - 新模型 **Composer** 经过了蒸馏。他们拥有核心数据。
+ 基准测试（**Benchmarks**）几乎没用。基准测试已经变成了营销手段。
+ 有一种可能性是我们将开始在更高的抽象层级构建代理，并且只是依赖 **Claude Code** 和这些其他代理来做很多框架和编排工作。
+ 不同的视角在代理中很重要。但不同的视角很重要，因为有不同的方法来解决问题，没有一种比另一种更好，你可能想要一个混合专家代理。
+ headless  Claude Code 的强大之处，可以自动化很多工作流程

