# 20260131 Voice agents are still running on 18-month-old models. 下
+ [https://www.youtube.com/watch?v=XHpvKIczRKo](https://www.youtube.com/watch?v=XHpvKIczRKo)
+ 很长一段时间里 **OpenAI** 在 **GPT-4o** 的首 token 延迟上，**P50** 和 **P90** 之间几乎没有差距
+ **Gemini** 模型，它们有时确实很快，但在生产环境中已经不能一直保持很快了
+ 很多可靠性就来自日复一日的微调、优化，让它们达到一种超级优化的状态。
+ 氛围感就是一切——我还没找到任何一个基准测试比亲自去听、去对话、去通过那个"感觉对不对"的测试更让我信服的
+ 评测包括 **Big Bench** 音频测试、**COVOST 2** 这种用 **BLEU** 分数衡量的
+ 语音对话从根本上就是长程多轮对话的使用场景，很明显这类数据在所有训练基础模型的训练数据集中都是严重不足的。
+ 我们不得不构建了一整套评测叫 **voice agent bench**——基本上就是发现你必须模拟真实世界的对话。
+ 我觉得在基准测试中非常重要的一点是捕捉这种细微差别——不存在一个明显最好的模型，否则那家公司早就占了所有 **API** 调用 100% 的市场份额了。
+ 一个现实问题：最先进的模型在不断进步，但它们大多是思考型模型，又大又慢。
+ 2026 年发展，有两个有意思的方向：
    - 多模型的快慢思考、长时间运行的进程、共享上下文，会成为 2026 年我们都在思考的重要课题。
        * 有一个快速的语音循环，然后有各种异步的、长时间运行的或并行的推理过程——比如护栏机制，比如把函数调用从快速循环中剥离出来
    - 钟摆会从思考型模型那边稍微摆回来一些
+ 在训练中交错插入思考 **token**——**GLM 4.7** 在这方面就很有意思——我们能不能从模型中获得更动态的思考能力，
+ 我觉得在自然度方面真正重要的是找到所有感觉奇怪的案例，然后用语言描述出来到底哪里奇怪

# 20260130 Voice agents are still running on 18-month-old models. 上
+ [https://www.youtube.com/watch?v=XHpvKIczRKo](https://www.youtube.com/watch?v=XHpvKIczRKo)
+ **Daily** 内部的一个基准测试变成了可以公开发布的东西
+ 挑战在于，不同提示词在不同系统上效果不同，不同的提示工程技巧也不一样
+ 现在延迟已经够低了，中断率和语音活动检测也够好了，人们对现状还算满意。但指令遵循——尤其是当你不断尝试越来越难的任务时——才是人们花大量时间的地方。
+ **Big Bench Audio** 是语音理解的代理指标
+ 评测暴露出的一个有趣现象是，模型选择领域出现了一个权衡——你现在必须在同一张图表上同时考量延迟和智能。
+ "思考模式"未来肯定是大趋势。但这些进步不总是能直接转化到语音 AI，所以很多实际部署中你还是看到一年、一年半前的老模型仍然作为系统基础。
+ **Ultravox** 一直致力于解决的是——我们长期以来坚信语音原生模型以及这种架构对实现越来越像人类对话的重要性。所以我们必须去掉 **ASR** 步骤、去掉这些中间环节。我们花了大量时间思考：怎么把语音作为一种模态加进去，同时不让模型变笨？这是一个极其困难的问题。

# 20260129 This Startup Is Trying To Solve The AI Memory Problem
+ [https://www.youtube.com/watch?v=Sr1STQP0cds](https://www.youtube.com/watch?v=Sr1STQP0cds)
+ LLM有一个根本性的问题：**LLM** 是无状态的，它们不像人类那样记住事情。
+ Mem0的想法来源于 Sadguru AI。那个应用在印度火了，而那个应用最常见的反馈之一是：这个应用很酷，但有点蠢，因为它不记得我的冥想旅程中的任何事情
+ Sadguru是一位非常著名的印度瑜伽士
+ 我们让开发者能够非常方便地给我们任何他们认为在用户层面重要的数据。我们尽最大努力理解其中什么是重要的，然后在此基础上构建一个状态。
+ 我们的算法实际上是一个混合数据存储架构。每当有非结构化信息进来时，我们基本上会根据信息类型将其分类为键值对，或者语义块，或者我们所说的图记忆——你在尝试创建你收集的不同事实之间的关系。
+ 我们高效地做到这一点，而且是实时的。所以这能帮助你在检索方面获得非常低的延迟，同时保持高准确度。
+ memory is an expectation problem.记忆是一个期望问题
+ 用自然语言表达的... 会被 **LLM** 解释。是的，然后我们会基于此形成规则，重新运行 pipeline，为你更新所有记忆。
+ 记忆应该是构建 AI 应用时的默认基础组件
+ 一个非常有趣的模式，人们不再只是捕获关于人类的记忆，而是正在构建越来越多的 Agent，所以他们想要关于 Agent 的记忆。他们也想捕获越来越多这类信息
+ 按照最佳工程实践，甚至按照第一性原理思考，你不会想把记忆绑定到任何模型提供商
+ 对于模型提供商来说，记忆是下一个护城河，因为模型正在变成商品
+ 我们最近在头脑风暴时想出了三句话。我们称之为：让它工作、让它中立、让它可移植。
+ 历史多次告诉我们，用户期望总是朝着更少摩擦的方向发展。
+ 在做创业时，你要确保应用 **DFS**（深度优先搜索）而不是 **BFS**（广度优先搜索），这样在创业时才能保持专注。所以如果你有多个想法，在 AI 时代你有太多想法了，始终保持专注，专注于一件事，深入，构建它，与客户交谈
+ 如果你像我一样执着，我从 2012 年就一直在尝试创办自己的公司。花了我 13-14 年。我想如果我当时更努力地思考，如果我对自己有更多信念，我可能早就成功了。所以我想说相信你自己。可能就不需要 13 年了，也许... 是的，我的意思是相信自己是一个非常简单但强大的声明。这是很棒的建议。

# 20260127 Zuckerberg Taught Him How to Beat 99% of Software Engineers 下
+ [https://www.youtube.com/watch?v=9iIiKh3pdPs](https://www.youtube.com/watch?v=9iIiKh3pdPs)
+ 编程面试，设计面试。但老实说，我觉得这些东西对于一个足够聪明的人来说都是可以轻松学会的。
+ 那些你真正想要的、且很难改变的特质：
    - 自驱力，也就是主动性和自己做事的动力
    - 接受反馈的能力，以及对自己有深刻洞察的能力
    - 倾向于协作的工作方式
+ 很多人觉得让自己的懒惰不被发现是一个巨大的人生秘诀，所以如果你只是努力工作，你就已经远远领先于大多数人了。
+ 不要把自己绑在那些明显正在消失的东西上
    - 你选择加入一家拒绝让你在日常编码中使用 AI 的公司，你就是在加入一个很快就会过时的企业
    - 拒绝加入一家不让我使用最新技术来写代码的公司
+ AI 会取代大多数可以系统化完成的工作。软件行业属于这类——它可能会取代大多数软件开发者
+ 大多数软件开发者需要进入那些技术专长不在于生产代码本身的领域
+ 我觉得随着 AI 智商水平的提升，在某个时刻它会取代如此多的工作，以至于普通人很难做到能让资本主义给他们报酬的事情。
+ **Bill Gates** 有句名言大意是：我们总是高估技术在未来几年内革新事物的速度，而总是低估它在 10 年内的影响。
+ 一旦代码开始自我修改，事情可能会非常快地进入二阶效应，然后一切都会变得很奇怪
+ 我们需要做的更大的事情是确保我们社会的规则——从立法层面——预见到这个未来，并且不会让一大批人陷入无法养活自己的境地。
+ 最重要的是在你选择做的事情上做到卓越，并且在那个领域执行力超过其他人
+ 即使在 AI 接管之后，我猜测我们会发明出一些东西，人们会仅仅因为是人类生产的而珍视它们
+ 人们已经开始意识到完美只是卓越的一种类型
+ 你要成为那种不是主要在敲代码的人，而是那种知道什么代码是有价值的、知道要避免什么陷阱的人

# 20260126 Zuckerberg Taught Him How to Beat 99% of Software Engineers 上
+ [https://www.youtube.com/watch?v=9iIiKh3pdPs](https://www.youtube.com/watch?v=9iIiKh3pdPs)
+ Meta 的distinguished engineer（杰出工程师）整个行业软件工程师的最高级别之一，这个职级比 **Senior** 整整高出四级，总共只有8个人
+ 在 Microsoft一个人可能每两到五年才换一次团队。在 Facebook，每六个月换团队是很常见的，因为产品发布得太快了。
+ 那些让你走到那一步的技能仍然会是让你变得出色的技能
+ 我们并不都具备我们职业生涯中最终会拥有的技能
+ 我们的大量时间都花在微优化和提高日常工作效率上，而不是做出最大的效率提升——选择正确的事情去做。
+ 你怎么成为那个说聪明话的人？首先你得变聪明，对吧？你得投入时间来获得智慧。但你还得知道何时以及如何说，以正确的时机、以正确的人能接受的方式说，才能产生你想要的影响。
+ **Zuck** 共事真的很棒。他超级专注。我发现他是真诚善意的，意思是他真的似乎想要什么是最好的。我也发现他总是非常直接坦诚很棒，就是很直接，会直接告诉你他不同意什么。会做一些观察。
+ 你先做，然后人们付钱给你因为他们喜欢你做的东西。想被雇为软件工程师的最好方法——人们鼓励你做东西并发布你在做什么。人们会来联系你

# 20260125 Fueling the Backbone of Voice AI: Inside Deepgram’s $130M Series C
+ [https://www.youtube.com/watch?v=iSmHCTisP6M](https://www.youtube.com/watch?v=iSmHCTisP6M)
+ 未来 5 年，可能 70-80% 的 token 都会是实时的。大约一年前宣布 **Deepgram** 实现了正向现金流
+ 世界上有 80 亿人，目前语音作为用户界面的渗透率还非常低，未来将增长 100 倍甚至更多。
+ 在实时场景中，如果你有很多轮对话，可靠性的重要性就呈爆炸式增长。可靠性确实是完全复合累积的
+ 听音频，你大脑的某个部分在做感知，它不是把音频转录成文字，它传递的是一种"印象"——在深度学习中我能类比的唯一东西就是一个丰富的嵌入向量。
+ 我们应该用这种受生物学启发的架构来构建高效系统。人类大脑只用 25 到 50 瓦的功率就能做到这些，能执行极其复杂的任务、多任务处理、理解上下文、进行类人对话
+ **Neuroplex** 的核心。它不只是语音转文字加 LLM 加文字转语音。它是在整个过程中传递上下文，而且是一个模块化架构，不是黑盒。
+ 我基本上就是在描述一个语音对语音模型，只是描述了一种不同的构建方式。
+ 市面上实时语音对语音模型的 S2S 采用率，通常非常低。因为无法很好地控制它，无法很好地调试它，无法很好地添加护栏。而且可靠性通常更差，延迟也更差，所有这些问题。
+ 它在今天系统中的体现方式，非常类似于自动驾驶能够取得领先的方式——通过让这些模型能够相互通信，有一个通信层，你可以听到警笛，同时通过激光雷达、摄像头看到周围发生的事情。
+ 拥有所有这些感官输入，然后做出决策：这个人下一步会做什么？车应该往哪里走？我认为这正是语音 AI 需要的——拥有下一个层次的自主性。
+ 你需要有并行系统。因为当你作为人类行走在世界上时，你不会走一步然后说"好，让我重新评估一下下一步要做什么，现在我再走一步"。说话也是一样。我一直在想接下来要说什么。同时思考很多背景想法，也在同时听你在说什么。
+ 如何在现实世界中工作，如何有不同的层级。你不需要重新发明轮子来预测语音 AI 的未来会发生什么。只需要看看这些不同层级中的不同系统，尝试构建冗余性。
+ 有趣的是，人们很容易想"但是算力会变得很便宜，这就不重要了"。也许 10 年 20 年后吧。但这里有一个假设的等价性其实不成立：如果你减少完成任务所需的算力，你就可以使用更智能的组件。所以只要模块化方法和大模型一样具有表达力，它就总是更胜一筹。

# 20260123 The ML Technique Every Founder Should Know
+ [https://www.youtube.com/watch?v=dC_3ys349bU](https://www.youtube.com/watch?v=dC_3ys349bU)
+ 从某种意义上说，所有 **LLM** 或所有机器学习模型都是在学习数据分布
+ **diffusion**，不是自回归 **LLM** 的新方法，它特别突出的地方是从高维映射到高维，尤其是在数据量少的情况下
+ 所以基本上就是一个"加噪器"和一个"去噪器"，去噪器就是你最终训练的模型
+ 起源于图像，但现在的应用范围远不止图像。**DeepMind** 刚刚因为将这个方法用于蛋白质折叠而获得诺贝尔奖。
+ 你可以用扩散策略论文里的方法来驾驶汽车，这是一个疯狂的结果。你还可以预测天气。这个技术能做的事情真的没有限制
+ 预测实际数据本身其实很难，预测误差可能更容易，预测速度比那还容易，预测整个扩散调度过程中的全局误差就更容易了
+ 扩散调度，这可能是扩散模型中最重要但最难理解的部分之一

# 20260120 MIT Researchers Just Solved Context Rot
+ [https://www.youtube.com/watch?v=KvrJaGqHz14](https://www.youtube.com/watch?v=KvrJaGqHz14)
+ context rot：上下文窗口长度实际上并不重要。无论是 20 万 tokens 还是 100 万 tokens，你的大语言模型的效果在大约 10 万 tokens 之后就会下降
+ **haystack test（大海捞针测试）**：本质上就是给大语言模型一个大文档，答案埋藏在文档的某个地方
+ RLM 不仅能处理巨大的任务，而且在小 token 长度的复杂任务上也表现得更有效
+ RLM 很简单：把提示词当作环境的一部分作为一个变量加载，然后编写代码来查看、分解，并在变量的程序片段上递归调用自己
+ 本质上就是分块，然后把每个块交给一个子大语言模型，把上下文卸载给一些小的迷你代理，作为工具调用来使用
+ 你曾经在 **Claude Code** 中使用过子代理来代替你做一些事情，因为你不想让所有上下文都进入主上下文窗口，那你实际上就已经在使用这些 **RLM** 系统的一些基本原理了
+ 重复同样的过程。一直是 **LLM** 套娃。这就是递归语言模型的工作原理
+ 答案被聚合起来，这样我们这边的家伙就能处理这些巨大的提示词，而实际上从未真正吸收它们。因为他有点像个骗子，让别人替他干活，最后却声称是他自己做的。这就是 **RLM**。
+ 大文档不应该直接大语言模型，而应该把它们当作环境的一部分，让大语言模型可以符号化地与之交互。
+ GSD 是 "Get Shit Done" 的缩写（字面意思：把事情搞定），是一个构建在 Claude Code 之上的上下文工程和元提示层（context engineering and meta-prompting layer）简单来说，GSD 是一套让 AI agent 保持专注、高效完成任务的方法论/工具，它的核心理念也是上下文窗口管理：

GSD 的主要特点：

+ 🧠 子代理编排（Subagent Orchestration）：为每个任务提供新鲜的上下文
+ 📝 显式项目文档：用明确的文档来管理上下文
+ 🗂️ 向量数据库集成：与 Pinecone、Weaviate 等配合使用
+ ⚙️ 禁用自动压缩：保持上下文的完整性

# 20260119 How To Get Your First Users
+ [https://www.youtube.com/watch?v=0kARDVL2nZg](https://www.youtube.com/watch?v=0kARDVL2nZg)
+ 当你刚起步时，你需要的不仅仅是最小可行产品（MVP），你需要的是"最小可进化产品"。
+ 还有一些人面临着非常迫切的问题，只要看起来能让他们的生活更轻松，他们愿意尝试任何新产品。
+ 教训很简单：找到你的第一批用户，与其说是一个说服问题，不如说是一个搜索问题。
+ 几个反直觉的启示：
    - 尽早收费：付费客户给出的反馈永远比免费用户更尖锐
    - 使用有针对性的个人触达方式：广告牌远不如有针对性的冷邮件或直接敲他们的门来得有效。
    - 尽早发布。这是 **YC**从一开始就一直倡导的理念。
    - 仔细研究你的早期用户：他们是如何做决定的？为什么他们会做出信任你这个奇怪的选择？你要理解他们是如何思考的，他们想要什么。
    - 快速实验，不要怕用户流失。
    - 你的早期用户不仅仅是给你反馈，他们最终会引导你的产品如何随时间演变。
    - **Tesla** 的故事揭示了另一个现实：产品进化是路径依赖的，取决于早期采用者想要什么。
+ 在 AI 时代，这个差距很重要。消费类应用可能会举步维艰，因为广告收入往往无法覆盖 AI 成本，而订阅费用必须挤进本就很小的个人预算。许多 AI 创业者选择先向专业消费者或企业销售，或者瞄准像医生这样具有高广告价值的用户。
+ 知道产品会发生很大变化，这让人感到解放——所以它从一开始不必完美。最终，它会变成什么样，取决于你从哪里开始，以及你和谁一起开始。

# 20260118 Build a Real-Time AI Sales Agent - Sarah Chieng & Zhenwei Gao, Cerebras
+ [https://www.youtube.com/watch?v=mwzk2rlwtZE](https://www.youtube.com/watch?v=mwzk2rlwtZE)
+ **Cerebras** 推理的秘密武器
    - **Cerebras** 的芯片没有内存带宽问题
    - **Cerebras** 在硬件上做了什么，使其能够比 **Nvidia GPU** 推理快 20 倍、30 倍甚至 70 倍
    - 一个核心需要访问哪些东西来完成计算呢？它需要权重、激活值、**KV cache**
    - **H100**存储在片外内存中
    - **Cerebras** 硬件（**WSC3**）上的每一个核心——共有 90 万个核心，相比 17,000 已经多很多了——每个核心都有自己专属的片上内存，也就是自己的 **SRAM**
+ 在推测解码中，你会组合两个模型。你使用一个较小的模型作为草稿模型，它可以非常快速地生成所有 token。然后使用较大的模型回过头来验证小模型的输出是否正确
+ voice agent 四个关键能力:
    - 它们能理解并响应口语
    - 可以处理很多复杂任务
    - 在任何系统中传达意图的最快方式
    - 能够跟踪对话的状态
+ **VAD** 可以帮助系统知道你什么时候在说话、什么时候没有，但同样重要的是要分析你在说什么、你说话的上下文，以及预测你是否已经表达完想法。

# 20260116 Meet Flux: Speech Recognition for Real-Time Voice Agents 2
+ [https://www.youtube.com/watch?v=ILUrgSmXGFU](https://www.youtube.com/watch?v=ILUrgSmXGFU)
+ 区别在于我们为你管理整个轮次的转录
+ 转录可能更新了，也可能没更新，但你会每四分之一秒收到这些事件。
+ 你的 **EOT**（轮次结束）置信度阈值是可配置的，默认是 0.7。当你收到这个事件时，就要触发代理的回复。
+ **speculative generation** 🔥：推测性生成（提前预测并生成内容）
+ 如果实际上并不是轮次结束，我们会发送"轮次恢复"事件——这表示我们以为轮次要结束或已结束，但其实没有。那只是一个停顿或误判，用户继续在说话。
+ **Silero VAD** 是一个可靠的语音活动检测器，但在检测轮次结束方面完全不智能。所以 **Silero VAD** 的精确率超低，但召回率很高——因为如果我设置很短的 VAD 超时，就会一直误判为轮次结束，产生误报。这就是为什么会出现在用户说话时打断他们的情况。
+ 得益于模型架构——它将完整轮次的语音转文字上下文与轮次检测方案结合在一起。因此，模型用于计算轮次结束的上下文比其他模型丰富得多。而且你不需要以增加额外延迟的方式把这些东西拼接在一起
+ 统 **STT** 只是听到声音然后转录，完全不知道正在进行对话。然后你通常在事后用说话人分离来补充，可能还用 **LLM** 做一些后处理。但没有实时的对话理解。
+ 我们在设计这个系统时，想要捕捉两个方面
    - 我们希望模型能用自然的对话术语来理解正在说的内容
    - 边听边思考
+ key term boosting （关键词提升）这是 Deepgram 让你可以给模型提供一些先验信息的方式，告诉它转录中可能出现的词，特别是重要的专有名词。然后模型会从这些先验中学习，更有可能正确识别这些词及其拼写和大小写。

# 20260115 Meet Flux: Speech Recognition for Real-Time Voice Agents 1
<!-- 这是一张图片，ocr 内容为：LEL DESIGNED FLUX IS THE FIRST SPEECH RE SH RECOGNITION MODEL D NOT ONLY STREAMING-FIRST, BUT ALSO CONVERSATION-FIRST. FLUX: THE FIRST CONVERSATIONAL SPEECH RECOGNITION MODEL, DESIGNED SPECIFICALLY FOR REAL-TIME VOICE AGENTS. MODEL-INTEGRATED TURN DETECTION:TURN DETECTION THAT UNDERSTANDS FULL TURN AUDIO AND SEMANTIC CONTEXT, FAR BEYOND SIMPLE VAD OR SEMANTIC BASED SOLUTIONS SINGLE APL THAT REPLACES STT + EXISTING TUM DETECTION ALTERNATIVES WITHOUT THE ADDED LATENCY OF A SEPARATE TURN-DETECTION MODEL OR COMPLEX CUSTOM LOGIC UITRA-LOW LATENCY WITH THE ACCURACY OF NOVA-3: TRANSCRIPT UPDATES EVERY "QUARTER SECOND. SUB-300MS END OF TURN DETECTION CUSTOMIZABLE TURN-TAKING DYNAMICS: CONFIGURABLE TURN CONFIDENCE AND TURN TIMEOUT THRESHOLDS TO OPTIMIZE FOR YOUR USE CASE NEEDS CONVERSATIONAL CUES:INSTEAD OF JUST SENDING YOU TRANSCRIPTS,FLUX SENDS CONVERSATION EVENTS (STARTOFTURN, EAGERENDOFTURN,TURNRESUMED, ENDOFTURN) THAT TELL YOUR AGENT EXACTLY WHEN TO LISTEN,PREPARE RESPONSES, AND SPEAK -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1768490461238-e8349cba-905f-4ee4-b911-b3686b2fc6d0.png)

+ [https://www.youtube.com/watch?v=ILUrgSmXGFU](https://www.youtube.com/watch?v=ILUrgSmXGFU)
+ Deepgram Flux，第一个流式优先设计的语音识别模型——市场上现有的流式语音转文字模型从来都不是流式优先设计的，流式功能都是后来硬塞进去的。
+ 我们在行业中观察到的一个趋势是，一切都在向实时交互范式转变
+ 而现有的流式语音转文字方案其实并不是为此设计的。它们甚至不是为实时场景设计的，更别说实时交互式语音 AI 应用了
+ 现有的 STT（语音转文本）方案难以满足实时语音 AI 应用的需求：
    - 自然流畅的对话型智能体需要亚秒级响应延迟
    - 准确判断说话者**什么时候真正说完**，以及智能体**何时该回应**
    - 要复杂逻辑来管理轮次、打断、上下文、转写、工具调用以及端到端的语音 AI 流水线
+ 问题：
    - 过早抢话 → 在用户一句话没说完时插话，破坏信任感
    - 等待过久 → 出现机械、尴尬的停顿，严重影响互动体验
+ **Flux** 看起来会有些不同。**Flux** 会持续维护整个轮次的转录。它在持续计算轮次结束的概率。

# 20260114 **Inside OpenRouter’s Tech Stack and Use of Effect | Louis Vichy | Cause & Effect 6**
+ [https://www.youtube.com/watch?v=AVJIqQi11lM&t=1231s](https://www.youtube.com/watch?v=AVJIqQi11lM&t=1231s)
+ **X** 上看过 **Elon** 提到，**OpenRouter** 的第二个产品是向外部开放的数据。
+ 这些数据告诉你哪种模型花费最多、效用最高，也就是最常被用
+ 每周约 5 万亿 **token**，而一年前只有约 1500 亿，增长超过 20 倍。
+ 不只是模型 **proxy**，而是把不同模型的响应做了统一整合，这一点非常关键
+ 做 **wrapper** 容易，做开发者工具难，尤其是建在 **API** 之上的工具
+ 规模化冒出很多有意思的问题。我和合伙人最看重的一点就是类型安全。
+ “一切都用 **TypeScript**”。真的不夸张，**OpenRouter** 最初版本里，所有模型和端点都是代码里的 **TypeScript** 对象和字面量类型。我们用严格的 `satisfy` 来做引用
+ 当时新增模型和端点时，我们直接让 **Devin** 去做，这能自动化是因为类型安全。只要想加东西，就让模型改代码，其余就是 **TypeScript** 的类型报错来指引。
+ **OpenRouter** 现在跑在 **Cloudflare** 上，**Cloudflare** worker 更适合 **TypeScript**；它既严格又灵活，让工程师速度更快。
+ **Facebook** 首个原型是 **PHP**，现在仍主要是 **PHP**，他们没换语言，而是做了 **Hack**。这也是 **TypeScript** 的思路：让大家说同一种语言，所以我们代码里一切都是 **TypeScript**。
+ 脚本也用 **TypeScript**，不写 **bash**。我们用 **ZX** 和 **TS** 写脚本跑数据库迁移或即时响应脚本，这样沟通同一种语言，别人也容易接手；而且代码易抽成共享包复用，基本不会有死代码。
+ 大概要三年，一个人才会真正明白：数据库迁移脚本不必追求极致速度。也不需要定制框架，哪怕脚本只是一行
+ 适应 **TypeScript**——更准确是整个 **Node.js** 工具体系——大概要五年，确实要花时间，不只是语言本身。因为如果一个人在 **Golang** 里待久了，home  directory 很可能已经被一堆 **Go** 二进制“污染”。
+ 在 **Cloudflare** 之前我们都在 **Vercel**，直到他们开始收 **egress** 费用；而且网络连接层不是一等公民，很多细节被抽象掉，用起来不够透明、也不够可控，体验上比较别扭，排查问题也更难。
+ 很多时候我更想让类型自动推导，因为我讨厌手写类型。我喜欢尽可能推导类型，所以我常出的“考题”是：能否从第三方库推导出类型，看他们是否踩过这个坑。
+ **Cloudflare** 这边我们用 **Hono** 把所有 **API** 包在同一路径下，路由统一；入口只做鉴权和中间件，其他都下沉到核心逻辑里，仍是普通函数，整体流程很清楚，写起来也更直观。明白。
+ **JavaScript** 在足够多的服务器上跑时并不需要为单个请求用多线程，而是同时响应成千上万请求，请求还会分散到不同实例。
+ **Effect**。它要成为可测试的 **consolidation engine**，内置 **OpenTelemetry**，后来成了开源项目

# 20260113 Unlock Better RAG & AI Agents with Docling
+ [https://www.youtube.com/watch?v=rrQHnibpXX8](https://www.youtube.com/watch?v=rrQHnibpXX8)
+ **RAG** 管道或 **AI agent** 中最大的一块缺失拼图——那就是数据准备
+ 因为想让你的模型给出更好、更准确的回答，它就得完全理解你使用的数据
+ **RAG** 或 **agentic AI** 的真正挑战不是构建 agent，而是整理背后的知识和上下文。
+ **Docling** 输出的是富层次结构的 **Docling** 文档，包含元素类型、标题和每个元素的元数据，所以开箱即用就有结构化的分块能力。这意味着可以按章节、表格和标题来切分，并自动携带父级上下文（如标题和页眉），产生比固定大小切分更连贯的块和更好的检索信号。这是个大事，因为你从 **PDF** 开始就获得了类型安全和数据验证，把非结构化数据变成了真正的结构化输出。

# 20260112 Manus决定出售前最后的访谈：啊，这奇幻的2025年漂流啊…
+ [https://www.bilibili.com/video/BV1knvYBDEjs](https://www.bilibili.com/video/BV1knvYBDEjs)
+ 学习要跟具体的需求做牵引：为了做preloading学习了NLP



+ 2013年，在Transform之前，google 石破天惊论文Word2vec问世：可靠高效的把“离散”的自然语言文本转变成“稠密向量”，从此新世界的大门打开了。
+ 历史会押韵但肯定不会重复
+ 创业早一步是先驱，早十步就是先烈
+ 不是产品的用户，肯定做不好产品
+ 开朗的内向
+ 每增加一个东西都会稀释所有的价值
+ 浏览器插件是一个绝妙的观察用户到底怎么使用AI的窗口
+ 没有乔布斯的命得了乔布斯的病，身心健全是CEO最稀缺的特质
+ 以后不再区分应用公司和大模型公司，因为模型训练已经变成通识
+ 模型训练一定是追赶不上产品经理的思维活跃度
+ Taste(品味)是体现在内部的Evaluation和Benchamark,这个所有公司的护城河
+ AI团队规模应该只能作为一个被动指标
+ 浏览器端侧模型就是一个坑，AI接管浏览器也是一个坑，AI真正有价值的是长任务
+ 历史上两次浏览器的迁移，网景到IE，IE到chrome全部是因为分发引起的
+ “任何一个复杂的问题，都会有一个简单又清晰的回答”
+ 发布一款自己不看好的产品会陷入自证循环，导致增加很多机会成本
+ 一个团队如果有一个正向现金流的产品，在做第二个产品的决策的时候会非常的客观大胆有理智
+ Manus创意来源于很多非工程师都在用cursor
+ 产品型公司不应该在Bet上纠结和拧巴，以下是三个Bet
    - 相信Manus能被做出来，这个可能是最难的
    - 不自己训练模型，相信Agent能解决问题
    - 不要做中国版Cursor,而是差异化竞争
+ 为什么要做通用Agent?
    - 之前的创业失败原因是专有模型被通用模型干掉了
    - Manus底层的的技术机制是通用的
    - 更适合观察用户的集体行为模型
    - 让长尾用户满意，更容易获得Aha moment惊喜。
    - 对于个人来说，长尾并不等于低频
    - 上下文和记忆可以更好的流转，永远能被垂直功能多做一步，并且能产生内部网络效应
+ Agent和ChatBot都是套壳，但壳的深度不是一个数量级
+ 环境是Agent里面最重要的因素
    - Agant 不光要Scale Up 还需要 scale Out
    - Manus是基于Firecracker轻量级别的全虚拟沙盒技术，主要是很多专业的软件都在windows
    - 给模型一个完整虚拟机预装软件比Function Call更有用
+ AI时代会让产能变得很大，不做什么比做什么更加重要，Manu是克制的产品，我们每个月都在想删除什么
+ 现在的大模型后训练还是都基于chatbot的场景，并不适合Agent
    - Chatbot的预训练更倾向于着急回答问题，长任务中的上下文压力，会影响下一个Token EOS的概率
    - Agent的基础假设来源于姚舜雨的Paper, ReAct，所有会更有耐心
    - 没有为现代的context engine 做预训练，比如Compassion Awareness(压缩意识)
    - Long CoT的reasoning效果不是很好，agent需要 ReAct那种交错式的思考,这块还有很大的改进空间
+ 消耗的token越多证明产品影响力足够大，就能影响模型本身的发展。同时也是给大模型公司做Evaluation
+ 评价顶级模型
    - Anthropic在Coding领域是非常领先的，Claude Opus 4.5 被低估了
    - Gemini在多模态领域是断层式的强，gemini可能是唯一的能使用Google索引方式
    - OpenAI在纯推理能力上比较强的领先，OpenAI是刷榜的最好选择
+ 应用公司做整合，挺反直觉的。但其实符合直觉的事情也轮不到我们做、
+ 因为用户的轨迹数据是留在应用层的，所有应用公司都有独特的数据飞轮，最终都会以自己训练模型的方式变现出来
+ 两种数据飞轮：
    - 用的人越多，失败率越低，轮次开销越小
    - 专门有10个人的团队在搭建内部Evaluation体系和自动化
+ 关注的几个指标：
    - Coding 是Manus的灵魂
    - GUI的理解能力
    - 广义的tool call 能力
    - 对错误的自我意识
+ 通用Agent,你实际上做的是一个普普通通的人，而人可以把垂直Agent当成工具
+ Manus 是 RLI（远程劳动者指标，能完成多少远程工作者能完成的事情 ） 的 SOTA,但是也只有2.5%
+ 通用Agent跟垂直Agent的区别主要是输入和输出的内置量，但最终的技术会趋同
+ 很重要的产品思路：Manus不是替代人而是提升人
+ Manus要提供永远比Chatgpt更好的体验
+ Proactiveness AI 是未来Manus投入的方向，本意来源于Agency(本来就是自主，主动的意思)
+ Manus刻意的选择：永远是不断优化头部场景，且保持很强长尾能力的系统
+ AI coding 不是垂直领域，而是通用领域
+ Gemini3 在静态前端的美学已经追上了Cloud Code
+ Chatbot的Token消耗量是可以预测的，而Agent消耗量是成百上千倍。Manus的Token消耗量
+ 选择出海的原因很简单，海外对生产力工具的付费意愿更强
+ 版本号的意义：你得让用户意识到你有变化，用户才能真正感受到变化
+ Manus每天收到的负面反馈比正面反馈多
+ 对于一个高价值用户，他的manus用量可能是一个普通用户的一千倍，所以Manus把高价值用户的高价值Task做好，去追求营收而不是DAU，这个跟互联网的底层逻辑不一样：
    - AI不是平台变化而是技术增量，强者具有先发优势
    - 传统互联网边际成本很低，而到目前为止，AI更像是制造业而不是互联网
+ 目前为止，并没有看到一个能爆发网络效应的AI产品
+ “纯血派”Agent:  没有人为加的约束，智能主导的而不是规则主导的Agentic workflow
    - 符合The Better Lesson: 用通用的方法投入更大的算力去解决问题，而不是加入更多的人为知识
    - 舍弃直接修补问题的传统产品直觉，而是用智能的泛化性去解决甚至是未发现的问题
    - 如果一个功能能靠Agent的泛化能力去做，就不需要自己去做
+ 如果你是在做agent, 你其实是在做两个产品，一个是给人用的，一个是给模型用的，而这两个的思维模型是不一样的
    - 模型的思维模型跟人不一样的，把Agent人格化其实是人的自恋
    - 不要过于执着让Aegnt模仿人类分工
+ Manus 中有特色的组织架构：
    - Sandbox Team: 教会模型怎么使用电脑
    - Agent Team: 设计一套稳健统一的架构，以持续跟上模型的进步
        * Agent 工程师
        * Evaluation Team
        * 很小的 Research Team
+ LLM发展速度以周为单位，如何让Agent跟上，有个方法论：
    - Agent有两个重要的影响质量的变化：LMM和Agent框架
    - 需要让每次LLM的迭代，保证让Agent框架受益最多
    - 调整agent框架，让同一序列的最强最弱LLM版本对比，Delta最大
+ 很多单独的feature,做起来一时爽，长期维护迭代升级很难。而应该利用别人已经有的能力合作
+ 最重要的技术决策：
    - 不盲目追求Reasoning这条路，而是用单独的Planning Stage
    - 对MCP的决策非常保守
        * MCP会严重污染 Action Space,导致缓存命中率低
        * 研究了一套不在原生Action Space的MCP调用方法（好像说的是Token logits）
+ 做Agent跟训练模型很像：更重要的是做对1000件小事，而不是做对3件大事
+ 后悔一开始太盲目相信小模型：
    - 以为通过工具调用可以动态弥补小模型的参数化知识，实验结果发现不是
    - 反思原因是很难完全分离知识和记忆，哪些是泛化能力，哪些是参数化的背板
+ 很多Context Engineering 方面的工作需要做大量实验，无法靠纯工程解决
+ 给LLM厂商的建议：
    - 200K以上的Context  已经不重要了,比起更大的Length，LLM的 上下文压缩意识更加重要。
    - Reasoning要更好的结合Observation
    - 模型与人的交互应该是异步并行，而不是chatbot那种轮次交替
    - 模型应该更关注错误的处理和恢复能力
+ 技术肯定是要服务于产品，但是技术对产品有一票否决权
+ 鼓励团队使用任何第三方的AI产品，费用全部报销
+ 对未来三个月的预测：
    - Proactiveness AI 会有很大的进展：人跟AI交互的时间变短，但是产出的价值更高
    - 会有一次较大的产品成本降低
+ AI时代跟互联网时代的区别在于，无法因为用户总注意时长而产生瓶颈导致稳态
+ Agent其实已经爆发了
+ WeWork的好处是可以动态扩容
+ 

<font style="background-color:rgb(10, 61, 75);">  
</font>

# 20260111 Automating Large Scale Refactors with Parallel Agents - Robert Brennan, AllHands（下）
+ [https://www.youtube.com/watch?v=rcsliSIy_YU](https://www.youtube.com/watch?v=rcsliSIy_YU)
+ 拆分成子任务：
    - 基本能一把梭
    - 能放进一个commit
    - 可以并行执行
    - 人类可验收
    - 任务之间的依赖关系，先后关系 清晰
+ 依赖树迁移法：最适合大型项目 / 大规模迁移（migration），尤其适合强类型语言。
    - 枚举所有文件，并标注哪些文件“内部依赖最少”（最独立）。
    - 从“依赖最少”的文件开始迁移。
    - 当所有文件都迁移完后，把它们“汇总/整合（collate）”。
+ 搭脚手架迭代迁移法：最适合那些可以在“中间态”（新旧并存）下继续运转的项目。
    - 先搭一个脚手架，让你可以同时处在“两个世界”（旧系统 + 新系统）里。
    - 然后让 agents 分别去改各个文件/组件/服务。
        * 确保每个 agent 的改动都能通过测试，并且应用能构建成功。
        * 确保没人偷懒！（别只改一半就跑）
    - 最后，把脚手架彻底拆掉。这一步通常也可以交给 agent！
+ 为什么需要上下文共享：
    - 我们会在执行过程中不断学到新东西！
    - 我们需要把新信息在一整队 agents 之间共享/同步。
    - 这里有多种策略，各有优缺点。
+ 上下文共享策略：全部共享
    - 你可以干脆只跑一个 agent 做完整任务。
    - 者把每个 agent 的历史对话/执行记录共享给其他 agent。
    - 但这有不少问题：
        * 会极大膨胀上下文窗口，导致成本上升。
        * 会污染上下文窗口，让 agents 更容易被无关信息带偏。
        * 会拖慢整体速度。
+ 上下文共享：消息传递
    - 你可以给 agents 配工具，让它们彼此发消息交流！

# 20260110 How Tolan builds voice-first AI with GPT-5.1
[https://openai.com/index/tolan/](https://openai.com/index/tolan/)

<!-- 这是一张图片，ocr 内容为：'WHAT TRIPS DOES THE USER HAVE COMING UP?" EMBED QUESTIONS USER:'I M SO "WHAT IS THE USER DOING THE WEEK OF 2025-11-02?" &QUERY MEMORY EXCITED FOR MY TRIP SYNTHESIZE QUESTIONS THIS WEEKEND" VECTOR DB "WHAT DOES THE USER LIKE TO DO?"  TOLAN: "I KNOW. MERGE RESULTS CAMPING WITH STEVEN SETS WITH MEAN IN YOSEMITE IS GOING RECIPROCAL RANK TO BE EPIC!" USER:"FOR OUR NEXT STORE:"EVAN HOPES TO COMBINE,EDIT, CLUSTER MEMORIES BY COMPRESS TRIP,I REALLY HOPE REFLECT GO TO ICELAND WITH REFINE MEMORIES EMBEDDING(KNN) WE GO TO ICELAND* HIS FRIEND STEVEN'' WITHIN  CLUSTER -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767999394394-154365da-c13e-44a2-903d-3f62a4f0ea8d.png)

+ Tolan 随着时间的推移，它会从对话中学习。
+ 语音 AI 提高了延迟和上下文管理的门槛，但也比文本实现了更开放、探索性的互动。团队将精力集中在两个关键杠杆上：记忆和角色设计。
+ OpenAI GPT-5.1 和 Responses API 的推出将语音启动时间缩短了 0.7 秒以上
+ 上下文处理：与许多在多轮对话中缓存提示的智能体不同，Tolan 每一轮都从头重建其上下文窗口.这种架构使 Tolan 能够实时适应突然的主题转变，这是自然语音交互的基本要求。
    - 最近消息的摘要
    - 角色卡片
    - 向量检索的记忆
    - 语气指导
    - 实时应用信号
+ 缓存提示根本行不通.实时重建方法既是技术密集型的，也是 Tolan 成功的基础
+ Tolan 构建了一个记忆系统，不仅保留事实和偏好，还保留情感上的"氛围"信号——这些线索有助于引导 Tolan 该如何回应
    - text-embedding-3-large 模型嵌入
    - Turbopuffer ：一个高速向量数据库，支持低于 50 毫秒的查询时间
    - 系统根据聊天信息生成“多个提问”来触发记忆召回（通过生成多角度的子问题来提高召回率）
    - 每晚压缩任务，移除低价值或冗余的条目来解决记忆矛盾
+ 科幻作家撰写角色支架来进行个性管理
+ 一个并行系统监控对话的情感基调（tenor），并动态调整 Tolan 的表达方式
+ GPT-5.1 的过渡是一个转折点，分层的提示指令——语气支架、记忆注入、性格特征——被更忠实地遵循了。不需要变通方案
+ 记忆召回失误下降了 30%（基于产品内的挫败信号），在 GPT-5.1 驱动的角色上线后，次日用户留存率上升了超过 20%。
+ 通过 summary threshold（摘要阈值）来创建聊天摘要
+ “用户回复后” 再做 提取，反思（kNN），压缩

# 20260109 Automating Large Scale Refactors with Parallel Agents - Robert Brennan, AllHands
[https://www.youtube.com/watch?v=rcsliSIy_YU](https://www.youtube.com/watch?v=rcsliSIy_YU)

<!-- 这是一张图片，ocr 内容为：EVOLUTION OF AI-DRIVEN DEVELOPMENT PLUGINS CLOUD AGENTS ORCHESTRATION IDES LOCAL AGENTS OPENHANDS CLOUD SDK CLI CODEX CLINE GEMINI JULES W WINDSURF CLAUDE GITHUB CURSOR DEVIN CODE COPILOT AGENTIC TACTICAL MEDIAN DEVELOPER EARLY ADOPTER -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767917963431-96ad5e8d-5217-41e7-b4e1-316b9650b9e4.png)

+ 有大量繁琐的工作涉及技术债务、代码维护和代码现代化。这些任务非常适合自动化.但它们往往规模太大，不适合单次一把梭
+ OpenDev：我们意识到，这不应该在黑盒子里发生。如果我们的工作将要改变，我们希望这种改变是由软件开发社区驱动的，我们希望在这种改变中拥有发言权。
+ 即使你今天把大语言模型冻结住、不让它们再进步，未来两三年软件工程这份工作仍然会发生巨大变化——因为我们还在摸索如何将这项技术落地。
+ Evolution of AI-Driven Development：
    - **Plugins → IDEs → Local Agents → Cloud Agents → Orchestration**
    - **Tactica** -》 **Median Developer** -〉**Early Adopter** -》Agentic
+ agent 创造 agent。我认为这是当前能力的最前沿。人们刚开始试验这个，刚开始在规模化层面看到成功，但确实有一些任务非常适合这种工作流。它有潜力真正自动化掉每个当代软件公司底下堆积如山的技术债务。
+ 对于 **Open Hands**，我们一开始主要做云 agent，后来稍微退了一步，构建了类似 **Cloud Code** 的本地 CLI，以便在开发者当前所在的位置与他们会面
+ 采用了编排的顶尖 1% 工程师，他们的生产力获得了巨大提升，能够处理其他团队根本顾不上的大量技术债务积压。通常是那些高度可重复、高度可自动化的任务。
    - 复整个代码库中的 CVE
    - 给 Python 3 代码库添加类型注解
    - 把 Java 单体应用拆分成微服务
    - 旧版 Java 迁移到新版 Java
    - **Spark 2** 任务迁移到 **Spark 3**
    - 把整个前端从 **Redux** 迁移到了 **Zustand**
+ 什么这些任务不能one shot it（一把梭）？
    - Agent Problems：
        * 上下文窗口有限
        * 偷懒
        * 缺乏领域知识
        * 错误会逐步累积
    - 人的问题
        * 难以传达隐性经验
        * 难以把大任务拆解成清晰步骤
        * **需要中途审查/阶段性同步（check-in）**
        * 完成”的标准不够明确（DoD 不清晰）
+ 我想说清楚：我们并不期望每个开发者都去做 agent 编排。在熟悉的环境中、在 IDE 旁边本地运行 **Cloud Code**，至少在未来几年会是常见的工作流。你不会看到所有软件工程都有 3000% 的生产力提升，大概只会得到大家报告的那 20% 的提升
+ 我认为这真的很难做对。这就是需要大量思考的地方——如何分解任务，以便我能验证每个单独的步骤，以便我能真正自动化整个过程，而不是最后得到一堆乱糟糟的代码
+ 如果你刚开始尝试这个，我建议把自己限制在大约 3 到 5 个并发 agent。我发现超过这个数量，你的大脑就会崩溃。但对于那些真正大规模采用编排的人，我们看到他们同时运行数百甚至数千个 agent

# 20260108  Ralph Mode for Deep Agents: Running an Agent Forever
[https://www.youtube.com/watch?v=yi4XNKcUS8Q](https://www.youtube.com/watch?v=yi4XNKcUS8Q)

<!-- 这是一张图片，ocr 内容为：RALPH MODE FOR DEEP AGENTS RALPH LOOP TASK "BUILD A PYTHON COURSE" ITERATION 1 HIGERT TEMIREG ITERATION 2 DEEPAGENT ITERATION 3 CTRL+C TO STOP OR MAX_ITERS REACHED FILESYSTEM (WORK PERSISTS) -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767829489126-9e1a18ac-cbce-42ff-88f1-4aaaa555307f.png)

+ Ralph loop：一个 agent 只靠一个简单的 **while loop** 加上 **file system**，究竟能做到什么
+ 文件系统就像“工作日志 + 记忆仓库”，保存每一轮迭代的产出和进展。
+ 现在想让 agent 做“长时间运行”的工作，最简单的方法之一，就是强制它一轮又一轮地继续跑这个循环。
+ agent 非常非常擅长用 **file system**。而文件系统之所以好，是因为如果再配合 **Git** 这种“历史记录”工具，它们就能把之前做过的工作一路追踪、记下来。
+ 归根结底，你基本上就是在一个不断更新的文件系统上，用同一个提示词一遍遍地循环运行 **Ralph**。
+ **Ralph** 会先创建一个待办事项列表（to-do list）。这是因为我们是基于 **deep agents** 构建的，而 **deep agents** 内置了这种原语，也就是我们的待办事项列表工具
+ 在这些长时间运行的 **agent** 任务中，上下文管理仍然很难做。
    - 压缩、摘要、卸载等手段都是为了减轻“上下文腐烂”（context rot）的问题
    - 如果我们重启循环，让 **Ralph** 看到“嘿，这些是大概做出的变更，这是我现在开始的地方”，那么我们就拥有了一个相对干净的上下文窗口，它可以继续在某种循环中工作。

# 20260107 Build a Prompt Learning Loop - SallyAnn DeLucia & Fuad Ali, Arize
[https://www.youtube.com/watch?v=SbcQYbrvAfI](https://www.youtube.com/watch?v=SbcQYbrvAfI)

<!-- 这是一张图片，ocr 内容为：1 OTHER ISSUE I'D LIKE TO MENTION DOMAIN EXPERTS TECHNICALUSERS AL ENGINEER DATA AL PRODUCT SUBJECT MATTER SCIENTIST MANAGER EXPERTS DEVELOPER RESPONSIBILITIES RESPONSIBILITIES DOMAIN PROMPT ENGINEERING CODE/AUTOMATION TRACK AND RUN EVALS PIPELINES/FRAMEWORKS APPLICATION PERFORMANCE/COSTS ENSURE PRODUCT SUCCESS -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767744561294-f353fb8d-a6ef-42dc-bc9a-c058af701d19.png)

+ agent会失败，很多时候不是模型弱，而是环境和指令弱。
    - 适应性与自我学习：系统没有从环境中沉淀/学习出稳定的“系统级指令”。
    - 确定性 vs 非确定性失衡：要么不做规划，要么规划太死板，缺少可控的灵活度。
    - 下文工程不到位：缺工具、缺工具使用指引、缺关键上下文（尤其是数据被预先裁剪后导致信息不全）。
    - 技术侧往往更擅长工程实现，却不一定懂“什么回答才对/什么体验才好”；
    - 业务侧最懂“什么才叫对”，却可能不擅长写代码、看 trace、调 pipeline。
+ RL 还要改权重
+ meta prompting 　只有分数，没有为什么和怎么改

# 20250106 AI Periodic Table Explained: Mapping LLMs, RAG & AI Agent Frameworks
[https://www.youtube.com/watch?v=ESBMgZHzfG0](https://www.youtube.com/watch?v=ESBMgZHzfG0)

<!-- 这是一张图片，ocr 内容为：15 62 54 61CKG. LETAIEML REACIVE VALID. MODILS LOWL P EM PAIMITIES PROMPTS EMBEDDINGS LLM LOU 2 FC 19 VX WU COMOSIT LONS FUNCEION GUORDRAILS RAG MULEIMODAL VECEOR CALL LOW了 BE FW FT AG WS DEPLOYMENT FRAMEWORY AGENT FINETUNE LED -KEAN SMALL LOU G MA TH LS SNISUAWE MULTI-AGONE SYNTHETIC THINKING INEE REE. -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767655054093-e099c2a1-54ab-4937-b44c-7fffdf39af0b.png)

+ Primitives
    - **Prompt** — 你给 AI 的指令
    - **Embeddings** — 把文本转成能比较相似度的数字向量
    - **LLM** — 能理解和生成语言的大模型
+ Compositions
    - **Function Calling** — LLM 调用外部工具获取真实数据
    - **Vector Database** — 存储和检索向量的数据库
    - **RAG** — 先检索相关文档，再让 LLM 生成答案
    - **Guardrails** — 防止 AI 输出不当内容的安全过滤器
    - **Multi-modal** — 能处理文本、图像、音频的模型
+ Deployment
    - **Agents** — 能自主规划、执行、迭代直到完成目标的 AI
    - **Fine Tuning** — 用特定数据训练模型，把知识烘焙进权重
    - **Framework** — 搭建 AI 系统的开发平台（如 LangChain）
    - **Red Teaming** — 模拟攻击来测试 AI 的安全漏洞
    - **Small Models** — 蒸馏后的轻量模型，快且便宜
+ Emerging
    - **Multi-agent** — 多个 AI 协作分工解决复杂问题
    - **Synthetic Data** — 用 AI 生成训练数据来训练 AI
    - **Interpretability** — 理解模型为什么这样做（打开黑盒）
    - **Thinking Models** — 会花时间推理的模型（如 o1）
+ **生产级 RAG**：Embeddings → Vector DB → RAG → Prompt → LLM + Guardrails
+ **Agentic Loop**：Agents ⟲ Function Calling → Framework
+ 下次有人向你推销一个炫酷的新 AI 功能、新 AI 产品、甚至 AI 创业点子时，试着把它映射到这张表上。他们在用哪些元素？他们在运行什么反应？他们是否缺少安全元素？他们是否过度设计了编排？他们是否在用思考模型，而小模型其实就能胜任？

# 20260105 The Tree That Thinks
<!-- 这是一张图片，ocr 内容为：WISDOM OFTEN HIDES IN UNGLAMOROUS PLACES. -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767618495495-1f17c907-1287-4744-80c9-f917a1531256.png)

+ [https://www.youtube.com/watch?v=JlJgacKxrwY](https://www.youtube.com/watch?v=JlJgacKxrwY)
+ 信息不断回到自身，不断放大、不断回响，形成一场巨大的自我强化噪音风暴，直到整个系统彻底崩溃。
+ 对任何思维系统的最大威胁，不是缺乏脑力或数据，而是无法从自身失控的反馈中挣脱出来。它真的会被自己的想法淹死。
+ 循环感觉像是一个巨大的「原来如此」时刻，好像一切都连上了，但它实际上是一种智识上的失忆症——系统已经完全忘记了原始想法是从哪里来的，只是在无尽地重新发现自己的回声。
+ 想要一切与一切同时连接，这其中有一种不成熟。真正的成长、真正的成熟是关于做出选择。是关于理解结构实际上给你自由，而不是拿走它。
+ 任何智能系统的终极目标：扎根于清晰的目标，向丰富复杂的意义世界分支延伸，最重要的是，有纪律为自己创造结构，这样它就不会成为自己聪明才智的受害者。

# 20260104 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（下）
+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 想把信息注入模型：
    - RL
    - SFT
    - LoRA
    - Prefix Tuning
    - 记忆层
+ 当你在规模上做这件事时，什么最高效？我不太确定，但我认为 **Prefix Tuning** 是一个很好的候选，因为 KV 缓存现在非常常用。
+ 记忆层。这是另一种将数据注入模型的方法，我认为很好。这就像在 MLP 上添加一个专家，但这个专家只是一个巨大的可微分查找表。记忆层很酷的一点是它是可控的
+ 现在有很多相互矛盾的证据。有人认为 **LoRA** 好，有人认为 **Prefix Tuning** 好。这些人认为记忆层好。我真的不确定，但我认为会是其中之一
+ RL 的作用是调整模型的行为模式（比如让它更礼貌、更谨慎），而不是往模型里灌入大量新的事实性知识
+ **RL vs SFT 参数需求**：RL 因为学习信号稀疏（只有 0/1），比 SFT 需要少 1000 倍的参数
+ 在很长一段时间内，基本上没有你只训练模型而从不做 **RAG** 的场景。我认为你会两者都做。也许如果你有大量文档，也许每天你做一次大训练，然后每次服务时你也做 **RAG**
+ RAG的根本问题在于chunking（分块）：即使retrieval（检索）做的足够好，也只是找到需要的一部分信息，无法真正在所有内容上进行推理。在极限情况下，有些类型的数据无论你怎么分块，你永远不会得到你需要的一切。
+ **LoRA**，也许你给每个用户训练几兆字节。在现实的短期内，可能更像是每天更新一次
+ 当你开始看到对数据集更稀疏的更新，或者有一些新数据进来但不是很多且相当频繁时，你可能想转向更接近 **Deep Research** 的推理时方法。
+ 做的好的话，提示词是隐含的。在理想情况下根本不需要提示词。把提示词压缩进权重，产生一个不需要提示词就能给出相同输出的模型，然后推理成本更低
+ 一旦你开始对模型进行多次更新，特别是当你有相互矛盾的信息时你怎么办。我认为最优的合成数据策略是在训练过程中以某种方式解决这个问题
+ 联邦学习：我认为它很难的一个问题是现在模型太大了，网络成本太高
+ 创建一个什么都不知道的模型真的很难。所以我更倾向于专业化的模型，擅长你关心的东西但不擅长其他东西，而不是倡导一个什么都不擅长的模型。
+ **时间元素**：如何处理不同时间的信息是未解决的研究问题

# 20260103 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（中）
+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 要得到更好的模型，你总得在某个地方付出代价。你要么需要生成更好的数据、在数据上花更多时间，要么需要在训练上花时间，要么需要在推理上花时间。
+ **RAG** 的好处是它基本能用，但任何更好的方案都会更贵。
+ LLM有固定的容量：每个参数可以存储 3.6 比特，所以10亿参数能存储4GB。存储无关知识会占用宝贵空间
+ 微调：
    - 会完美记忆训练数据，前缀微调的方式可以做到0损失，做到125倍的压缩比
    - 会丧失泛化能力（无法写诗）
    - 会覆盖原来就有的知识
    - 只能复述原文
+ 合成数据：
    - 他们拿一个小数据集，生成一个非常大的、更多样化的数据集，这个数据集能代表他们关心的内容。
    - 即使你没有很多数据，如果你愿意生成一个描述你现有数据的大型合成数据集，你实际上可以在上面训练模型，而且效果很好。
    - **Self-Edit**（自适应语言模型），缩写是 **SEAL**。他们让模型决定生成什么数据来让自己变得更好。在一些受限场景下，这实际上是有效的。这相当诡异。
    - 我们想把东西训练进权重。我们可以生成大型合成数据集来描述非常小的数据集，而且效果不错
+ 当更新整个参数的时候会发生**catastrophic forgetting**（灾难性遗忘）：这是一个存在很久的问题，但没人真正知道怎么解决。这非常非常难。
+ 我们有这个巨大的模型，我们在上面添加一小部分来控制它，只训练那些参数。这样我们保留了模型中的大部分信息
    - **Full fine-tuning**：更新所有参数（容易遗忘）
    - **Prefix Tuning**：只训练 KV 缓存
    - **Memory Layers**：大型查找表
    - **MoE 扩展**：在混合专家模型上添加专家
    - **LoRA**：低秩适配，只训练小矩阵
+ what properties we want？
    - 轻量级
    - Resistant to forgetting (抗遗忘性)
    - High capacity (高容量)
    - 快速推理
+ **Thinking Machines** 的新训练 API。它基本上建立在这个想法上：只要你使用 **LoRA** 并批量处理 **LoRA**，你就可以为每个人提供一个模型。
+ **LoRA 的权衡**："学得少，忘得也少"——表达能力较弱但保留更多原有知识
+ **RL vs SFT**：强化学习可能更适合参数高效方法，SFT 可能需要全量微调



# 20260102 LLM Memory: Breaking the Context Window Bottleneck - Dr. Jack Morris, Cornell（上）
<!-- 这是一张图片，ocr 内容为：CONTEXT ROT:HOW INCREASING INPUT TOKENS IMPACTS LLM PERFORMANCE REPEATED WORDS-PERFORMANCE BY INPUT LENGTH(TOKENS) CLAUDE SONNET 4 1.0 GPT-4.1 QWEN3-32B GEMINI 2.5 FLASH 0.9 AVERAGE NORMALIZED LEVENSHTEIN SCORE 0.8 0.7 0.6 0.5 102 104 103 INPUT LENGTH(TOKENS) -->
![](https://cdn.nlark.com/yuque/0/2026/png/250863/1767334806619-7a11663c-78f8-411c-9d30-d0a45eb5b312.png)

+ [https://www.youtube.com/watch?v=Jty4s9-Jb78](https://www.youtube.com/watch?v=Jty4s9-Jb78)
+ 那些更小众的、或者我称之为"长尾"的任务，对 ChatGPT 来说真的很难
+ 如何把知识注入到模型的参数里？
    - full context
    - RAG
    - weights：非常新、目前还没人做的
+ "context rot（上下文腐烂）"：上下文加东西时性能数量级的下降，：
    - 1000 token 上下文时，每秒能输出 1 万 token；128k token 上下文时，每秒只能输出 130 个
    - 问题出在一个叫"self attention"的小模块上
    - 所有输入的词都需要互相"看"对方，这造成了二次方的依赖关系。4 个 token，矩阵有 16 个元素；12 个 token，就有 144 个。
+ 计算量和模型性能之间存在固有的权衡：让模型在输入那么多 token 时"不崩溃"和让模型真正能在大量 token 间正确推理，这是两码事
+ 一旦1 万 token，模型基本就不工作了。虽然没崩溃——输出的东西语法正确、看起来有意义——但实际上没解决问题。
+ context rot方面，claude 表现最好，但评测并不好，但实际使用上是真的好
+ MiniMax M2 选择了全注意力机制
+ 向量数据库的问题：
    - 使用向量数据库没有安全优势，而且它们在大规模运行时很难。
    - 不具有适应性。所以在嵌入的时候需要加入上下文，有额外的token消耗
    - 有一些关系是无法被固定维度的向量捕获的，你必须进行推理才能回答所有可能的任务。
    - 需要跨文档"潜在推理"的问题无法解决
    - 那些文档中隐含但没有明确回答的问题，都无法被当前技术解决
+ agentic search ：它是一个可以抓取信息的模型，连续做一堆查询然后回复。并非RAG

# 20260101 How Claude Code Works - Jared Zoneraich, PromptLayer 下
+ [https://www.youtube.com/watch?v=RFKCzGlAU6Q](https://www.youtube.com/watch?v=RFKCzGlAU6Q)
+ skill:一个可扩展的system prompt
+ 统一差异格式（**Unified diff**）是一项agent的标准。
+ 未来的范式
    - 将推理模型作为一种工具来使用，
    - 我觉得我们可以从“to-do lists”中学到很多
    - Skills
+ AI therapist problem，这类问题没有“全局最大值”（即唯一最优解）
+ Codex
    - 与 **Claude Code** 非常相似，采用相同的主循环架构。有趣的是，它的核心是用 **Rust** 编写的，并且是开源的。
    - 它更多是由事件驱动的；在并发线程、提交队列和事件输出方面做了更多工作。
+ AMP
    - 愿景：我们如何构建一个能在“对智能体最友好”的环境中运行的助手？
    - 上下文管理：相比 压缩，移交（**handoff**）开启一个新线程，并把必要的信息传给它，速度很快。压缩是「有损压缩」，移交是「精准打包」
+ Cursor
    - “界面优先”而非“命令行优先”的
    - 新模型 **Composer** 经过了蒸馏。他们拥有核心数据。
+ 基准测试（**Benchmarks**）几乎没用。基准测试已经变成了营销手段。
+ 有一种可能性是我们将开始在更高的抽象层级构建代理，并且只是依赖 **Claude Code** 和这些其他代理来做很多框架和编排工作。
+ 不同的视角在代理中很重要。但不同的视角很重要，因为有不同的方法来解决问题，没有一种比另一种更好，你可能想要一个混合专家代理。
+ headless  Claude Code 的强大之处，可以自动化很多工作流程

