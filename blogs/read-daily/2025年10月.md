# 20251024 Building LangChain and LangGraph 1.0 上
+ [https://www.youtube.com/watch?v=r5Z_gYZb4Ns](https://www.youtube.com/watch?v=r5Z_gYZb4Ns)
+ 回到开端，**LangChain** 将近三年前发布时，核心包含两个部分。
    - 其一是一组“集成能力”.模型只是其中之一；还有向量数据库、文档加载器等一系列组件。
    - 另一类是高层接口，它们让上手变得非常容易。**RAG** 五行代码、**SQL** 五行代码即可跑起来。
+ 发布 **LangGraph**——那是在 **LangChain** 发布约一年半之后
+ 最初构建 **LangGraph** 时，我们着重考虑了两个方面
    - 其中之一就是你刚提到的“controllability(可控性)”
        * 很多“提示词”是隐藏的；如今我们称之为 **context engineering**（上下文工程），当时这些“隐性的上下文工程”已在默默发挥作用——利弊皆有。它让上手很容易，但一旦要定制、要修改，你就必须拥有更底层的可控性。
        * 我认为“模型能力”与“生产级可靠性”之间确实存在落差——一放大到生产，多个维度都会暴露问题，比如仅仅是用户数量和交互方式的多样化就会带来挑战。
        * 另一个维度是“the longrunning (长时间运行)agents”。它们越有用，人们越想让它们做更多事，但这也更容易“偏航/跑偏”。
    - runtime（运行时）
        * “**持久化的执行环境**”：即便 **agent** 长时间运行期间出错，也无需取消整次运行。一旦出错，可以从“**检查点（checkpoint）**”恢复，因为我们会持续缓存应用状态。
        * 因为 **agent** 运行很久，用户会着急想看进度、想互动。因此把“**流式（streaming）**”作为框架中的一等公民就变得非常关键，也对各种类型应用都有帮助。
        * 当时“**流式**”和“**人参与环（human-in-the-loop）**”这些点，我们在 **LangChain** 刚开始时并不明确，因为框架很基础，后来才“回填/改造（retrofit）”进去
        * **LangChain 1.0** 的关键点之一，就是它构建在 **LangGraph** 之上。
+ **LangChain 1.0** 的一个“核心支柱”，就是这个 **create agent** 抽象。
    - **LangGraph** 的用户对 **create react agent** 很熟悉——那是建立在 **LangGraph** 之上的 **React** 风格 agent 抽象
    - 后来我们看到，很多“长尾需求”其实一开始用 **LangGraph** 做会更容易。
    - 但真正的“核心模式”在于：大家逐渐“收敛”到把 **agent** 理解为“**LLM** 在一个循环中调用工具”的这种理念。
    - **middleware** 允许开发者在 **agent** 的核心循环“任意节点”插入额外逻辑。这也让该核心循环具备“可扩展性”，以适配多种不同应用。

# 20251023 Context Engineering for AI Agents with LangChain and Manus 5
+ [https://www.youtube.com/watch?v=6_BcCthVvb8](https://www.youtube.com/watch?v=6_BcCthVvb8)
+ 拥有一个**视角不同**的独立 agent 也很重要，它能做外部审阅；做规划时还可以使用不同的模型——比如有时 **rock** 会产出很有意思的见解
+ 很多系统按“角色”来划分 agent，我们认为那只是**类比人类公司**的组织方式，本质上是**人类上下文容量受限**所导致的。
+ **Manis** 虽是多智能体，但不按角色拆分。我们只保留少数几类：**general-executor agent**、**planner agent**、**knowledge-management agent**，以及可能的**data-API-registration agent**。我们对增加子 agent 非常谨慎，因为**跨 agent 沟通很难**；更倾向把能力做成**agent-as-tool** 的可调用工具。
+ 其实是**生硬的类比**，把**人类的组织架构（org chart）****强行套到子 agent 上。那么对你们来说，核心就是****规划**与**知识管理**。
+ **Manis** 里有一个**知识系统**。**knowledge agent** 会回看用户与 agent 的对话，判断哪些内容**应该写入长期记忆**——就这么简单
+ 我们在安全护栏上投入很多精力：至少要**阻断越界数据**。如果遭遇**prompt injection（提示词投毒）**，我们会对**出站流量**做检查——确保没有**token 信息**泄出。若用户要把沙箱内内容**输出/打印**，我们会做“**去除/净化**”流程，避免信息外泄。
+ 目前在 **Manis**，只要是**敏感操作**——无论在浏览器还是沙箱中——都需要**人工确认**；你必须同意，或者**手动接管**才能完成。要设计一个“完美方案”很难，所以我们采取**渐进式**策略：现在更多地让用户接管；等模型侧的护栏更强，再**减少人工干预**。
+ **Manis** 上线时——我们使用像 **Gaia** 这样的**公开学术基准**。但真正面向公众后，我们发现**严重“失配（misaligned）**”：**Gaia** 得分高的模型，**用户并不喜欢**。
    - 第一，也是最重要的：**每次会话结束**我们都会请用户**打 1–5 星**。这才是**金标准（gold standard）**——我们最在意**平均用户评分**。这是第一个维度。
    - 第二：我们依然会做**内部自动化测试**，并且**结果可验证**。例如我们**自建了**带**明确标准答案**的数据集。**因为市面上的大多是**阅读/静态理解类（read-on）**任务。我们设计了**执行型或**事务型（transactional）****任务；有了****sandbox**，我们可以**频繁重置**测试环境。
    - 第三点（**尤其重要**）：我们投入了**大量实习生**；诸如**网页生成**、**数据可视化**这样的产物，必须靠**真人**评估，因为要设计一个能判断“**是否好看（visually appealing）**”的**奖励模型（reward model）****非常困难——这涉及****审美（taste）**。因此我们仍然**高度依赖**（真人评估），非常、非常依赖。
+ **“可验证奖励的强化学习（RL）”**，和**“直接做工具调用型 agent”**之间如何取舍？****cloud code**** 做得非常好；他们**自建了 harness（测试/操作支架）**，并在其上做 ****RL****，对他们提供的工具**适配度极高
+ MCP 改变了格局。要支持 MCP，动作空间（action space）就不是固定的；没有固定动作空间就很难设计出好的奖励（reward），rollouts（轨迹/样本）也难以大量生成，反馈会失衡（unbalanced）。想训练一个支持 MCP 的模型，本质上就是在自建“基础模型（foundation model）”。
+ 我们在探索新路子——或可称为个性化（personalization）或某种在线学习（online learning）——但用的是不改参数（parameter-free）的方法，比如collective feedbacks（群体性反馈）

# 20251022 Context Engineering for AI Agents with LangChain and Manus 4
+ [https://www.youtube.com/watch?v=6_BcCthVvb8](https://www.youtube.com/watch?v=6_BcCthVvb8)
+ **sub agents**——我们内部称 **agent as tool**。在模型看来它仍是一个函数（比如 **advanced search**），但实际上会触发另一个 **sub agent**，并且有**固定的输出 schema**。
+ **agent–agent communication**：既要传足信息，又不能让子代理的 **prefill** 负担过重。
+ **wide research**（内部称 **agentic MapReduce**）。一次会话背后有完整虚拟机，我们通过共享同一 **sandbox** 与文件系统来传递上下文，只需传路径。
+ 主代理在派生子代理时**定义输出 schema**；子代理使用 **submit result** 工具回传；通过 **constraint decoding** 强制输出符合 schema。
+ 我们目前不使用开源模型——不是质量问题，而是**成本**。输入长输出短时，**KV cache** 至关重要；分布式 **KV cache** 自建很难，而**前沿提供商**的基础设施更成熟。
+ 我们在代理型任务上用 **Anthropic**，也关注 **Gemini** 与 **Open** 系列的进展；不同实验室分工各异。我们做**任务级**、甚至**步骤级**路由（若能校验 **KV cache** 行为），并进行大量内部评估。
+ 不同模型可承载的工具数不同，但**经验值**是尽量别超过约 30 个。让原生函数足够**atomic**（原子化）——我们只保留 10–20 个，其它都交给 **sandbox**。
+ 计算机是 **Turing-complete** 的；有 **shell** 和文本编辑器就足够“完备”。把能外包的都**offload**到 **sandbox**。
+ **Hybrid**（混合）方法很关键：纯 **code agents** 无法利用 **constraint decoding**，更容易跑偏。但处理大数据时，可把数据留在 **Python** 运行时内存里，只回传结果。
+ 们早期维护 **to-do.md**，但非常费 **token**——多达三分之一的动作都在改清单。现在改用独立的 **planner agent**（**agent as tool**），做**结构化规划**，更省 **token**。

# 20251021 Context Engineering for AI Agents with LangChain and Manus 3
+ [https://www.youtube.com/watch?v=6_BcCthVvb8](https://www.youtube.com/watch?v=6_BcCthVvb8)
+ 我们在system prompt里加了一个提示，告诉 **Manis**：“有许多预装的命令行工具放在某个特定文件夹里。”另外，对于最常用的那些工具，我们已经把它们“注入”到系统提示词里了，不过这个列表非常精简。
+ 我们不会教代理具体如何使用这些工具；我们只列出来，并且告诉代理：你可以安全地使用 **--help** 这个参数，因为所有这些工具都是由我们团队开发，因此它们的格式是一致的
+ 在 **Manis**，我们**不使用索引型数据库**，因为目前你也知道，每个会话里的每个**沙盒**都是新的，而用户希望快速交互。所以我们没有时间**临时**去构建索引，更像 **cloud code** 的做法。我们依赖类似 **grep** 和 **glob** 这样的工具
+ 但如果你考虑要构建一些做**更长期的记忆**，或者要整合**企业级知识库**，你仍然得依赖**外部的向量索引**。不仅取决于你能访问的信息量；对于 **Manis** 来说，它运行在沙盒里；而**编程代理**通常在**代码库（codebase）****内操作。所以这最终****取决于规模**。
+ 在 **Manis** 里我们有一个叫 **knowledge** 的概念，有点像**显式记忆**。”这不会**自动**写入记忆，而是会**弹出对话框**提示：“我从上一次对话中学到了这些，你是否接受？”你可以**接受**或**拒绝**。需要用户确认
+ 我们已经对 **Manis** 做了**五次重构**——从三月上线到现在十月。
+ 模型不仅在**提升**，而且**行为在变化**。我们的内部方法是：**不执着**于**静态基准**上的分数；而是**固定**代理架构，在不同模型之间**切换**做对比。如果同一架构**从弱模型切到强模型**能获得**显著收益**，那它就更可能**具备前瞻性**
+ 因为**今天的强模型**，明天**弱模型也可能达到**，所以**切换测试**可以提供**早期信号**。我们每**一两个月**做一次**回顾**，用**开源模型**和**提前接入**的闭源模型做研究，以**提前为下一次发布**做准备。
+ **数据存储格式**的最佳实践：我们优先**按行（line-based）****的格式，这样模型可以用 ****grep**** 或按****行范围**读取。**Markdown** 有时会带来麻烦——某些模型会输出**过多项目符号**——所以我们更倾向于**纯文本**。
+ 摘要是**不可逆**的。做法是**定义一个模式（schema）**——像带字段的表单——让 **AI** 去**填空**：例如“修改过的文件”“用户目标”“我停在了哪里”等。**不要**使用**自由格式**的摘要。
+ **压缩**不仅针对**工具调用**本身；如果把中间结果**卸载到文件系统或外部状态**，很多操作就可以**可逆**：通常都自带**唯一标识**，比如**文件路径**、**URL**、甚至**搜索查询**，这些标识**天然存在**。
+ 当**工具调用**返回**令牌开销巨大的**结果时，你不想把整块内容塞进历史上下文。可选做法：**先发全量再移除**（**Claude** 采用过）、**先做摘要再发送**、或者**先发全量随后压缩为链接**——取决于场景。
+ 对于**复杂搜索**（多查询、筛要点、丢冗余），可以用**子代理（agent-as-tool）**，例如提供一个“**高级搜索**”的函数。

# 20251019 Building more effective AI agents
+ [https://www.youtube.com/watch?v=uhJJgc-0iTQ](https://www.youtube.com/watch?v=uhJJgc-0iTQ)
+ 我们在训练过程中（训练 **Claude** 处理“智能体式”任务）会让 **Claude** 反复练习“以智能体的方式行事”。我们给它开放式的问题去处理，让它在给出最终答案前，可以分多步行动并调用工具，探索所处环境与当前任务本身。
+ 通过 **RL（强化学习）** 等训练机制，**Claude** 学会了在“指导或反馈有限”的情况下，如何完成这些任务的“目标/准则”。
+ 我们把“编码”视为智能体的一项**基础能力**，会带来强烈的“外溢效应”——先在最难的环节上训练，其他事情就会相对容易。
+ 在很多场景下，“写代码产出成果物”会比“直接生成该成果物”更好——尤其当问题更复杂时，这是一条更可行的路径。
+ **Claude Code**。尽管名字叫“Code”，其实它是“通用型智能体内核”，只是**最常用于编码**。我们鼓励开发者把该 **SDK** 作为“智能体循环”的核心，这样就不用重复造轮子；并可通过 **MCP** 插入自定义工具或业务逻辑。——对，也就是说它足够可定制：能去掉“仅限编码”的部分，换上你需要的提示词或工具。
+ **Claude Skills** 将 **MD** 扩展为“可复用资源包”：不仅是说明文档，还能打包 **PowerPoint** 模板、代码辅助脚本、图片/素材等。也就是说，不仅提供“指令”，还提供“可反复使用的资源”（如管理层头像，供多份演示复用）。把这些打包给 **Claude**，它就“随拿随用”。
+ 把**多智能体（multi-agent）****当作一种“测试时计算”（test-time compute）来使用，让****Claude**——甚至是“许多个 **Claude**”——并行处理同一个问题，往往能比只用一个模型得到更好的最终答案。
+ 我的研究的一部分是在训练 **Claude** 成为**更好的“管理者”**：懂得如何给子代理**下达清晰指令**，并确保**从它们那里得到**自己真正需要的东西。
+ 简洁架构的价值——随着系统复杂度上升，“可观测性与可验证性”会变难；应从“可行的最简方案”起步（单次调用或 **Claude Code SDK** 的简单用法），只在必要时再叠加层次，否则会损害可观测性。
+ 多智能体不同于“智能体的工作流”。它指多个智能体**并发**运行（如父代理派生 5 个子代理并行处理）。我们的“深度搜索”产品：编排器生成子代理并行搜索 → 更快回答。在 **Claude Code** 中，模型会为“token 很多的子任务”启用子代理，以保护主上下文。
+ 智能体设计范式：并行化、**MapReduce** 与“测试时计算”——应用：编码；凡可并行或可 **MapReduce** 的任务；将大型多部分产出拆给多个子代理（节省上下文、加快结果）
+ 用工具与子代理协同解题——可以让代理“同质多解”或“差异化专长”；
+ 常见失效模式——**过度设计**会损失效率：交流往返过多、实际推进不足（类比：人类组织的沟通开销）。
+ 设计 **MCP**/工具时：**不要**按 **API** 一一对应，而要**贴合 UI**——模型在本质上扮演“用户”。
+ 与其做 3 个工具（载入 **Slack** 会话；用户 **ID**→用户名；频道 **ID**→频道名），让模型连调 3 次，不如提供一个“一步到位”的工具，把界面该有的信息一次性呈现（就像用户看到的 **Slack**）。**减少交互次数，配齐上下文**。
+ 智能体将优先在“可验证领域”（如软件工程）普及。关键下一步：通过“**计算机使用**”自行校验产物（写出网页应用→打开并测试→定位并修复缺陷），从而让人类不必一直充当 **QA**。

# 20251018 Context Engineering for AI Agents with LangChain and Manus 2
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1760753502130-afa9ac04-34a0-41b4-93bd-b44befa871af.png)

+ [https://www.youtube.com/watch?v=6_BcCthVvb8](https://www.youtube.com/watch?v=6_BcCthVvb8)
+ context isolation
    - **Cognition** 的那篇博文，其中警示不要使用多代理（multi-agent）架构。因为一旦有多个代理，要在它们之间同步信息会变成一场“噩梦”。
    - **Go** 语言社区有句名言：“**不要通过共享内存来通信；要通过通信来共享内存。**”这不直接针对 **agent**，有时也未必适用，但它凸显了两种模式：**“以通信为主”** 与 **“以共享内存为主”**。若把“**内存**”映射为“**上下文**”，这两者的**类比**就很清楚
    - **以通信为主”****是经典的****子智能体**模式：主智能体写**指令**并发给子体；子体的全部上下文即这份指令。
        * **Claude Code** 的典型做法：用它的“任务（task）”工具把一个独立而清晰的子任务委派给某个子代理。相反，对于更复杂的场景
    - 而对**更复杂**的场景，“**以共享内存为主**”意味着子体能看到**完整的既往上下文**（所有**工具使用历史**），同时仍有自己的**system prompt** 与**动作空间**
+ context offloading
    - 当你决定集成 **MCP** 时，你会发现“工具本身”也会占据大量上下文；上下文里工具过多会导致混乱——我们称之为“上下文混乱（context confusion）”。模型可能会调用错误的工具，甚至调用并不存在的工具。
        * 一个常见做法是对“工具描述”做动态 **RAG**，例如根据当前任务或状态按需加载工具
            + 因为工具定义位于上下文的最前部，你的 KV 缓存每次都会被重置。
            + 模型以前对“已移除工具”的调用记录仍然保留在上下文里
    - 分层动作空间（layered action space）
        * 函数调用（function calling）
            + 只使用固定数量的“原子函数（atomic functions）
        * 沙箱工具（sandbox utilities）
            + 每个  会话都运行在一个完整的“虚拟机沙箱”里，它运行在我们定制的 **Linux** 系统上
            + 我们不会把 **MCP** 工具注入到“函数调用空间”；相反，所有 **MCP** 相关操作都在沙箱内通过命令行完成。
            + 里的权衡是：它非常适合“大输出”，但对需要低时延、与前端频繁往返的交互并不理想，因为你通常还需要把交互过程可视化展示给用户。
        * 包与 API（packages and API）
            + 我们在 **manask** 中预装了大量 **API key**，**mans** 可以用这些 key 访问 **API**
            + 这类方式非常适合“需要大量内存内计算，但不必把全部数据塞进模型上下文”的任务。
            + 由于“代码与 **API**”高度可组合（composable），你可以在“一步”里串联很多事情……但它并非“模式安全（schema-safe）”，几乎无法对“代码”做约束解码。
            + 凡是能在编译器/解释器运行时内完成的，就用“代码”；否则用沙箱工具或函数调用
+ 这五个维度——卸载（offload）、化简（reduce）、检索（retrieve）、隔离（isolate）与缓存（cache）
    - 卸载与检索能让化简更高效
    - 稳定的检索让隔离更安全；
    - 隔离会让上下文变慢、降低化简频率；
    - 更多的隔离与化简会影响缓存效率与输出质量
+ 归根结底，“上下文工程（context engineering）”是一门在多个可能冲突目标间做平衡的“科学与艺术”。
+ 回望 **Manis** 上线后的六七个月，效果提升最大的并不是更多“花哨的层”或“聪明的检索技巧”，而是“简化”：去除不必要的花招，更多地信任模型本身。

# 20251017 Context Engineering for AI Agents with LangChain and Manus 1
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1760664213835-21fc8678-fdd3-42fb-866a-c33956d6747e.png)

![](https://cdn.nlark.com/yuque/0/2025/png/250863/1760664255958-89d61397-ed7b-4c83-b621-3e8881dc55ec.png)

+ [https://www.youtube.com/watch?v=6_BcCthVvb8](https://www.youtube.com/watch?v=6_BcCthVvb8)
+ 做 **agent** 的人会发现：**上下文会不断增长**，而且是以一种特定方式增长。通常我们让一个 **LLM** 绑定若干 **tool**，并让它在循环中**自主**调用工具。
    - 每次工具调用都会产生一个“工具观测/返回”，它会**追加**到聊天消息列表。随着运行推进，消息会不断累积，出现**无上界**的膨胀。
    - **Manis** 指出，常见任务大约需要 **50 次**工具调用；**Anthropics** 也提到，生产环境中的智能体可能会经历**上百轮**对话。
+ **上下文越大，性能越下降**。
+ Karpathy： 可以把 **context engineering** 理解为一种“精细的**艺术与科学**”：在每一步只向**上下文窗口**填入**恰到好处**的信息，以对抗因自由调用工具而引发的上下文爆炸。
    - **上下文卸载（context offloading）**：没必要把所有信息都留在 **agent** 的消息历史里。可以把信息**卸载**到外部（不占用上下文窗口），并在需要时**检索**回来。
        * 一种常见做法是**使用文件系统**：把工具**冗长**的输出直接写入磁盘，只把**最小化引用**（如文件路径）回传给 **agent**——这样“**高 token** 的网页搜索结果”等就不会永远占据上下文
    - **下文压缩/缩减**：与卸载不同，这里通过**摘要**或**压缩**信息来减小体积。对工具输出做摘要很直观；**裁剪**较旧的工具消息也是一招。**Claude 4.5** 甚至**开箱即用**地支持了这一点。
        * 还可以对**整段历史**进行**压缩**——**Cloud Code** 在上下文占比达到阈值时会触发压缩。**Cognition** 的文章也谈到在**智能体到智能体的交接（handoff）****时进行****摘要**与**剪枝**
    - **上下文按需检索**：对高质量 **agent** 至关重要的，主要分成两种流派：
        * 一派是**建立索引 + 语义搜索**（如 **Cursor**，**Lee Robinson** 在 **openi demo day** 的演讲）
        * **基于文件的简单检索**（如 **glob**、**grep**），这在 **cloud code** 中被使用
    - 上下文隔离：将上下文**拆分**到多个**子智能体（sub‑agent）****中。每个子体都有自己的上下文窗口，实现****关注点分离**
    - 上下文缓存（caching）：**Manis** 在这方面有经验
+ 很多**非共识**想法往往能带来最大的启发
+ **为什么**还需要 **context engineering**？
    - 创业公司**不要过早**做**专项模型**，应尽可能**依赖通用模型**并辅以 **context engineering**。这如今几乎成了**共识**
    - 着产品成熟、开源**基座模型**变强，你会很想**选个强基座**、用自家数据**微调**，把它打造成**场景利器**。我们也试过——但这又是一个**陷阱**。
    - **MCP** 的发布让 **Manis** 从**紧凑且静态**的动作空间，转向**近乎无限可扩展**的设计。
    - 要在**边界**上**坚持立场**。眼下，**context engineering** 是**应用**与**模型**之间最清晰、最务实的**分界线**——**相信这个选择**
+ context reduction
    - 分成：压缩化（compaction），摘要化（summarization）
    - 在 **Manis** 中，每次**工具调用/返回**都有两种格式：**完整版**与**紧凑版**。紧凑版会**剔除**那些可由**文件系统/外部状态**“**重建**”的信息。信息并未丢失——只是被**外置化（externalized）**了。
    - 我们认为**可逆性**至关重要：**agent** 会基于**既往动作/观测**进行**链式推断**，你无法预知**哪一步**会在**十步之后**变得关键。**Compaction** 提供的正是**可逆的缩减**。
    - **Compaction** 终究有限；上下文仍会增长并触顶。接着我们会与**摘要化**联用——但要**谨慎**。在摘要前，先把**关键部分****卸载**到文件
    - **compaction 可逆**，**summarization 不可逆**
    - 1M上下文，在200k（20%）的时候就会出现context rot，识别**腐坏前阈值**（常在 **128k–200k**）作为**触发点**
    - 先**compaction**，再考虑**summarization**。注意 **compaction ≠ 全量压缩**：可只**压缩最老的 50%**，把**较新的**调用**完整保留**，作为**新鲜的 few‑shot 示例**
    - **Compaction** 之后要评估**释放的上下文**是否**可观**；若**很小**，再上**summarization**。做摘要时**始终**用**完整版数据**，而不是紧凑版；并**完整保留**最近几次**工具调用/返回**

# 20251016 self-driving（自动驾驶） 和 voice agent 有诸多相似之处
+  双回路架构体系： （perception layer 的 pub/sub 响应外部） + （background processes 保持连续）
+  类似self-driving的L1–L5的这种分级思维方式，对voice agents 也很适用
+  self-driving（自动驾驶）里我们见过这样的趋势：先把很多模型结合起来；等摸清“什么组合有效”，再把它们分离；然后又结合、再分离，来回摆动。voice agents 的 e2e VS 级联 也是这样的过程。这种“钟摆式的来回摆动”恰是行业前进之道——既尽可能捕获信号，又保持职责分离。
+  离不开tool-use与orchestration的 hybrid system（混合式系统）
+  需要双向流式的LLM

十年后，觉大多数 AI 都会是real time的，而语音对话是第一个成熟案例



# 20251016 Building Voice AI Like the Human Brain With Scott Stephenson from Deepgram 下
+ [https://www.youtube.com/watch?v=pmcR9mkI33E](https://www.youtube.com/watch?v=pmcR9mkI33E)
+ self-driving（自动驾驶） 和 voice agent 有诸多相似之处：
    - 双回路架构体系： （perception layer 的 pub/sub 响应外部） + （background processes 保持连续）
    - 类似self-driving的L1–L5的这种分级思维方式，对voice agents 也很适用
    - self-driving（自动驾驶）里我们见过这样的趋势：先把很多模型结合起来；等摸清“什么组合有效”，再把它们分离；然后又结合、再分离，来回摆动。voice agents 的 e2e VS 级联 也是这样的过程。这种“钟摆式的来回摆动”恰是行业前进之道——既尽可能捕获信号，又保持职责分离。
    - 离不开tool-use与orchestration的 hybrid system（混合式系统）
    - 需要双向流式的LLM
+ 十年后，觉大多数 AI 都会是real time的，而语音对话是第一个成熟案例
+ 这些系统的工作方式是，确实有某种东西在响应外部世界，这个，你知道的，感知层，发布-订阅的思维方式但同时还有一个内部的实时计算，就像是试图保持机器人存活一样，基本上是一个独立运行的程序。
    - 假如它在行走时有些绊了一下，它会先处理这个问题——“不、不，我得先别摔倒”；但它仍会“听着”外部指令，比如“向左转”等。优先级是：先不摔倒，然后再去左转
    - **voice agents** 也需要构建类似的体系
    - 你会同时收到很多输入，**CRM** 的信息在不断“触发”，诸如此类。这时系统就得想：“等等等等，我得做一个好的对话者，持续把对话接下去；后台进程在运行，我会在合适的时机插话。”
+ 类似 **L1、L2、L3、L4、L5** 这种分级思维方式，对**voice agents** 也很适用。
+ 我们现在有点处在我称之为按顺序串联 **text-to-speech** 和 **speech-to-text** 的阶段；这有点等同于用 **Python** 去控制一个 **servo**（伺服电机）那样的简单控制。
+ 我认为距离 **Boston Dynamics** 那种高级形态，路径其实很短，但我们必须解决许多问题：比如如何真正设计**fallbacks**（降级/兜底）；如何打造支持那种自主程度的**UX**（用户体验）？
+ 在**self-driving**（自动驾驶）里我们见过这样的趋势：先把很多模型**结合**起来；等摸清“什么组合有效”，再把它们**分离**；然后又**结合**、再**分离**，来回摆动。
+ 这种“钟摆式的来回摆动”恰是行业前进之道——既**尽可能捕获信号**，又保持**职责分离**。
+ 这里会反复经历“**融化—凝结**”的过程；你要根据手头的数据、客户需求、新出现的技术、可用 **GPUs**，以及为让模型更好运行而新发现的**knowhow**（诀窍）不断重构。
+ 一切**end‑to‑end** 的最终形态尚待观察
+ 但你无法摆脱**tool use**（工具调用）与**agents**（智能体），所以谈到工具调用时就不存在纯粹的端到端——现实并非如此运作。
+ 大家得接受一个事实：无论如何，到处都会是**hybrid system**（混合式系统）。
+ 问题在于：**何时**、**把什么** 组合在一起；而这由你的约束决定——你需要更快？更好、更准？有更多数据吗？等等。
+ 语音领域很酷的一点是用例极多，因此并不存在唯一正确答案。
+ **turn detection**（轮次检测）会强烈依赖你的场景：不同的**vocal patterns**（发声模式）、言语困难的人群、年长用户……这时通用方案未必好。
+ 实时适合部分场景，却不够可控；而要推动语音能力的边界，没有**tool use** 和多样化用例，很难做到。
+ 我认为未来多数 **AI** 都会是**real time**（比如 10 年后）；而现在**token count** 的大头还在**interactive**（交互式）或**batch**（批处理）模式；语音是对话领域第一个成熟的实时用例。
+ 因此必须发生一次大的转型：让实时做到低时延、高准确。实时与交互/批处理的**tool use** 本质不同：后者倾向“越大越好、参数越多越好，把数据往里**cram**（塞）”。
    - “第一次失败也无所谓——再试一次”的心态与实时完全不同；实时要求“极低时延且**always work**（始终可用）”。
    - 十年后，我们可能会说 80% 的 **AI** 都是实时的：网站实时生成，我的所作所为都由遍在的实时 **AI** 支撑。
+ 我们的 **Nova 3** 模型（以及 **Nova 2** 与更早的 **deep** 模型）本质上都是**batch**（批处理）模型，只是被“挤压改造”去以实时方式工作。
+ 用 **Flux** 会觉得“天啊，完全不同”，因为它是**real‑time‑first** 的范式。许多**transcription**（转写）与 **text‑to‑speech** 模型来自“实时不重要”的旧时代——更偏向音频**post‑processing**（后处理）或为电影/媒体生成音频。
+ 关键在**real‑time factor**：在实时回应人的同时，你转写的速度有多快？看起来相似，实则本质不同。
+ **neuroplex** 的总体路线是：为语音智能体栈的每个子组件加入**multitrack**（多轨）。
    - 在 **speech‑to‑text**（感知部分）里，文本是一条轨，**turn detection** 是另一条，所说**language**（语言）又是一条，再加上一块“**other/asterisk**”，最佳的理解方式是一个**embedding space**（嵌入空间）。
    - 若想传递**完整上下文**，就要摆脱仅限“人类可读”的表示：感知模型要接收上述多轨再加一个随时间变化的开放式**embedding**，并输出这些轨与嵌入。
    - 这如何与 **LLM** 协同？**LMs** 得重写：它们虽然逐**token** 流式生成，但几乎不支持**流式接收**——基本不存在**bidirectional streaming LLM**（双向流式 LLM）。
    - 这听起来可能“烧脑”，而且一度会很乱；就像 **AWS** 早先只有十来个选项，如今多得吓人——似乎会“永远很乱”。它会乱上一阵子，然后又**congeal**（凝聚）起来；大家会说“我需要**streaming‑embedding** 输入，是 128 维还是 256 维？”——我们将学会一套新的**vernacular**（行业术语）。
    - 客户会向所有供应商提出这些要求；要想携带**完整上下文**并实现优秀的**tool use**，整个技术栈都得重审。
    - 下一步：把这些能力注入到**perception / understanding / generation** 各环节。
    - 本质上就是全系统**speech‑to‑speech** 且保持完整上下文，但同时保留清晰的“分界”，人类可在这些离散接口处**检查**、**解释**、**调试**，甚至**施加偏置**；不同于那种完全“黑箱式”的端到端语音到语音……
    - 由对话型模型在后台**orchestrate**（编排），像机器人先处理“绊倒”那样；大约 **80% tokens** 由它承担，必要时再调用离散工具或其他模型。
+ **LLMs** 主要训练于互联网文本，但那并不是人类实际说话的方式。**Notebook LLM** 做得很出色——显然针对特定说话风格做了**tuning**（调优），效果极佳。
+ 你确实可以**风格化**某种说话模式；**podcasting** 的语气与客服通话或谈判完全不同——这对**logistics**（流程/执行组织）也有影响。
+ 仍欠缺一种“**self‑driving** 式”模型，既有**可控性**与**编排**，又能实现高**autonomy**，并具备**failovers**（故障切换）、**redundancy**（冗余）与**reliability**（可靠性）以支撑生产。

# 20251015 Building Voice AI Like the Human Brain With Scott Stephenson from Deepgram 1
+ [https://www.youtube.com/watch?v=pmcR9mkI33E](https://www.youtube.com/watch?v=pmcR9mkI33E)
+ **Deepgram ****最初从****transcription**（语音转写）起步，但现在为技术**stack**的各个“层”构建各种不同的语音模型。很早以前我们就把**end-to-end deep learning**（端到端深度学习）带进了语音领域
+ 我们团队要解决的问题是**latency**（时延）与**interruptions**（打断）之间的**trade-off**（权衡）。
+ 当下**voice agents**最大的问题之一就是“打断”。大家会尽量**crank down**（调低/压紧）**end-of-thought models**（**EOT**模型，判断一句话“思路结束”的模型）来降低**latency**，于是“时延—打断”的权衡几乎成了一个基本矛盾。
+ 如果模型只看**text output**，或只看**silence**，或只是一个**VAD detector**，就会出现大量**edge cases**（边缘情形），导致系统不是等待过久，就是误判打断。
+ **Dgram**要发布的新模型叫**Flux**。它的代号曾是**river**，因为它就像一种新的“**stream**”（流式）.它是一个**streaming-first**（优先面向流式）的模型，从零开始构建
    - **latency**被**显著降低**。而且它还包含**end-of-turn detector**（回合结束检测器）等能力。
    - 些信号都**随流式**送达，因此如果你不想，**不必**再额外配置**VAD**或其他**EOT**检测器。
    - 很多时候难以看出巨大提升，因为现在的语音已经很“像真人”；但**Flux**的**速度**和**回合结束检测**的效果**立竿见影**。
    - 回合检测**与**转写**同时进行，本身就**大幅改进**了体验
+ **Flux**是我们世界观/架构观的第一步，即我们的**neuroplex architecture**（**Neuroplex**架构）。**Flux**是其中的第一块，给整段对话提供更多**context**（上下文）
+ 把当下常见的**cascaded system**（级联系统）类比成人脑，它只在“灰质”里各干各的，彼此并不通过“白质”对话；因此**context**只能靠**text streams**（文本流水线）传递。
    - 于是就成了**speech-to-text**输入、文本输出，但**无法**传达全部信息：现在轮到谁说话、所用语言、说话者是否兴奋、语速快慢……这些额外信号被**flattened out**（被压平成单一文本）而**丢失**。
    - 而在**Neuroplex**里，**STT**、**LLM**、**TTS**等每个阶段都会让**context**在其中**流动**：既**接收**上下文，也**输出**上下文。每一层还提供**good / better / best**的档位，因此你可以在**速度/质量/成本**之间灵活**伸缩**
+ 我觉得这和**自动驾驶（self-driving）****系统的演进非常相似：你需要****perception**（感知）外界正在发生什么；接着预测“别人下一步会做什么”、我“该说什么”（**planning** 规划）；然后是“我该如何说出来”（**controls** 控制，或 **text to speech** 文本转语音）。而且这些模型往往都是**nondeterministic**（非确定性的），还要**in parallel**（并行）工作，以实现**autonomous**（自主性）
+ 你们正在走向一种与自动驾驶很相似的架构——不是“一个模型把结果**传递**给下一个”，而是各个模型都在**publishing**（发布）消息，其他模型再去**subscribe**（订阅）。
+ 可以把人脑也看作一种“模型”，但更像**pub-sub**：各模块都在“发布我所感知到的世界”。然后每个模块自己决定：我是否要关注**empathy**（共情）、**tone**（语气/语调）、**background noise**（背景噪声）？比如在自动驾驶里，有警笛声（音频），但当你要决定怎样**刹车（brake）**时，可能并不需要那路音频信号。
+ 广义的**robotics**（机器人学）——比如**自动驾驶**——让这些想法落地，也为我们开路。如果想看**voice agent**在**real time**（实时）里该如何运作，可以借鉴这些系统——像会走路的仿生狗，**unitry**／**Boston Dynamics** 等等。

# 20251014  OpenAI's new AI chip
+ [https://www.youtube.com/watch?v=9btw343FHb4](https://www.youtube.com/watch?v=9btw343FHb4)
+ **OpenAI**与**Broadcom**宣布达成战略合作，将部署**10 gigawatts**（10吉瓦）规模、由**OpenAI**设计的**AI accelerators**（**AI**加速器
+ 从很多角度看现在的基础设施建设者，你会说：这是人类历史上规模最大的联合工业项目。我们正在定义文明的下一代“操作系统”。‘
+ 当我们意识到全球对产能——尤其是**推理产能（inference capacity）**——的需求之大时，我们开始思考：能否做一颗**专门**面向这种高度特定负载的芯片？
+ 让我如此兴奋的原因之一是：通过**全栈优化**，我们能获得巨大的效率提升；由此带来更好的性能、更快的模型、更低成本的模型
+ 我们甚至把自家的模型用在这颗芯片的设计上，**提前了进度（pull in the schedule）**，并实现了**大幅面积缩减**
+ 把人类已经优化过的部件拿来，再**倾注算力（compute）**进去，模型就能给出它自己的优化方案。
+ 有一批即将出现的技术——其中一些由我们推动，例如**Snowcap**——承诺**能效提升100倍**：把**1 GW**数据中心的能力压缩到**10 MW**能耗实现。不仅是**Snowcap**，整套**颠覆性**技术将在本十年后半段开始落地。

# 20251011 Building with MCP and the Claude API
+ [https://www.youtube.com/watch?v=aZLr962R6Ag](https://www.youtube.com/watch?v=aZLr962R6Ag)
+ “实现一次，处处配置”。**Claude Code** 与 **Claude.ai** 上的网页搜索功能保持一致。也就是在有大量应用需要类似连接时，打造“通用兼容性”。
+ 它可能是“史上增长最快的开源协议”。— 哇。— 增长相当“平流层级”的快，需求极其旺盛……
+ “恍然大悟”的转折是支持远程 **MCP**：过去一切都得用户本地跑，提供方无法托管。原生支持远程托管后，配置成本大幅下降，用户可快速上手。
+ **Context7**：通过抓取并维护最新文档（如 **Next.js**、我们的 **API**），弥补模型的知识截止；配置一次，**Claude** 即可获取最新内容。
+ 喜欢把 **Playwright** 当 **MCP** 服务器，让 **Claude** 能像用户一样点击操作浏览器。它原本能读 **CSS/HTML**，但不能“看”页面；**Playwright** 补上了这点。
+ 每次请求最终都会拼成“一条提示词”——连 **JSON** 的描述字符串也会进入发给 **Claude** 的最终提示。
+ 反模式：把请求塞满工具/服务器——既费 **token** 又让模型混淆（如 **Linear** 与 **Asana** 都有 _get project status_）。要“确定性”，只保留当下相关的；旧话题应裁剪。
+ 不存在“神奇上限”：上下文窗口有限；每个服务器都会带来函数定义吞噬 **token**；更少但更相关的工具通常更好。
+ **MCP** 接口设计不同于传统 **API**。在 **LLM** 语境下，用少量工具（如“**get info**”）+ 良好的自然语言描述，可替代一堆细粒度端点；更友好也更高效。
+ 会有“魔法感”：增加服务器的组合会出现意料之外的涌现特性。
+ 我们做了一个知识图谱服务器，只有两个工具（“创建记忆”“连接记忆”），界面极简。

# 20251007 Claude Coded: Sonnet 4.5, Claude Code 2.0, and more.
+ [https://www.youtube.com/watch?v=Yct0MvNtdfU](https://www.youtube.com/watch?v=Yct0MvNtdfU)
+ **Claudson 4.5** 已经在所有你能获取 **Claude** 的地方上线了，而且它是当今世界上最强的代码模型
    - SweetBench** 基准上处于领先地位，经过验证的得分为 77.2%。
    - 在复杂任务上能持续专注超过 30 个小时。
    - **OS World** 测试上——这是一项评估 AI 能否像人类一样实际使用电脑的测试——**Claude** 的成绩从 4 个月前的 42% 跃升到如今的 61% 以上。
    - VsCode 插件，你通过专用的侧边栏面板实时查看 **Claude** 所做的更改，该面板能以内联差异的形式展示修改内容。
    - 一个全新的 **checkpoints** 功能，它让你可以放心地运行大型任务，并在需要时瞬间回滚到先前状态。
+ **Context editing** 会在接近 Token 上限时，自动清理上下文窗口中陈旧的工具调用与返回结果
+ **memory tool** 允许 **Claude** 通过基于文件的系统，将信息存储在上下文窗口之外并进行查阅。
+ **Cloud Agent SDK**（由原 **Cloud Code SDK** 更名而来）向你开放与 **Cloud Code** 同等的核心工具、上下文管理系统与权限框架，帮助你构建自有智能体。

# 20251006 they are lying to you about AI development 2
+ 2026 年年中，模型将能**自主**连续工作**整整一个工作日（8 小时）**
+ **在 2026 年底之前**，至少会有一种模型在**多个行业**达到人类专家的表现。而到 2027 年底，模型将在许多任务上**经常**胜过专家。
+ whistleblower：吹哨人（揭露机构不当行为者）
+ **Goodhart’s law**（古德哈特定律）：当**指标**变成**目标**而非参考时，麻烦就来了。评测基准在大家开始**对着考点刷分**之前还算可信；一旦“高分”成了目的，体系就会失真。
+ “**心智的自行车**”（常归因于 **Steve Jobs**）：人类本身并不快，但有了**自行车**效率奇高；计算机是**心智的自行车**——我们创造的工具让我们更强。
+ “**AI 不是工具，而是竞争者**”AI 越强，人类越不值钱
+ 为什么**计算机**曾加剧不平等，而“**Chad GBT**（指 **ChatGPT**）”似乎在改善？90 年代计算机扩大了**工资差距**；而现在不少研究发现 **AI** 对**弱势/困难群体**的帮助**大于**对专家的帮助。
    - 想想你用 **ChatGPT** 的方式：**迭代**、**打磨**、**发现改进机会**。
    - 不是一问了之，而是来回对话半小时，直到确认答案**正确**。
+ 研究者把**认知工作**拆成三部分：**执行**、**机会判断**、**收益判断**。
+ **AI** 在**执行**上越强，你的**判断力**就越有价值。这是**乘法效应**，而非**替代效应**
+ 数学证明了一个**反直觉**结论：工具越强，会**扩大**能**发现机会**者与不能者之间的差距。
+ 为什么 **AI** 眼下看起来在**走向不平等**？因为我们仍在**第一阶段**——它主要在**弥补技能差异**。**处境艰难的员工**获得了巨大的提升；而**专家**本就擅长**执行**
+ **ADHD** 人群从 **ChatGPT** 等聊天机器人中**受益显著**。这类工具帮助他们**弥补因 ADHD 形成的能力缺口**，在多方面**跨越障碍**，并提升他们的**可及性/可达性**。
+ **第二阶段**将要到来：当**执行几乎免费**（人人能设计/写代码/写作）时，**唯一关键**的是*_判断力_
+ **AI 越强，完全自动化反而**更**不可能**。原因：**自动化系统**的判断是**固定的**，**难以适应**。**实时调节判断**是**人类独有**，工具越强，这点越**稀缺且增值**。
+ **AI** 越强，**决策权**应从“擅长**执行**的人”转向“擅长**看见机会**的人”。
+ 能**看见可改进之处**就是你的**护城河**。研究者称之为 **opportunity judgment（机会判断）**，它将成为**最有价值**的经济技能。
+ 单说“**AI 取代岗位**”**偏题**了：他们衡量的是**当下**的工作，而 **AI** 正在**改变“工作”的定义**。
+ 真正的问题不是“**AI 会不会替代我**”，而是“**我是否在培养驾驶更强大‘自行车’的判断力**？”——而这些“车”**很快会更快**。

# 20251005 they are lying to you about AI development
+ [https://www.youtube.com/watch?v=6Iahem_Ihr8](https://www.youtube.com/watch?v=6Iahem_Ihr8)
+ 关于AI整体走向，有些常被忽略的事实——它要比许多人预期的更“细腻复杂”。
+ 在**黑箱技术**下，其极限是**双重指数级（doubly exponential）**。而他们是借助 **Chad GPT**——确切说是 **GPT5**——完成关键步骤的。
+ **2025 年 9 月**，他认为 **AI** 已经能涉足“在人类智识活动中最具人类特质”的一项：**证明量子复杂度类之间的 oracle separation**。**GPT5** 也许写不完整篇论文，但它能**帮你解套卡点**——这是一个绝佳“甜蜜点（sweet spot）”。
+ 能“补上你**理应**想到的那点洞见”的 **AI** 是**大事**：它加速的是**发现过程本身**，而不只是 **LaTeX** 排版或参考文献。本文只是“成千上万案例中的一个缩影”。——事实是，聊天模型正帮助研究者推动**新的科学发现**。
+ **Google Deep Mind** 的 **Alpha Evolve** 展示了类似案例：改进 **Gemini** 训练流程，提高数据中心效率，节省数百万成本；并在数周内优化硬件规格，对比人类工程师可能需要六个月。
+ 之所以很多人“感觉不出模型在进步”，是因为模型的智力在某个时刻已经**超过**了**X/Twitter** 的平均用户。我这里转述可能不够准确，但核心是：仅凭聊天感受不出提升，并**不意味着**模型没有提升。像 **Julian**、**Scott Aronson** 这样的人**能**察觉到差异。
+ **Meter Research**。他们**衡量**模型完成**长任务**的能力，发现模型**可完成的任务时长**大约**每 7 个月翻一倍**。
+ **“每 7 个月翻倍”是在追溯到 2020 年之前**时的趋势；若只看 **2024 年以来**（红线），斜率更陡——**每 4 个月翻倍**。

# 20251004  Building the future of agents with Claude
+ [https://www.youtube.com/watch?v=XuvKFsktX0Q](https://www.youtube.com/watch?v=XuvKFsktX0Q)
+ Claude Developer Platform ，以前叫 **Anthropic API**
+ “智能体”几乎已经成了一个**热词**：现在好像人人都在做“智能体”，而当一个行业术语热到这种程度，**定义**就会变得模糊，仿佛什么都能叫智能体。
+ 在 **Anthropic**，我们更看重的一点是：**模型具备一定自主性**——能自己选择要调用的工具、发起调用、处理结果，并据此决定**下一步**。作为一家**基础研究实验室**，我们更关注模型的**推理过程**及其“如何作出决策”，这也是我们认为**智能体**的关键要素。
+ “**agentic** 的做法”更妙的一点在于：随着模型每隔几个月升级一次、**智能上限**提高，如果采用更**纯粹的智能体范式**，这些服务**会随之变得更好**。相反，若你的工作流里**脚手架（scaffolding）****太多，就等于给模型套上了****边界**；某些情况下没问题，但这样可能**无法充分利用**下一代模型带来的**更高智能**。
+ 围绕模型的**框架**演进很快，用来**编排（orchestrate）****智能体、榨取模型能力。但行业里的共识在于：不少框架****过重**、**意见过强（opinionated）**，于是又有人回到“一个循环就够了”的主张。
+ 我们的立场是：许多场景里**确实是一个循环**，但我们能做的“独门功夫”在于**提供工具与特性**。我们希望提供**适度有观点**但**不至于过重**的框架/工具，避免**挡住**模型本该发挥的能力。
+ 我们称之为**“给模型解开镣铐（unhobble）”**。模型本来就有很多能力；即便是**现世代模型**，其智能仍**未被完全解锁**。只要把**需要的工具**给它，让它**自由使用**，就能获得很好的结果。
+ **智能施用位置**的转移：从**开发者主导**转向**模型自我摸索**。
+ —— 这很令人兴奋：作为开发者，我的创意总有边界、能想到的用例有限；但模型会自己**想办法**把事做成。所以，**“解束缚”模型**真的很棒。
+ **操作层面**，首推 **Claude Code SDK**。我们围绕模型构建了一个**智能体“挽具”（harness）****来驱动那个循环，自动完成****工具调用**与**功能使用**。它最初为**编码场景**而生，但我们很快意识到它其实是一个**通用的智能体“挽具”**。这个 **SDK** 提供了**开箱即用（out-of-the-box）****的方式来****原型化**智能体，你无需亲自搭循环、写工具调用。它构建在 **Messages API** 和前面提到的那些工具之上。
+ 当你把其他一切都移除后，剩下的就是**智能体循环（agentic loop）**，而你真正需要的只是一个**极简**的内核。
+ **在 ****Claude Code SDK**** 之前，大家都在各自实现某种**提示缓存（prompt caching）、**工具调用**与**循环**的管理方案。
+ 能**清晰表述**你对智能体项目的**预期产出**，非常有助于**界定智能体边界/范围**。
+ **SDK** 提供的是一个可部署在任意位置的**智能体循环运行时**。我们把 **SDK** 解锁的能力**上推到“更高阶抽象”**：把**循环**与**工具调用自动化**都**交到你手里**，再据此打磨**开箱即用、可规模化**的**场景化解决方案**。
+ 帮助用户**抬高智能上限**，拿到**最佳结果**。更高阶抽象不仅仅是为了“更容易”，也因为我们**贴近研究与推理（inference）**，能确保我们的**抽象**与**智能体循环**与 **Claude** 的协同**极强**。
+ 随着任务**变长**、工具**增多**，一个持续的大问题是：**长任务的可观测性（observability**。大家都想要好结果，但可能需要**纠偏（steering）**、**提示调优（prompt tuning）****或****重构工具调用**。我们有能力**在平台上逐步提供可观测性**——这是我们的**重点方向**。
+ 我们新增了**上下文管理**能力：在智能体循环里，你可能会发起 **10–100** 次**工具调用**，单次消耗 **100–1000** 个 **token**。我们允许**模型**移除**较早且不再需要**的**工具调用痕迹**。**清理提示（declutter the prompt）** 后，模型往往**更能聚焦**。
+ 我们还加入了**智能体记忆**：人类**重复执行**任务会**越做越好**，因为会总结**启发式**（比如“这个检索用 **Wikipedia** 更好”）。我们给模型一个**记笔记的记忆工具**；当**卡住（stumped）或开始新任务**时，它可以**回看笔记**。

