# 20251214 Hard Won Lessons from Building Effective AI Coding Agents – Nik Pash, Cline 下
+ [https://www.youtube.com/watch?v=I8fs4omN1no](https://www.youtube.com/watch?v=I8fs4omN1no)
+ 我们想淘汰两类任务：太简单的“琐碎任务”，以及没有可靠开始/结束状态的任务。
+ 把真实世界的编程数据转换成 **RL environments**”这一过程完全自动化，用来训练模型”
+ 「最终产物（Outcome）」长什么样：**把一个真实任务做成“可复现、可打分”的容器化 RL 环境**，然后用统一的 harness 去跑不同 agent / 不同模型，并输出可对比的结果。
+ **评测的“结果”不是一句主观判断，而是一套可复制的环境 + 可自动验证的 verifier（通常靠 unit tests / outcome checks）+ 全过程记录**。
+ 当构建过程越来越自动化之后，瓶颈会从“工程实现”转移到：**收集更多高质量的真实工程任务**（数据/任务供给变成最难的部分）。
+ Truth Nuke：
    - 各大做智能体的实验室/公司，拥有当今最丰富的一份“真实世界编程任务”数据集
    - 把它封闭起来，会损害科学进步，并拖慢最前沿能力的提升
    - 做开源的 agent 实验室/团队，有责任去发布“环境”
+ Cline-bench：你照常做开源项目开发，只要开启 provider 并 opt-in，你的真实任务就可能被转成可复现的 RL/eval 环境，进而构成开源 benchmark + 训练数据。

# 20251213 《Domain Modeling Made Functional 》Introducing Domain-Driven Design 1
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1765625508873-003d5635-dfd7-4088-91f6-b41e4f835254.png)

+ DDD不适用于系统软件、游戏等等，它对商业软件和企业软件尤其有用，因为在这些场景中开发者必须与其他非技术团队协作
+ 反馈回路：开发团队会定期向领域专家交付一些东西；领域专家可以迅速纠正任何误解，供下一次迭代使用。
+ （最重要的）**source code** 本身都共享同一个模型，代码会被设计成直接反映这套共享的心智模型——这就是 **domain-driven design** 的目标
+ 这种清晰度往往会揭示哪些组件价值更高，从而让更多开发精力聚焦在它们上面，而把更少精力放在低价值组件上。
+ 共享模型指导原则：
    - 关注业务事件和工作流，而不是只盯着数据结构。
    - 把问题域拆分成更小的子域。
    - 在解决方案中为每个子域建立各自的模型。
    - 建立一套通用语言（称为 **“Ubiquitous Language”**），让项目中所有参与者共享，并在代码中无处不在地使用它。

# 20251213  Hard Won Lessons from Building Effective AI Coding Agents – Nik Pash, Cline 上
+ [https://www.youtube.com/watch?v=I8fs4omN1no](https://www.youtube.com/watch?v=I8fs4omN1no)
+ ![](https://cdn.nlark.com/yuque/0/2025/png/250863/1765607674920-61979477-61a6-4376-9afe-db597a7c8a42.png)
+ 一个“残酷的事实”开始：很多年来，我们一直靠在弱模型外面搭一堆聪明的“脚手架”来弥补它们的不足。
    - 现在做 **agents** 的瓶颈不再是那些“聪明小技巧”了，真正的瓶颈是“模型本身”。
    - 这些索引系统、搜索套路、**RAG** 层、函数调用脚手架——它们其实都是干扰项，是旧时代用来“凑合应付”的机制。
    - 每一轮发布周期（每次大版本迭代），模型能力的提升都会把你那些“工程上的小聪明”直接冲刷掉、变得不重要。
    - “极简主义”才是赢家：用最基础的工具——终端、**grep**、文件系统，以及原生的工具调用引导（shaping）就够了。Stop overthinking
+ That’s it. Stop overthinking：
    - 适配不同 API 形态（API shapes）：对 Chat Completions / Anthropic / Responses 等不同接口风格做地道的兼容封装。
    - 回填推理轨迹（reasoning traces）：在适用时把过程状态/轨迹 rehydrate 回模型调用里，保证上下文连续。
    - 按模型选择工具调用方式：原生 tool calling 强的用 structured output；不强的用 XML tool calling。
    - 微调系统层提示与反馈：调整 system prompt + system feedback，贴合各模型的 quirks（小毛病/偏好）。
    - 路由到靠谱的 provider
+ 结论：
    - 再聪明的 agent 工程也**不会**让模型变强；让模型变强的是 **benchmarks**。
    - 所有前沿模型（frontier models）的进步，最终都能追溯到它们训练时所“对着练”的那些环境。
    - 推理能力的每一次提升，都来自一个 benchmark。
    - agent 可靠性上的每一次“重大突破”，都来自 **RL** 训练环境。
+ **Gemini 3.0** 在 **Terminus** 上的得分，比全球绝大多数“模型 + agent 组合”都更高，而且几乎是开箱即用的效果。
+ **Terminus** 的核心点就在于：它没有花哨的工具调用，也没有所谓的“上下文工程”功能。所以结论就是：能力（capability）胜过脚手架（scaffolding）。
+ 整场分享的核心结论就是：如果你在做 agents——放轻松。那些自以为很聪明的工程小技巧，收一收。别想太多。
+ _model agnostic_：字面“对模型无偏好”；常指“平台不押注单一模型供应商”。
+ 模型并不是“凭空”就变得更会用工具了；它们之所以进步，是因为人们搭建了 **RL** 环境，逼它们反复练习。
+ 处理失败、反复重试这些能力很关键：我们看到的每一次推理能力跃迁，都来自 benchmark；而每一次 agent 可靠性的跃迁，则来自 **RL** 环境。
+ 说白了，benchmark 就是一个“环境”——比如一个 **Docker** 容器，你把 agent 丢进去让它自由折腾。
    - 它包含一个起始状态（代码快照）以及一个起始 prompt。
    - 最后还要有一个 verifier，用来检查最终状态是否正确，或至少是可接受的。
    - **RL** 环境和 benchmark 有什么不同？其实差别并不大；唯一真正的区别在于 reward（奖励）怎么被使用。
    - benchmark 用来“测量”模型；而 **RL** 环境用来“提升”模型——分数不会只停留在排行榜上，而是会用来更新 policy model 的参数权重。
+ **Klein** 如何把真实工程任务“流水线化”成可训练的 **RL** 环境：
    - 先用 sub-agents 并行筛任务（看来源、过程、结果，并淘汰 trivial/不可复现任务），
    - 再做环境考古复原、容器化（**Docker**、移除 **Git** 防 reward hacking），
    - 最后难点落在“怎么设计一个好 verifier”。
    - 

# 20251212  Build a voice agent with LangChain
+ [https://www.youtube.com/watch?v=kDPzdyX76cg](https://www.youtube.com/watch?v=kDPzdyX76cg)
+ “**create agent**”是我们在发布 **LangChain 1.0** 时推出的一种新模式。
+ 它提供了一种方式，让你可以直接使用一个“预先封装好的、支持工具调用的智能体”。
+ 在正常人类对话里，你说完话后，对方通常大约在 250～750 毫秒内就会接话回应。
+ **Time to first bite**：当用户（或智能体）说完话之后，要过多久你才能在耳朵里听到返回的音频？
+ **transcription accuracy**（转写准确率）：当我在说话时，我的话被转成文字并传给智能体时，到底准不准？
+ **turn detection**（轮次/轮替检测）：我什么时候算“说完了”，智能体应该在何时开始回答？
+ 实时模型在历史上往往一直会慢半拍（lagging behind）。
+ 意图识别（intent detection）并不是“开箱即用的标准能力”。而实时模型的一个新颖之处在于：它能直接从音频流（audio stream）里理解语气（tone）和意图（intent），并用这种信息去影响自己的
+ 有些 **speech to text providers** 已经把 **VAD** 内置（built in）了。比如这个例子里我们用的是 **Assembly AI**，它本身就内置了 **VAD model**。所以如果我们能尽量把事情“交给同一个流程/同一个进程”（defer to one process）来做
+ 智能体时还可以考虑另一个做法：**mimic**（模拟）一个额外的步骤，用一个本地模型去理解说话者的意图（**intent**）和语气（**tone**），然后把这些信息作为上下文（**context**）传递给智能体（**agent step**）。

# 20251210 Agent Reinforcement Fine Tuning – Will Hang & Cathy Zhou, OpenAI
+ [https://www.youtube.com/watch?v=p1CmPZ2j6Lk](https://www.youtube.com/watch?v=p1CmPZ2j6Lk)
+ 让agent和model不同的地方在于：它具备与外部世界交互的能力。它可以自己完成任务，把事情独立做完，而不用每一步都通过你来操作。
+ 这些智能体并不是“瞎”调用工具，它们在调用工具的同时也在进行推理
+ 它们与外部世界的交互（比如调用工具），和它们的推理轨迹是交织在同一个上下文窗口里的。
+ 你可以把任务本身简化，也可以在任务周围加上更好的“护栏”（guard rails）。
+ Fine-tuning，就是在你的任务上对智能体做端到端训练，通过调整模型的参数权重，让它在这个任务上的表现更上一层楼。
+ **agent RFT** 会根据你自己定义的学习信号来调整模型权重，从而教会模型：什么是好的行为，什么是糟糕的行为。在训练过程中，智能体会尝试很多种不同的工具调用方式，来完成你的任务。
+ **agent RFT** 的好处：它能帮助你提升推理类模型的表现，尤其是那些需要多步调用工具、与外部世界反复交互来完成任务的模型。
+ **agent RF** 在样本效率上也很高。我们见过有用户真的只用了大概 10 条示例数据，就已经取得了不错的效果，这其实挺惊人的。
+ **domain shift**领域偏移：你的环境、你的“世界”，往往和我们在内部训练模型时所用的世界不太一样。
+ **agent RFT** 可以通过这套“改变权重的训练过程”，让模型重新适应你的领域，最终得到一个真正“理解你环境”的智能体。
+ 对于每一次智能体的 rollout，我们都会生成一个唯一标识符，用来标记这一次特定的 rollout；然后，我们会把在你系统里发生的所有工具调用都和这个 UUID 关联起来。
+ 只有在你已经觉得把这些传统方法都用到了极致、从任务里已经“榨干了汁”，但还是想继续突破的时候，你才会求助于 **agent RFT**，去进一步推动你这个任务的性能边界。
+ **RFT** 在学习“并行调用工具”这件事上特别擅长。
+ 事实证明，我们并不需要特别庞大的 kernel 数据集，也能训练出一个会写 kernel 的好模型——前提是，我们要设计出一个好的奖励函数。
+ 五条关键原则：
    - 你需要有一个清晰、毫不含糊的“成功标准”。
    - 确保你的训练集和评测集尽可能“镜像”真实的线上流量
    - 你要确保在“探索”过程中，如果对同一个数据点多采样几次，模型就有机会取得更好的表现，这样它才能真正“向自己学习”
    - 尽量确保你的奖励函数是“不可被钻空子”的
    - 能让奖励更“连续”，而不是简单的“对 / 错”二元。

