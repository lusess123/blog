# 20251210 Agent Reinforcement Fine Tuning – Will Hang & Cathy Zhou, OpenAI
+ [https://www.youtube.com/watch?v=p1CmPZ2j6Lk](https://www.youtube.com/watch?v=p1CmPZ2j6Lk)
+ 让agent和model不同的地方在于：它具备与外部世界交互的能力。它可以自己完成任务，把事情独立做完，而不用每一步都通过你来操作。
+ 这些智能体并不是“瞎”调用工具，它们在调用工具的同时也在进行推理
+ 它们与外部世界的交互（比如调用工具），和它们的推理轨迹是交织在同一个上下文窗口里的。
+ 你可以把任务本身简化，也可以在任务周围加上更好的“护栏”（guard rails）。
+ Fine-tuning，就是在你的任务上对智能体做端到端训练，通过调整模型的参数权重，让它在这个任务上的表现更上一层楼。
+ **agent RFT** 会根据你自己定义的学习信号来调整模型权重，从而教会模型：什么是好的行为，什么是糟糕的行为。在训练过程中，智能体会尝试很多种不同的工具调用方式，来完成你的任务。
+ **agent RFT** 的好处：它能帮助你提升推理类模型的表现，尤其是那些需要多步调用工具、与外部世界反复交互来完成任务的模型。
+ **agent RF** 在样本效率上也很高。我们见过有用户真的只用了大概 10 条示例数据，就已经取得了不错的效果，这其实挺惊人的。
+ **domain shift**领域偏移：你的环境、你的“世界”，往往和我们在内部训练模型时所用的世界不太一样。
+ **agent RFT** 可以通过这套“改变权重的训练过程”，让模型重新适应你的领域，最终得到一个真正“理解你环境”的智能体。
+ 对于每一次智能体的 rollout，我们都会生成一个唯一标识符，用来标记这一次特定的 rollout；然后，我们会把在你系统里发生的所有工具调用都和这个 UUID 关联起来。
+ 只有在你已经觉得把这些传统方法都用到了极致、从任务里已经“榨干了汁”，但还是想继续突破的时候，你才会求助于 **agent RFT**，去进一步推动你这个任务的性能边界。
+ **RFT** 在学习“并行调用工具”这件事上特别擅长。
+ 事实证明，我们并不需要特别庞大的 kernel 数据集，也能训练出一个会写 kernel 的好模型——前提是，我们要设计出一个好的奖励函数。
+ 五条关键原则：
    - 你需要有一个清晰、毫不含糊的“成功标准”。
    - 确保你的训练集和评测集尽可能“镜像”真实的线上流量
    - 你要确保在“探索”过程中，如果对同一个数据点多采样几次，模型就有机会取得更好的表现，这样它才能真正“向自己学习”
    - 尽量确保你的奖励函数是“不可被钻空子”的
    - 能让奖励更“连续”，而不是简单的“对 / 错”二元。

