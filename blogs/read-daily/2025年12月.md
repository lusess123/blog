# 20251231 How Claude Code Works - Jared Zoneraich, PromptLayer 中
<!-- 这是一张图片，ocr 内容为：GOODBYE TO DAGS INPUT:API KEY SQUAD  DATASET CLEAN DATASET INPUT:N ROWS TO KEEP FILTERED  DATASET DATASET ROWS INPUT:EMBEDDING SERVICE IP TITLES INPUT:MODEL NAME TEXT CONTENTS EMBEDDINGS INPUT:VECTOR DB CONFIG EMBEDDING CONTIG INPUT: CLASS NAME METADATA EMBEDDING METRIC INPUT:BATCH SIZE DATA OBJECTS INDEX  NAME EMBEDDING DIMENSION CLIENT VECTOR DB INITIALIZE VECTOR DB INDICES PUSH TO VECTOR DB RESET VECTOR DB -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1767145543132-8820f0b8-da7d-4483-9229-daf0c5b4e991.png)

+ [https://www.youtube.com/watch?v=RFKCzGlAU6Q](https://www.youtube.com/watch?v=RFKCzGlAU6Q)
+ Bash 是通用适配器
    - 生态好，可组合，按需使用
    - 一次性 Python 脚本 → 运行它 → 删除它
+ to-do lists是一个Function call
    - 同一时间只能有 **一个** 任务处于 **进行中** 状态
    - 如果遇到阻塞/错误 → 保持任务为 **进行中**，并创建新任务
    - 倚赖模型推理来遵循待办事项列表指令
    - 无客户端验证或强制执行
    - 首次推理轮次会生成一个稳定的、类型化的计划（JSON/块）
    - 后续轮次可以对计划进行补充（添加/移除/重新排序）
    - 稳定的 **id** = hash（title + scope + paths）
    - 在UI中显示计划的增量
+ 一个预测：在不久的将来，你所有的 ChatGPT 窗口、所有 Claude 窗口都会配备沙盒。
+ 从复杂DAG到简单代理循环的范式转变
+ 理想的中间立场是使用这种代理范式——一个主循环加上工具调用，但让你的工具调用非常严格。
+ 于边缘情况，把它放进一个结构化的工具中，然后你可以对其进行评估、版本控制等等。
+ 异步缓冲(H2A):
    - 解耦工具 I/O 和推理
    - 将完整日志保存在磁盘，精简上下文传递给模型
    - 当达到容量时，丢弃中间部分并总结头部/尾部
+ **自动上下文压缩器 (wU2)**
    - 在大约 92% 的上下文窗口使用时触发
    - 总结对话内容
    - 将重要信息移至长期存储（Markdown）
+ think"、"think hard"、"think harder"，而"ultra think" 是触发短语
+ 沙箱与权限
    - WebFetch 和 Access 在隔离容器中运行：
        * 它某种程度上把它放进一个子代理中
    - 最复杂的客户端代码是权限系统
    - 命令前缀分类器（检测 git log, npm test 等
+ 每个工作者都有自己的短期上下文；主代理分叉，子代理收集数据，最终由主代理聚合。
+ 基于泄露的系统提示词：
    - 返回不要超过4行，除非询问细节
    - 不要使用 "Here is..." 或 "I will..." —— 直接做
    - 使用工具完成工作，而不是文字解释
    - 匹配项目中现有的代码风格
    - 除非要求，否则不要添加代码注释
    - 并行运行命令，广泛搜索，使用 TodoWrite 跟踪任务

# 20251230 How Claude Code Works - Jared Zoneraich, PromptLayer 上
<!-- 这是一张图片，ocr 内容为：WHY IS CLAUDE CODE SO GOOD? #1 SIMPLE ARCHITECTURE #2 BETTER MODELS #3 BETTERMODELS #4 BETTERMODELS -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1767059111468-3a6d6a96-74b8-4b4d-80f2-dd71bc47cfd9.png)

+ [https://www.youtube.com/watch?v=RFKCzGlAU6Q](https://www.youtube.com/watch?v=RFKCzGlAU6Q)
+ promptlayer核心理念是：我们相信要做严谨的提示词工程，也要做严谨的 agent 开发。我们认为产品团队要参与，工程团队也要参与
+ 我是个超级拥趸。我们甚至真的把整个工程团队的工作方式，围绕 Claude Code 重构了一遍”
+ 我们定了个规矩：如果用 Claude Code 能在一小时内搞定的事，直接做掉；别把它排进优先级列表里
+ **Anthropic** 把这种思路叫做 “AGI pill”：别为了今天模型的缺陷去过度工程化地绕来绕去。
    - 很多“突破”其实挺无聊：就是 **Anthropic** 发了个更强的模型而已。
    - **cloud code** 哲学是：忽略 embedding，忽略分类器，忽略（各种）模式匹配
        * 比如用 “grep”（本地搜索命令工具）来替代 **RAG**
        * Cursor 1.0： 会在本地建一个向量数据库来理解 repo，并做一大堆检索研究
        * cloud code：放个 Markdown 文件就行了。用户需要时自己改；agent 需要时自己改
    - 不再搞那种 master prompt 分叉成三条、再分叉成四条的复杂流程；实际上只需要少量、简单的工具调用就行
+ **cloud code**（以及今天的大多数 coding agent）本质上就是一个带工具调用的 `while` 循环。Anthropic称为n0
    - 当我讲这些工具时，记住一件事：它们全都是“人类任务”;亮点：工具在模仿人的工作流
    - **edit**关键点在于：多数时候它用的是 **diff**，而不是整份重写文件。这样更快、占用上下文更少、出问题也更少。
    - bash is All you need :你跑一个任务，它会临时生成一个 **Python** 文件，跑完再把它删掉。它之所以好用，这就是精髓
    - **web search**、**web fetch**。它们有个巧妙点：可以把这类工作“下沉”到更便宜、更快的模型去做

# 20251229 Continual System Prompt Learning for Code Agents – Aparna Dhinakaran, Arize
<!-- 这是一张图片，ocr 内容为：POST REPLY ANDREJ KARPATHY @KARPATHY WE'RE MISSING (AT LEAST ONE) MAJOR PARADIGM FOR LLM LEARNING. NOT SURE WHAT TO CALL IT, POSSIBLY IT HAS A NAME - SYSTEM PROMPT LEARNING? PRETRAINING IS FOR KNOWLEDGE. FINETUNING(SL/RL) IS FOR HABITUAL BEHAVIOR. BOTH OF THESE INVOLVE A CHANGE IN PARAMETERS BUT A LOT OF HUMAN LEARNING FEELS MORE LIKE A CHANGE IN SYSTEM PROMPT.YOU ENCOUNTER A PROBLEM, FIGURE SOMETHING OUT, THEN "REMEMBER" SOMETHING IN FAIRLY EXPLICIT TERMS FOR THE NEXT TIME.E.G."LT SEEMS WHEN L ENCOUNTER THIS AND THAT KIND OF A PROBLEM, I SHOULD TRY THIS AND THAT KIND OF AN APPROACH/SOLUTION" IT FEELS ITES FOR YOURSELF, I.E. SOMETHING LIKE THE "MEMORY" MORE LIKE TAKING NOTES FOR FEATURE BUT NOT TO STORE PER-USER RANDOM FACTS, BUT GENERAL/GLOBAL PROBLEM SOLVING KNOWLEDGE AND STRATEGIES. LLMS ARE QUITE LITERALLY LIKE THE GUY IN MEMENTO, EXCEPT WE HAVEN'T GIVEN THEM THEIR SCRATCHPAD YET. NOTE THAT THIS PARADIGM IS ALSO SIGNIFICANTLY MORE POWERFUL AND DATA EFFICIENT BECAUSE A KNOWLEDGE-GUIDED"REVIEW" STAGE IS A SIGNIFICANTLY HIGHER DIMENSIONAL FEEDBACK CHANNEL THAN A REWARD SCALER. I WAS PROMPTED TO JOT DOWN THIS SHOWER OF THOUGHTS AFTER READING THROUGH CLAUDE'S SYSTEM PROMPT, WHICH CURRENTLY SEEMS TO BE AROUND 17,000 WORDS, SPECIFYING NOT JUST BASIC BEHAVIOR STYLE/PREFERENCES (E.G. REFUSE VARIOUS REQUESTS RELATED TO SONG LYRICS) BUT ALSO A LARGE AMOUNT OF GENERAL PROBLEM SOLVING STRATEGIES,E.G.: "IF CLAUDE IS ASKED TO COUNT WORDS, LETTERS, AND CHARACTERS, IT THINKS STEP BY STEP BEFORE ANSWERING THE PERSON. IT EXPLICITLY COUNTS THE WORDS, LETTERS,OR CHARACTERS BY ASSIGNING A NUMBER TO EACH. IT ONLY ANSWERS THE -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1766971629154-232a4649-b45b-4cb4-9ad4-8119d6200b3b.png)

<!-- 这是一张图片，ocr 内容为：代码代理的持续系统提示学习 NEW YOUTUBE.COM/WATCH?V-PPDSN... 这是@KARPATHY今年一个被低估的观察结果.现在,这是人工智能领域最 激烈争论的领域之一,涉及持续学习,GEPA和 A和评估. @APARNADHINAK 论了她今年将提示学习应用于@ANTHROPICAI的CLAUDE CODE和 @CLINE 经验 PROMPT LEARNING ON CODING AGENTS CODING AGENT: LLM EVALS: PASS WRITES CODE: RUN UNIT TESTS OR FAIL? SWE BENCH CLINE GPT-4.1 GPT-5 ENGLISH FEEDBACK WHY DID TESTS PASS/FAIL? /CLINERULESOPTIMIZED RULES WHY IS THE CODE WRONG/RIGHT? WHY DID CLINE TAKE THIS PATH? ALGORITHM:META-PROMPTING GPT-5 OPTIMIZES OLD RULESET ENGINEERING THE FUTURE OF AI AI ENGINEER CONTINUAL SYSTEM PROMPT LEARNING FOR CODE AGENTS-APARNA DHINAKARAN,ARIZE AL ENGINEER SUMMARIZE SHARE SAVE ASK -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1766971645593-eb5c28c4-1c8e-40b6-956f-f337a988a025.png)

+ [https://www.youtube.com/watch?v=pP_dSNz_EdQ](https://www.youtube.com/watch?v=pP_dSNz_EdQ)
+ Karpathi：反复迭代这些提示词，正在形成一种新的 **paradigm**，他把它称作 **system prompt learning**。
+ 类似RL，但不同于RL，RL既很“耗时间”，又很“吃数据”，用在system prompt learning，属于overkill(杀鸡用牛刀)
+ 通常都会自带一个“你可以往后追加规则”的东西，**Cline** 这边有 **rules**，**cloud MD** 这边有 **cloud MD file**，而且它一开始是空的。
+ **LLM-as-a-judge**：写出真正高质量的 **evals**，才是你获得“最有价值洞察”的方式：你到底该做什么来改进你的 **agents**。
+ 一个巨大的 meta prompt帮助我们迭代 system prompt
+ 效果： 150 个样例训练数据迭代出来的system prompt，让claude code 提升了5%
+ Prompt Learning  VS GEPA
    - GEPA是**DSPY** 的一个 **prompt optimizer**（提示词优化器）
    - **GEPA** 需要很多很多次 **loops** 和 **rollouts**
    - 围绕使用 **English feedback** 的底层思路（**underlying approach**）其实是一样的
    - 而prompt learning花了很多时间去开发并迭代 **evals**

# 20251227 The Infinite Software Crisis – Jake Nations, Netflix（下）
<!-- 这是一张图片，ocr 内容为：THE QUESTION ISN'T WHETHER WE'LL USE AI. THE QUESTION IS WHETHER WE'LL STILL UNDERSTAND OUR OWN SYSTEMS WHEN AL IS WRITING MOST OF THE CODE. -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1766803787072-6e352ef4-bdfd-4ecb-93ec-865d8e0c7a45.png)

+ [https://www.youtube.com/watch?v=eIoohUmYpGI](https://www.youtube.com/watch?v=eIoohUmYpGI)
+ 上下文压缩的三步法
    - Research：通过有重点的反复迭代，建立“共享上下文”。
    - planning：制定结构化的、细到可执行的 **implementation plan**（实施计划）。
    - implementation：基于已验证的 **specification** 来生成代码。
+ AI时代的no silver bullet: Not better prompts, not better models, not even writing specs.真正只有一件无法回避的事，把你的 system理解到足够深，深到你可以安全地改动它。
+ 人类的The Knowledge Gap(认知鸿沟)：
    - **AI** 能在几秒内生成 1,000 行代码；
    - 但理解它可能要花上数小时甚至数天；
    - 如果真的纠缠到一团乱，甚至可能永远都理解不完。
+ 人类的Pattern Recognition（模式识别）
    - 每一次我们为了赶进度而跳过思考，我们就会失去识别问题的能力。
    - 那种会提醒你“这开始变得纠缠、越来越乱”的直觉？——只有当你真的知道自己在看什么时，它才会被触发。
    - 当我坚持要更简单的架构时，是因为我亲身经历过“另一种（更糟的）方案”带来的后果。
+ Software is a human endeavor，难点从来不是把代码敲出来；难的是一开始就知道该敲什么。

# 20251223 The Infinite Software Crisis – Jake Nations, Netflix（上）
[https://www.youtube.com/watch?v=eIoohUmYpGI&t=598s](https://www.youtube.com/watch?v=eIoohUmYpGI&t=598s)

<!-- 这是一张图片，ocr 内容为：SIMPLE VS EASY SIMPLE EASY ONE FOLD,ONE BRAID ADJACENT, REACHABLE SINGLE RESPONSIBILITY COPY,PASTE,SHIP INSTALL A PACKAGE LACK OF ENTANGLEMENT REQUIRES DESIGN,THOUGHT,UNTANGLING JUST PUT IT CLOSER RICH HICKEY, 2011- SIMPLE MADE EASY:WE USE THESE WORDS NTERCHANGEABLY,BUT THEYTE NOT THE ATAL.CONFIAN IS WHY WE'RE DROWNING IN COMPLEXITY. BEYANG LIU MAHESH MURAG KEVIN HOU END NATALIE SERRINO MPT AIE/CODE ANTHROP\C GIMLET GLE DEEPMINDGOOGLE DEEPMIND SOURCEGRAPH THE ANSWER,I THINK,COMES D ES DOWN TO TWO PRESENTED BY WORDS WE TEND TO ARIZE XGOOSE TURBOPUFFER MODAL GOOGLE DEEPMIND BAZ THE INFINITE SOFTWARE CRISISIS-JAKE NATIONS,NETFLIX AI ENGINEER 保存 剪辑 1777 分享 AIE 30.8万位订阅者 -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1766453491571-29cf0d70-537d-4d7f-9ef7-169a86c73aa0.png)

+ and now 我不得不承认：我们都在交付一些自己已经不再理解的代码
+ History Repeats（历史总是不断的重复）
    - 大型线上生产系统总会以意想不到的方式出故障——比如最近 **CloudFare**发生的事.而一旦真出事了，你最好得看得懂你正在调试的那段代码。
    - 问题在于：现在我们生成代码的速度太快、量也太大，以至于我们的理解根本很难跟得上。
    - 每一代软件工程师最终都会撞上一堵墙：软件的复杂度会超过他们能够掌控的范围。
    - 我们并不是第一批面对“软件危机”的人；但我们是第一批在这种“无限规模的生成”之下去面对它的人。
        * 1970年代，构建大型系统，我们有了C语言
        * 1990年代，个人电脑普及，人人都能写写代码，我们有OOP的Java
        * 2000年代，敏捷开发，Scrum Master 来告诉我们该干什么——不再搞瀑布式了。
        * 2010 年代，Cloud&Mobile，Devops,软件真的“吞噬了世界”
        * now，我们有了 AI,我们描述得有多快，就能生成代码有多快。
+ Easy 并不等于 Simple
    - 1986年的论文《No Silver Bullet(没有银弹)》说：真正难的不是写代码的机械部分，而是理解问题与设计方案——这一点再多工具也无法替代。
    - 有经验的工程师最后都会写出自己都看不懂的代码
    - Easy 的意思是：你能很快把东西加进系统里；simple 的意思是：你能理解自己做出来的东西。
    - 而走“easy的路”是人的本性，每次我们选择 easy，本质上都是选择“现在的速度”——换来“之后的复杂度”，这一切都是trade-off（取舍）
    - 而AI打破了这个trade-off的平衡，它把“easy那条路”做得几乎零摩擦，以至于我们都不再去考虑“simply那条路”了。复杂度会不断复利式地累积，直到为时已晚。
        * 与其说工程师是跟 “复杂度”对抗，不如说是跟“人性”对抗。AI是一个巨大的诱惑
        * 技术债不会被“识别”为债；它只会被当成更多的代码而已。
        * complexity 是easy 的反面， AI本身无法区分essential complexity（根本复杂度）和accidental complexity（偶然复杂度）
        * 当两种复杂度已经纠缠到极深时，AI 不但难以重构，甚至容易在旧逻辑上再叠新层；我们人类是能分辨差别的——至少当我们慢下来认真思考时可以
    - 

# 20251221 Dispatch from the Future: building an AI-native Company – Dan Shipper, Every, AI & I
<!-- 这是一张图片，ocr 内容为：MANAGERS CAN COMMIT CODE (EVEN THE CEO) FAVORITE BUT LIKE IS ALSO THE HORROR I HE FUTURE OF AL AI ENGINEER THINK OF OF DISPATCH FROM THE FUTURE:BUILDING AN AL-NATIVE COMPANY - DAN SHIPPER ,EVERY,AL&1 AI ENGINEER X剪辑 保存 分享 367 AIE 30.5万位订阅者 -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1766289205966-a3aa92be-da8e-4e52-89d2-7735bf4d62f8.png)

+ [https://www.youtube.com/watch?v=MGzymaYBiss](https://www.youtube.com/watch?v=MGzymaYBiss)
+ 关于Every:
    - 仅仅 15 个人就运营着四款软件产品
    - 过去 6 个月每个月的 **MR** 都以两位数的比例增长
    - 超过 7,000 名付费订阅者，以及超过 100,000 名免费订阅者
    - 总共只融资了大约 100 万美元
    - 99% 的代码都是由 **AI agents** 写的,没有人手写代码；甚至可以说根本没人在“亲自写代码”。
    - 每一个 app 基本都是由一个开发者独立搭建出来的
+ 学习如何用 **AI** 来做工程、来打造公司，这对我们所有人来说都是新课题。
+ The 10x Difference: 90% vs 100% AI Adoption
    - 一个组织里 90% 的工程师用 **AI**，和 100% 的工程师都用 **AI**——完全是两个世界。
    - 哪怕你公司里只有 10% 的人还在用更传统的工程方法，你就会被迫“整体退回”到那个传统世界里去。
+ 新的工程我们叫**compounding engineering**（复利式工程/滚雪球工程）
    - 传统工程里：每做一个 **feature**（功能），都会让下一个 **feature** 更难构建/开发。
    - compounding engineering，每做一个 **feature**，都会让下一个 **feature** 更容易构建/开发。
+ 四步闭环，把自己的 **tacit knowledge**（隐性知识/默会知识）转化成一套 **prompts**（提示词/指令）集合，让你的 **org**去使用
    - **planning**（规划）
    - **delegation**（委派）
    - **assessment**（评估）
    - **codify**（固化/成文化/规则化）
        * 找 bug、修正计划、委派任务的过程中不断“捡到”的那些 **tacit**（隐性）知识，变成一套明确的 **prompts** 集合
        * 这些 **prompts** 会被写进你的 **cloud MD** 文件、你的 **sub agents**（子智能体）、你的 **slash commands**（斜杠命令），skill,然后你就开始搭建出一个“提示词库/流程库”
+ 我们可以并行推进多个功能和 bug。
+ 因为代码变便宜了，你可以快速做原型、尝试更冒险的想法，并做更多实验
+ 公司会逐渐转向一种“**demo culture**（演示文化）”，而不是一种“书面文化”或“口头文化”。
+ **Tacit code-sharing**（隐性/默会式的代码共享）意味着：开发者可以彼此学习——而且几乎不需要额外花时间成本。
    - 直接让 **agents** 读同事的 **repo**，在自己 **tech stack/framework** 里复刻实现；
+ 一旦你采用它，就会出现很多不那么显而易见的 **second-order effects**（二阶效应/间接效应）。
+ 一个特别酷的现象：新入职的人第一天就能产生生产力。——你还可以雇一些专家级的 **freelancers**（自由职业者/外包专家）来“临时插入”帮一把。
+ 工程师不只维护自己手头的产品，也会跨到其他产品里提交修复或改进。
+ AI translates：每个团队都可以用自己最喜欢、最顺手的 **tech stack**（技术栈）和编程语言来做事，剩下的交给AI
    - 这而且如果你想雇一些顶尖的 **freelancers**（自由职业者/外包专家），它也会很有帮助。
+ 管理者也可以提交代码（甚至 **CEO** 也可以）
+ **AI** 让工程师即使在 **fractured attention**（碎片化注意力）状态下也能推进工作
+ 一种完全新的思维方式：如何去思考——甚至是“重新思考如何思考”——管理者该怎么与自己做出来的产品互动

# 20251220 Multi Agent Systems Explained: How AI Agents & LLMs Work Together
<!-- 这是一张图片，ocr 内容为：DEC MULTI-AGENT SYSTEMS FLEX NETWO FL AGENT SCALE LM DOM. SP. MAS >SINGLE HIER. (TREE) () MALF. COMPLEX/ MAFS COORD. DOM. UNPRED. RES. SCALE. SUBSC MULTI AGENT SYSTEMS EXPLAINED:HOW AL AGENTS & LLMS WORK TOGETHER -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1766201531294-cd536949-b17e-4536-b58e-26c1fc04cb98.png)

+ [https://www.youtube.com/watch?v=sWH0T4Zez6I](https://www.youtube.com/watch?v=sWH0T4Zez6I)
+ **multi-agent systems**（多智能体系统）的工作方式：许多简单的 **AI agents**（AI 智能体），每个只负责一小块任务，组合在一起就能解决巨大而复杂的问题。
+ 从本质上说，**AI agent** 是一种 **autonomous**（自主的）系统，它能够代表另一个智能体或另一个系统去执行任务。它通过设计自己的 **workflow**（工作流），并使用现有的工具来做到这一点。
+ **multi-agent systems** 更进一步：它既让各个智能体保持 **autonomous**（自治），又让它们在特定的 **agent structures**（智能体结构）里协作与协调。
+ 简单/监督式”的层级结构：其中一个智能体拥有对其他智能体的 **decision-making authority**（决策权）。
+ 同一层级上的所有智能体扮演相同角色，并拥有同等权限。并且这些智能体会进行“横向协调”（**coordinate laterally**）。
+ 高层负责协调，底层负责执行。不过要记住：权力并不一定必须是严格的“自上而下”（top-down）或“中心化”（centralized）。我们希望结构是 **dynamic**（动态的），让权力可以根据智能体的 **expertise**（专长）或具体情境（situational basis）而发生转移。
+ 信不信由你（believe it or not），多智能体系统往往会 **outperform**（表现优于）单一智能体。对智能体来说，可用的 **action plans**（行动方案）越多，就越会发生学习与反思。
+ 当一个 **AI agent** 能吸收（incorporate）其他智能体的知识与反馈时，就能实现更大规模的信息整合（information synthesis）。
+ 挑战：
    - 同LLM导致的失败
    - **coordination complexity**（协调复杂度）
        * 共享信息、解决冲突，并同步决策
    - **unpredictable behavior**（不可预测行为）
        * 问题很复杂，而且可能跨越多个领域（spans various domains），也许它资源有限，或者它需要在不断变化的环境中扩展规模，否则使用 单智能体

# 20251218  What We Learned Deploying AI within Bloomberg’s Engineering Organization – Lei Zhang, Bloomberg 下
+ [https://www.youtube.com/watch?v=Q81AzlA-VE8](https://www.youtube.com/watch?v=Q81AzlA-VE8)
+ 平台的指导原则：
    - 要让“正确的事”变得极其容易做；让“错误的事”变得夸张地难做。
    - Demo 很容易；但一旦上生产，就必须有质量控制
+ the paved path：
    - LLMs Gateway
    - MCP Directory Hub
    - 创建与部署走 PaaS
+ 关于培训：
    - 新人加入公司团队之前，先培训学习一种新的做事方式；回到团队时，他们会去挑战一些资深同事：“我们为什么不这样做？”
    - 统计数据表名：一线个人贡献者（**IC**）对AI的采用力度远强于领导层。
+ 技术把门槛（skill）拉低后，真正的差异从“会不会做”转移到“怎么判断、做什么选择（judgement）

# 20251217  What We Learned Deploying AI within Bloomberg’s Engineering Organization – Lei Zhang, Bloomberg 上
+ [https://www.youtube.com/watch?v=Q81AzlA-VE8](https://www.youtube.com/watch?v=Q81AzlA-VE8)
+ Bloommer 介绍：
    - 9,000 名工程师，有 500 多名员工专注于面向客户的 AI 产品
    - 拥有全球最大的 **JavaScript** 代码库之一
    - 旗舰产品——叫做 **Bloomer terminal**
+ 除非我们真的部署起来去试，否则不会知道“最佳做法”是什么。
+ 思考：你要怎么让你的代码库持续演进（持续变好/跟上变化）？
+ Uplift agents（代码库自动抬升/维护智能体）：你不用天天修修补补/升级维护，而是能把精力放在更有创造力、更有业务价值的工作上。
    - AI 生成的 patch **必须能用“确定性”的方式验对错**
    - Time-to-merge（合并耗时变长）
    - **从“写代码”变成“定义正确任务 + 定义可验证结果”**，这本身更难
+ Incident Response agents：一个故障/事故响应智能体
    - 告警去重、聚类、关联，先把“哪几个是同一件事”讲清楚。
    - 自动拉取“最相关的上下文包”，把关键链接/图表/日志片段集中成一个摘要
    - 做时间线对齐（告警时间 vs 发布/配置变更 vs 指标变化），帮助快速形成假设并验证
    - 自动算 blast radius（爆炸半径）+ 影响面，给出可沟通的“影响评估”

# 20251216 《Domain Modeling Made Functional 》Introducing Domain-Driven Design 2
+ 静态数据——也就是“只是放在那里、没有被使用”的数据——不会贡献任何价值。
+ 无论触发因素是什么，把它作为设计的一部分记录下来都很重要。我们把这些东西称为 **Domain Events**。
+ **Domain Events** 是我们要建模的几乎所有业务流程的起点。
+ **Domain Events** 总是用过去时来写——表示“某件事已经发生了”——因为它是一个无法改变的事实。
+ **Event Storming** 的人常说的：“有问题的人，以及有答案的人。”

# 20251214 Hard Won Lessons from Building Effective AI Coding Agents – Nik Pash, Cline 下
+ [https://www.youtube.com/watch?v=I8fs4omN1no](https://www.youtube.com/watch?v=I8fs4omN1no)
+ 我们想淘汰两类任务：太简单的“琐碎任务”，以及没有可靠开始/结束状态的任务。
+ 把真实世界的编程数据转换成 **RL environments**”这一过程完全自动化，用来训练模型”
+ 「最终产物（Outcome）」长什么样：**把一个真实任务做成“可复现、可打分”的容器化 RL 环境**，然后用统一的 harness 去跑不同 agent / 不同模型，并输出可对比的结果。
+ **评测的“结果”不是一句主观判断，而是一套可复制的环境 + 可自动验证的 verifier（通常靠 unit tests / outcome checks）+ 全过程记录**。
+ 当构建过程越来越自动化之后，瓶颈会从“工程实现”转移到：**收集更多高质量的真实工程任务**（数据/任务供给变成最难的部分）。
+ Truth Nuke：
    - 各大做智能体的实验室/公司，拥有当今最丰富的一份“真实世界编程任务”数据集
    - 把它封闭起来，会损害科学进步，并拖慢最前沿能力的提升
    - 做开源的 agent 实验室/团队，有责任去发布“环境”
+ Cline-bench：你照常做开源项目开发，只要开启 provider 并 opt-in，你的真实任务就可能被转成可复现的 RL/eval 环境，进而构成开源 benchmark + 训练数据。

# 20251213 《Domain Modeling Made Functional 》Introducing Domain-Driven Design 1
<!-- 这是一张图片，ocr 内容为：DEVELOPMENTY DOMAIN TEAM EXPERTS SHARED MODEL MENTAL OTHER CODE STAKEHOLDERS -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1765625508873-003d5635-dfd7-4088-91f6-b41e4f835254.png)

+ DDD不适用于系统软件、游戏等等，它对商业软件和企业软件尤其有用，因为在这些场景中开发者必须与其他非技术团队协作
+ 反馈回路：开发团队会定期向领域专家交付一些东西；领域专家可以迅速纠正任何误解，供下一次迭代使用。
+ （最重要的）**source code** 本身都共享同一个模型，代码会被设计成直接反映这套共享的心智模型——这就是 **domain-driven design** 的目标
+ 这种清晰度往往会揭示哪些组件价值更高，从而让更多开发精力聚焦在它们上面，而把更少精力放在低价值组件上。
+ 共享模型指导原则：
    - 关注业务事件和工作流，而不是只盯着数据结构。
    - 把问题域拆分成更小的子域。
    - 在解决方案中为每个子域建立各自的模型。
    - 建立一套通用语言（称为 **“Ubiquitous Language”**），让项目中所有参与者共享，并在代码中无处不在地使用它。

# 20251213  Hard Won Lessons from Building Effective AI Coding Agents – Nik Pash, Cline 上
+ [https://www.youtube.com/watch?v=I8fs4omN1no](https://www.youtube.com/watch?v=I8fs4omN1no)
+ <!-- 这是一张图片，ocr 内容为：AGENTS AREN'T BOTTLENECKED BY CLEVER TRICKS ANYMORE. THEY'RE 1. BOTTLENECKED BY MODELS. ALL THESE INDEXING SYSTEMS, SEARCH PATTERNS,RAG LAYERS, 2. FUNCTION-CALLING SCAFFOLDS:THEY'RE DISTRACTIONS,LEG S,LEGACY COPING MECHANISMS. 3. MODEL CAPABILITY WIPES AWAY YOUR ENGINEERING CLEVERNESS EVERY RELEASE CYCLE. SO LET'S START WITH BITTER TRUTHG MIND FOR YEARS WE COMPENSATED FOR WEAK MODELS -->
![](https://cdn.nlark.com/yuque/0/2025/png/250863/1765607674920-61979477-61a6-4376-9afe-db597a7c8a42.png)
+ 一个“残酷的事实”开始：很多年来，我们一直靠在弱模型外面搭一堆聪明的“脚手架”来弥补它们的不足。
    - 现在做 **agents** 的瓶颈不再是那些“聪明小技巧”了，真正的瓶颈是“模型本身”。
    - 这些索引系统、搜索套路、**RAG** 层、函数调用脚手架——它们其实都是干扰项，是旧时代用来“凑合应付”的机制。
    - 每一轮发布周期（每次大版本迭代），模型能力的提升都会把你那些“工程上的小聪明”直接冲刷掉、变得不重要。
    - “极简主义”才是赢家：用最基础的工具——终端、**grep**、文件系统，以及原生的工具调用引导（shaping）就够了。Stop overthinking
+ That’s it. Stop overthinking：
    - 适配不同 API 形态（API shapes）：对 Chat Completions / Anthropic / Responses 等不同接口风格做地道的兼容封装。
    - 回填推理轨迹（reasoning traces）：在适用时把过程状态/轨迹 rehydrate 回模型调用里，保证上下文连续。
    - 按模型选择工具调用方式：原生 tool calling 强的用 structured output；不强的用 XML tool calling。
    - 微调系统层提示与反馈：调整 system prompt + system feedback，贴合各模型的 quirks（小毛病/偏好）。
    - 路由到靠谱的 provider
+ 结论：
    - 再聪明的 agent 工程也**不会**让模型变强；让模型变强的是 **benchmarks**。
    - 所有前沿模型（frontier models）的进步，最终都能追溯到它们训练时所“对着练”的那些环境。
    - 推理能力的每一次提升，都来自一个 benchmark。
    - agent 可靠性上的每一次“重大突破”，都来自 **RL** 训练环境。
+ **Gemini 3.0** 在 **Terminus** 上的得分，比全球绝大多数“模型 + agent 组合”都更高，而且几乎是开箱即用的效果。
+ **Terminus** 的核心点就在于：它没有花哨的工具调用，也没有所谓的“上下文工程”功能。所以结论就是：能力（capability）胜过脚手架（scaffolding）。
+ 整场分享的核心结论就是：如果你在做 agents——放轻松。那些自以为很聪明的工程小技巧，收一收。别想太多。
+ _model agnostic_：字面“对模型无偏好”；常指“平台不押注单一模型供应商”。
+ 模型并不是“凭空”就变得更会用工具了；它们之所以进步，是因为人们搭建了 **RL** 环境，逼它们反复练习。
+ 处理失败、反复重试这些能力很关键：我们看到的每一次推理能力跃迁，都来自 benchmark；而每一次 agent 可靠性的跃迁，则来自 **RL** 环境。
+ 说白了，benchmark 就是一个“环境”——比如一个 **Docker** 容器，你把 agent 丢进去让它自由折腾。
    - 它包含一个起始状态（代码快照）以及一个起始 prompt。
    - 最后还要有一个 verifier，用来检查最终状态是否正确，或至少是可接受的。
    - **RL** 环境和 benchmark 有什么不同？其实差别并不大；唯一真正的区别在于 reward（奖励）怎么被使用。
    - benchmark 用来“测量”模型；而 **RL** 环境用来“提升”模型——分数不会只停留在排行榜上，而是会用来更新 policy model 的参数权重。
+ **Klein** 如何把真实工程任务“流水线化”成可训练的 **RL** 环境：
    - 先用 sub-agents 并行筛任务（看来源、过程、结果，并淘汰 trivial/不可复现任务），
    - 再做环境考古复原、容器化（**Docker**、移除 **Git** 防 reward hacking），
    - 最后难点落在“怎么设计一个好 verifier”。
    - 

# 20251212  Build a voice agent with LangChain
+ [https://www.youtube.com/watch?v=kDPzdyX76cg](https://www.youtube.com/watch?v=kDPzdyX76cg)
+ “**create agent**”是我们在发布 **LangChain 1.0** 时推出的一种新模式。
+ 它提供了一种方式，让你可以直接使用一个“预先封装好的、支持工具调用的智能体”。
+ 在正常人类对话里，你说完话后，对方通常大约在 250～750 毫秒内就会接话回应。
+ **Time to first bite**：当用户（或智能体）说完话之后，要过多久你才能在耳朵里听到返回的音频？
+ **transcription accuracy**（转写准确率）：当我在说话时，我的话被转成文字并传给智能体时，到底准不准？
+ **turn detection**（轮次/轮替检测）：我什么时候算“说完了”，智能体应该在何时开始回答？
+ 实时模型在历史上往往一直会慢半拍（lagging behind）。
+ 意图识别（intent detection）并不是“开箱即用的标准能力”。而实时模型的一个新颖之处在于：它能直接从音频流（audio stream）里理解语气（tone）和意图（intent），并用这种信息去影响自己的
+ 有些 **speech to text providers** 已经把 **VAD** 内置（built in）了。比如这个例子里我们用的是 **Assembly AI**，它本身就内置了 **VAD model**。所以如果我们能尽量把事情“交给同一个流程/同一个进程”（defer to one process）来做
+ 智能体时还可以考虑另一个做法：**mimic**（模拟）一个额外的步骤，用一个本地模型去理解说话者的意图（**intent**）和语气（**tone**），然后把这些信息作为上下文（**context**）传递给智能体（**agent step**）。

# 20251210 Agent Reinforcement Fine Tuning – Will Hang & Cathy Zhou, OpenAI
+ [https://www.youtube.com/watch?v=p1CmPZ2j6Lk](https://www.youtube.com/watch?v=p1CmPZ2j6Lk)
+ 让agent和model不同的地方在于：它具备与外部世界交互的能力。它可以自己完成任务，把事情独立做完，而不用每一步都通过你来操作。
+ 这些智能体并不是“瞎”调用工具，它们在调用工具的同时也在进行推理
+ 它们与外部世界的交互（比如调用工具），和它们的推理轨迹是交织在同一个上下文窗口里的。
+ 你可以把任务本身简化，也可以在任务周围加上更好的“护栏”（guard rails）。
+ Fine-tuning，就是在你的任务上对智能体做端到端训练，通过调整模型的参数权重，让它在这个任务上的表现更上一层楼。
+ **agent RFT** 会根据你自己定义的学习信号来调整模型权重，从而教会模型：什么是好的行为，什么是糟糕的行为。在训练过程中，智能体会尝试很多种不同的工具调用方式，来完成你的任务。
+ **agent RFT** 的好处：它能帮助你提升推理类模型的表现，尤其是那些需要多步调用工具、与外部世界反复交互来完成任务的模型。
+ **agent RF** 在样本效率上也很高。我们见过有用户真的只用了大概 10 条示例数据，就已经取得了不错的效果，这其实挺惊人的。
+ **domain shift**领域偏移：你的环境、你的“世界”，往往和我们在内部训练模型时所用的世界不太一样。
+ **agent RFT** 可以通过这套“改变权重的训练过程”，让模型重新适应你的领域，最终得到一个真正“理解你环境”的智能体。
+ 对于每一次智能体的 rollout，我们都会生成一个唯一标识符，用来标记这一次特定的 rollout；然后，我们会把在你系统里发生的所有工具调用都和这个 UUID 关联起来。
+ 只有在你已经觉得把这些传统方法都用到了极致、从任务里已经“榨干了汁”，但还是想继续突破的时候，你才会求助于 **agent RFT**，去进一步推动你这个任务的性能边界。
+ **RFT** 在学习“并行调用工具”这件事上特别擅长。
+ 事实证明，我们并不需要特别庞大的 kernel 数据集，也能训练出一个会写 kernel 的好模型——前提是，我们要设计出一个好的奖励函数。
+ 五条关键原则：
    - 你需要有一个清晰、毫不含糊的“成功标准”。
    - 确保你的训练集和评测集尽可能“镜像”真实的线上流量
    - 你要确保在“探索”过程中，如果对同一个数据点多采样几次，模型就有机会取得更好的表现，这样它才能真正“向自己学习”
    - 尽量确保你的奖励函数是“不可被钻空子”的
    - 能让奖励更“连续”，而不是简单的“对 / 错”二元。

