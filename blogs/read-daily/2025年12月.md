# 20251212  Build a voice agent with LangChain
+ [https://www.youtube.com/watch?v=kDPzdyX76cg](https://www.youtube.com/watch?v=kDPzdyX76cg)
+ “**create agent**”是我们在发布 **LangChain 1.0** 时推出的一种新模式。
+ 它提供了一种方式，让你可以直接使用一个“预先封装好的、支持工具调用的智能体”。
+ 在正常人类对话里，你说完话后，对方通常大约在 250～750 毫秒内就会接话回应。
+ **Time to first bite**：当用户（或智能体）说完话之后，要过多久你才能在耳朵里听到返回的音频？
+ **transcription accuracy**（转写准确率）：当我在说话时，我的话被转成文字并传给智能体时，到底准不准？
+ **turn detection**（轮次/轮替检测）：我什么时候算“说完了”，智能体应该在何时开始回答？
+ 实时模型在历史上往往一直会慢半拍（lagging behind）。
+ 意图识别（intent detection）并不是“开箱即用的标准能力”。而实时模型的一个新颖之处在于：它能直接从音频流（audio stream）里理解语气（tone）和意图（intent），并用这种信息去影响自己的
+ 有些 **speech to text providers** 已经把 **VAD** 内置（built in）了。比如这个例子里我们用的是 **Assembly AI**，它本身就内置了 **VAD model**。所以如果我们能尽量把事情“交给同一个流程/同一个进程”（defer to one process）来做
+ 智能体时还可以考虑另一个做法：**mimic**（模拟）一个额外的步骤，用一个本地模型去理解说话者的意图（**intent**）和语气（**tone**），然后把这些信息作为上下文（**context**）传递给智能体（**agent step**）。

# 20251210 Agent Reinforcement Fine Tuning – Will Hang & Cathy Zhou, OpenAI
+ [https://www.youtube.com/watch?v=p1CmPZ2j6Lk](https://www.youtube.com/watch?v=p1CmPZ2j6Lk)
+ 让agent和model不同的地方在于：它具备与外部世界交互的能力。它可以自己完成任务，把事情独立做完，而不用每一步都通过你来操作。
+ 这些智能体并不是“瞎”调用工具，它们在调用工具的同时也在进行推理
+ 它们与外部世界的交互（比如调用工具），和它们的推理轨迹是交织在同一个上下文窗口里的。
+ 你可以把任务本身简化，也可以在任务周围加上更好的“护栏”（guard rails）。
+ Fine-tuning，就是在你的任务上对智能体做端到端训练，通过调整模型的参数权重，让它在这个任务上的表现更上一层楼。
+ **agent RFT** 会根据你自己定义的学习信号来调整模型权重，从而教会模型：什么是好的行为，什么是糟糕的行为。在训练过程中，智能体会尝试很多种不同的工具调用方式，来完成你的任务。
+ **agent RFT** 的好处：它能帮助你提升推理类模型的表现，尤其是那些需要多步调用工具、与外部世界反复交互来完成任务的模型。
+ **agent RF** 在样本效率上也很高。我们见过有用户真的只用了大概 10 条示例数据，就已经取得了不错的效果，这其实挺惊人的。
+ **domain shift**领域偏移：你的环境、你的“世界”，往往和我们在内部训练模型时所用的世界不太一样。
+ **agent RFT** 可以通过这套“改变权重的训练过程”，让模型重新适应你的领域，最终得到一个真正“理解你环境”的智能体。
+ 对于每一次智能体的 rollout，我们都会生成一个唯一标识符，用来标记这一次特定的 rollout；然后，我们会把在你系统里发生的所有工具调用都和这个 UUID 关联起来。
+ 只有在你已经觉得把这些传统方法都用到了极致、从任务里已经“榨干了汁”，但还是想继续突破的时候，你才会求助于 **agent RFT**，去进一步推动你这个任务的性能边界。
+ **RFT** 在学习“并行调用工具”这件事上特别擅长。
+ 事实证明，我们并不需要特别庞大的 kernel 数据集，也能训练出一个会写 kernel 的好模型——前提是，我们要设计出一个好的奖励函数。
+ 五条关键原则：
    - 你需要有一个清晰、毫不含糊的“成功标准”。
    - 确保你的训练集和评测集尽可能“镜像”真实的线上流量
    - 你要确保在“探索”过程中，如果对同一个数据点多采样几次，模型就有机会取得更好的表现，这样它才能真正“向自己学习”
    - 尽量确保你的奖励函数是“不可被钻空子”的
    - 能让奖励更“连续”，而不是简单的“对 / 错”二元。

