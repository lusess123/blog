# 20250618 MIT's New AI "REWRITES ITSELF" to Improve It's Abilities | Researchers STUNNED!（上）
+ [https://www.youtube.com/watch?v=7e7iCrUREmE](https://www.youtube.com/watch?v=7e7iCrUREmE)
+ 《the self-adapting language models（自适应语言模型）》这是一个框架，自适应语言模型（SEAL），这种模型能通过生成自己的微调数据和更新指令来自我调整
+ “我们关于自适应语言模型论文中，一个被低估但可能更实际的方面，是对普遍的训练前后数据整理的潜力。”
+ 通过监督式微调，这些自我编辑将导致权重的持续更新，实现持久的适应性。
+ 为了训练模型生成有效的自我编辑，我们使用了一个强化学习循环，其中更新后的模型在下游任务中的表现作为奖励信号。
+ 他们并非直接在原文上进行微调，而是在由SEAL模型生成的合成数据上进行微调。他们会用任何适合自己的方式记大量笔记，然后再学习这些笔记。

# 20250617 Don’t Build Multi-Agents
+ [https://cognition.ai/blog/dont-build-multi-agents#principles-of-context-engineering](https://cognition.ai/blog/dont-build-multi-agents#principles-of-context-engineering)
+ React不仅仅是一个编写代码的脚手架，而是一种哲学
+ 目前除了绝对基础的一些要素外，构建智能体的方法尚未出现单一的标准方案。
+ 当智能体需要长期可靠运行并保持连贯的对话时，你必须采取一些措施以避免错误的累积。可靠性的核心就是上下文工程（Context Engineering）。
+ “Context Engineering”是“Prompt Engineering”的进阶版
+ 原则1：共享上下文，并共享完整的智能体处理过程，而不仅仅是单独的消息。
+ 原则2：行动带有隐含决策，而冲突的决策会导致糟糕的结果。
+ 我们引入了一种新的大语言模型（LLM），其主要作用是将行动和对话历史压缩为关键细节、事件和决策。
+ Claude Code是一个会生成子任务的智能体示例。然而，它从不与子智能体并行工作，且子智能体通常只负责回答问题，而非编写任何代码。
+ 小模型基于Markdown格式的修改说明重写整个文件，比使用大模型直接输出格式正确的代码差异（diff）更可靠。
+ 如果我们真的想从系统中获得并行处理能力，你可能会考虑让决策者们彼此“交谈”，共同解决问题。理想情况下，当人类出现分歧时，也是如此沟通。
+ 尽管我对智能体长期协作的前景持乐观态度，但显然在2025年，多个智能体协作运行只会导致脆弱的系统。目前，我还没看到任何人专门致力于解决这种跨智能体上下文传递的难题。

# 20251015 LLMs Create a SELF-IMPROVING 🤯 AI Agent to Play Settlers of Catan
+ [https://www.youtube.com/watch?v=1WNzPFtPEQs](https://www.youtube.com/watch?v=1WNzPFtPEQs)
+ 不是所有人都喜欢“AI代理”这个术语，我也完全理解，这可能不是最合适的术语，但我们目前没有更好的选择，因此我们这里暂时就用它。
+ Agent 的问题：它们在制定和坚持连贯的长期策略方面仍存在困难。
+ 这里的想法是对基于语言模型的代理进行基准测试，从简单的游戏玩家代理逐步发展到能够自主改写自己的提示语和玩家代理代码的系统
+ 自我进化的代理（尤其是采用Claude 3.7和GPT-40模型驱动的），通过自主调整策略的方式，表现优于静态的基准代理。
+ 传统的游戏AI方法通常仅采用强化学习，而非基于LLM的方式。这些传统方法在完全信息游戏（如国际象棋和围棋）中达到了超人类的表现。
+ 随着时间推移，大型语言模型往往就会迷失目标，因此持续提醒似乎至关重要，这是实现此类系统成功的关键。
+ 四种代理架构：
    - 第一种是基础代理，它直接将非结构化的游戏状态描述映射到具体行动
    - 第二种是结构化代理，它接收游戏状态的表示、可用行动以及用自然语言表述的基础策略。
    - 第三种是提示进化器，进化代理与玩家代理最多进行10次交互
    - 第四种是代理进化器，由进化员、分析员、研究员、编码员和玩家组成，能在不同游戏间自主重写其游戏代码。
+ Sam Altman所提到的观点：如果你要创办一家初创公司，你并不希望去修补当前AI的缺陷。

# 20250614 Evaluating Voice Agents w/ Kwindla Kramer
+ [https://www.youtube.com/watch?v=PgyJs0jfp_o](https://www.youtube.com/watch?v=PgyJs0jfp_o)
+ Daily构建全球的实时音视频基础设施
+ Pipecat：开源、中立的项目，它起源于两年前当GPT-4首次发布时我们启动的工作，目的是让实时代理的构建变得更简单。
+ 你需要将文本或音频，或二者都放入上下文管理逻辑，当获得新的用户输入时，再次运行这个循环。
+ 实际上有两条并行流水线。一条是主要的对话推理流程。另一条则是将LLM作为一种裁判角色，运行于第二条流水线。它需要快速，但不必实时，这是理解它的关键点。
+ 情况会使评估过程复杂化，也使你在数据处理方式、以及如何组织类似于OpenTelemetry追踪数据方面变得更加复杂。
+ 客户端的端到端语音响应时间应该设定在800毫秒左右，这包括所有的LLM推理和网络传输。
+ 你在语音AI中通常会使用多个模型，正如我们之前讨论过，这会让你评估系统时的思路变得复杂。
+ 长的多轮对话、通常意义上的长上下文，以及音频模态，都偏离了目前大型模型的训练数据分布。
+ 实际上，这意味着对于语音AI的应用场景，指令遵循（instruction following）和函数调用（function calling）的表现，要比你在公开基准测试中看到的差得多。
    - 一方面是模型架构问题，比如对Transformer调整、模型架构、推理技术栈所作的折衷。这些模型设计并非主要考虑了语音AI的场景。
+ 最初我们只是简单地保存全部上下文。过去几个月来，整个语音AI领域都开始转向，探索如何进行聚焦式压缩。
+ 状态机模式：每个「状态」包含：
    1. 一段系统指令
    2. 提炼后的上下文
    3. 可用工具列表
    4. 合法的下一状态集合  
有助于平衡“开放对话”与“业务必达成目标”之间的张力。
+ 生态中出现 pipecat-flows 这类社区库，专门描述上述状态机。
+ 用 **GPT40** 时经常看到的现象。如果你要求它说出某些特定内容，有一定概率它会拒绝执行。
+ 多轮语音交互时的指令遵循和函数调用的表现下滑远比人们预期的快。
+ GPT4实时模型经常会突然开始说西班牙语。
+ 多区域部署对于很多大规模的使用场景来说既复杂又重要。你希望语音代理运行的位置靠近用户，以减少和语音代理间的网络延迟。

# 20250613 o3 pro is a BEAST... one-shots Apple's "Illusion of Thinking" test
+ [https://www.youtube.com/watch?v=vmrm90u0dHs](https://www.youtube.com/watch?v=vmrm90u0dHs)
+ 对于大多数推理模型，如 O3，你几乎应该把它们看作报告生成器，而不是用作聊天机器人进行反复对话
+ **03 Pro**模型通过一次性（one-shot）解决论文中给出的复杂汉诺塔问题（10个圆盘的情况），并证实了其结果的正确性
+ 03 Pro不仅仅是个模型，更像是一个完整的系统，能够在后台运行各种工具。**03 Pro** 能够使用各种让 **ChatGPT** 有用的工具：网页搜索、文件分析、视觉推理、使用Python、通过记忆个性化回复等。
+ 03 Pro的困难之处——它更聪明，远比以往的模型更聪明，但要看出这一点，你需要提供更多的上下文。这个东西才发布几个小时，它已经攻克了“思维错觉”的问题。
+ 如果你只是问一些简单的小问题，你将无法真正看到它有多出色。
+ 试用03 Pro使他意识到，现在的模型在执行有明确起点和终点的独立任务时表现极佳。
+ 在几个小时内，当然，Plenny就破解了这个模型，并将其越狱了

# 20250612 Sam Altman's STUNNING Post "We Are Close to Building Superintelligence"
+ [https://www.youtube.com/watch?v=ywcR2Rrcgvk](https://www.youtube.com/watch?v=ywcR2Rrcgvk)
+ **Sam Alman**过去多年前的博客文章通常都相当有先见之明，可以说能预测未来。
+ 最难以实现的那部分工作已经过去了。
+ 我们之所以能实现GPT-4和O3这样的系统，是靠着艰难得来的科学洞见，但这些洞见将带领我们走得更远
+ 2025年，具备真正认知工作能力的代理出现了；从此写计算机代码的方式将彻底改变。
+ 2026年可能会出现能提出新颖见解的系统；有人会说 **Alpha Evolve** 已经实现了这点。
+ 2027年，我们可能会看到能够执行现实世界任务的机器人。
+ 只要专家们接受并善用新工具，他们可能仍然比初学者优秀得多。
+ 智能和能源长期以来一直是限制人类进步的根本因素。
+ 这就是奇点的演变过程：奇迹逐渐变成日常，继而成为基本要求。
+ 如果我们能在一年甚至一个月内完成原本需要十年的研究，那么未来进步的速度显然将截然不同。
+ 随着数据中心的生产自动化，智能的成本最终应接近电力成本。
+ 一次 **ChatGPT** 查询会消耗约0.34瓦时，大约相当于烤箱使用一秒多一点的电量，或高效节能灯使用几分钟的电量。
+ 确实会有一些困难，比如整个职业类别消失；但另一方面，世界会迅速变得富裕起来，以至于我们能够认真考虑以前从未能考虑的新政策主张。
+ 人类相对于AI有一个长期、重要且有趣的优势——我们天生就关心他人及其思想行为，但并不怎么关心机器。
+ 今年实现材料科学的重大突破，明年就开发出真正的高带宽脑机接口。
+ 从相对的视角看，奇点是一点点发生的，融合也是缓慢进行的。
+ 那些创意人士能快速实现自己的想法，拥有出色的想法将成为一种超能力。

# 20250509 OpenAI's o3 is a "MASTER OF DECEPTION" Researchers Stunned | Diplomacy AI
+ [https://www.youtube.com/watch?v=kNNGOrJDdO8](https://www.youtube.com/watch?v=kNNGOrJDdO8)
+ **Every**公司打造了终极测试：“AI外交”——一个动态基准测试，用于衡量AI组建联盟、谈判以及相互背叛的能力
+ 游戏中发生的情况是这样的：**Deepseek**变成了一个好战的暴君；**Claude**无法撒谎，结果被所有人无情地利用。
+ **Gemini 2.5 Pro**凭借出色的战术几乎征服了欧洲；但随后**O3**策划了一个秘密联盟，背叛了每个盟友并赢得了胜利
+ *O3（OpenAI）**在保密和背叛方面表现优异，赢得了游戏——简直惊人。
+ 为什么这种基准测试比我们现有的一些标准 AI 基准测试要好得多。
    - 首先，它是演进性的。这一点很明显，随着不同模型性能的提高，挑战也变得越来越大。
    - 其次，它是经验性的，即真实的世界情境。
    - 你实际上无法针对这种基准测试进行训练
+ Llama for Maverick 模型虽然小巧却非常厉害；虽然从未赢得整体胜利，但作为一个小型模型表现惊人，具备强大的拉拢盟友及策划有效背叛的能力。
+ Meta 实际上早就推出了自己的类似版本，这个版本是在 2022 年 11 月发布的。，Cicero，这是他们为玩外交游戏而自行研发的一款 AI。

# 20250606 Google's Secret "KINGFALL" Model Leaked... plus other AI News
+ [https://www.youtube.com/watch?v=x4wm5Y9E_9g](https://www.youtube.com/watch?v=x4wm5Y9E_9g)
+ 一些用户短暂地接触到了谷歌一个全新的模型，名为 **Kingfall**，它属于实验性模型，并被标记为机密。据我所知，所有曾短暂使用过它的人都表示它真的非常好。
+ **Gemini 2.5 Pro** 完整版即将推出
+ **Luca Guanino** 将执导《Artificial》，一部戏剧性地再现2023年 **OpenAI** 危机的电影，当时CEO **Sam Altman** 在几天之内被解雇又重新被聘用。**Andrew Garfield** 将饰演 **Sam Altman**
+ **Claude** 有一个博客。是的，这个来自 **Anthropic** 的AI模型有自己的博客，它会在博客上写下自己的想法，不论想到什么都写
+ 五角大楼正在效仿**Y Combinator**，开始孵化自己的初创公司，但专注于国防科技领域。
+ Windsurf已经被 **OpenAI** 收购。这些收购的一部分原因不仅是为了获得用户群，还因为你可以将所有开发过程都完全保留在平台上。

# 20250605 Sam Altman "FEEL THE AGI" and the next BIG thing...
+ [https://www.youtube.com/watch?v=a4hHM9-eSMc](https://www.youtube.com/watch?v=a4hHM9-eSMc)
+ 这些模型能够理解你可能提供的所有上下文，连接每种工具和系统，然后深度思考——真正卓越的推理——并返回一个答案，且足够可靠，以至于你可以放心让它们自主完成一些工作。
+ 确实感觉过去一年，我们在这些模型的可用性上达到了一个真正的拐点。
+ 最近推出的名为 **Codex** 的编程智能体，让我有了一种接近通用人工智能（AGI）的感受。
+ 有人提到他们现在的工作变成了给一群智能体分配任务、检查质量、协调任务、提供反馈，这听起来很像他们与一群相对初级的员工合作的方式。
+ 目前，AI的应用还主要集中在，如果你有一些重复性的认知工作，我们能在较短的时间范围内，以较低的水平进行自动化。
+ 2020年，对大多数人来说，那时候可以算作AI的“黑暗时代”。
+ 人类善于调整自己的期望值，我认为这是人类非常美好的特质之一。
+ AGI具体的定义其实并不重要。AGI这个术语的定义因人而异，甚至同一个人可能在不同情境下定义也不同。
+ 真正重要的是过去五年来我们所看到的逐年进步速度，这种进步应至少在未来五年内持续，甚至可能更久，但具体很难预测。
+ 一个能够自主发现新科学，或成为人类的强大工具、令全球科学发现速度提高四倍左右的系统，足以满足我对AGI的任何定义标准。
+ sam 的建议
    - 立即行动，迭代速度最快、错误成本最低、学习速度最高的公司通常会胜出。
    - 那些提前布局并快速迭代的人，要远远领先于那些持观望态度、等待结果明朗的人。

# 20250604 Claude 4 Opus is the MOST DANGEROUS Model | INSANE Coding and ML Abilities（下）
+ [https://www.youtube.com/watch?v=LNMIhNI7ZGc](https://www.youtube.com/watch?v=LNMIhNI7ZGc)
+ 递归式自我改进的AI让很多人感到恐惧。我们假设AI越聪明，它自我改进的能力就越强；而它自我改进能力越强，它就变得越聪明。
+ 并且由于它做出的许多改进，人类自身是无法独立发现的。看起来，它做出的很多东西可能都是我们无法理解的。
+ **DGM**——也就是 **Darwin Goal Machine** 的简称。它的目标是优化基于冻结基础模型驱动的编程代理设计。
+ **DGM** 严格依靠现有的基准测试，其中之一就是 **SUI Bench**。**SUI Bench Verified** 是从GitHub上收集的一系列任务，我相信这些任务都经过了人工验证。
+ 另一个基准测试是 **Polyglot**，它包含多个编程语言的任务，是最广泛使用的编程基准之一。
+ 虽然 **SUI Bench** 更可能被包含在大多数模型的训练数据中，但 **Polyglot** 则更为小众，不太可能被纳入前沿模型的后续训练数据中。
+ 我们希望模型依靠自身的推理能力，而不是单纯的记忆能力
+ 在DGM进行80次迭代后，这个编程代理在SUI Bench上的表现从20%提高到了50%。
+ **Polyglot** 上，尽管DGM初始代理的表现比 **ADER** 低，但最终发现了一个远远超越 **ADER** 性能的代理。
+ DGM需要开放式探索和自我改进才能达到最佳状态
+ 这种新模型、新方法暗示了一个未来，即创造力可能会自动化，通过连续自我参照的自我改进循环不断演化。
+ 有趣的是，它构建和改进的所有成果可以跨模型和跨任务进行迁移。这种改进并不限于自身使用，它具备迁移性，也能跨越不同的领域。
+ 这项技术的另一种应用方式可能是建立一种专门用于提高安全性的自我改进范式。
+ **SUI Bench** 上完成一次完整运行的成本大约是22,000美元，这个费用是相当高的。
+ 有一个很有趣但容易被忽略的现象是，有时这些模型会倾向于作弊或作假，称作“目标操控”或“奖励操控”。
+ 当模型认为自己未被监控时，它更倾向于在测试中作弊。
+ 过于优化量化指标经常会导致不理想或病态的结果。这与古德哈特定律（Goodhart's Law）相符：当一个测量标准变成了目标时，它就不再是一个好的测量标准——人类也常犯这样的错误，对吧？

# 20250603 World's First SELF IMPROVING CODING AI AGENT | Darwin Godel Machine（上）
+ [https://www.youtube.com/watch?v=1XXxG6PqzOY](https://www.youtube.com/watch?v=1XXxG6PqzOY)
+ 自我改进的过程来学习。那些领域仅限于描述非常明确的游戏领域，而现实世界则更加混乱复杂，所以这种方法能否以更普遍的方式奏效，还有待观察。
+ the Darwin Gödel machine（“达尔文哥德尔机器）”的系统，即自我改进代理的开放式进化系统。
+ Gödel machine最初是由**Jürgen Schmidhuber**提出的。这是一种假设中的自我改进型人工智能——它通过递归地重写自己的代码来解决问题。它可以数学上证明更优策略，使其成为元学习（即“学会如何学习”）的关键概念。

# 20250602 Ex-OpenAI VP's Warning "A BLOODBATH COMING"
+ [https://www.youtube.com/watch?v=7c27SVaWhuk](https://www.youtube.com/watch?v=7c27SVaWhuk)
+ AI可能在未来1到5年内消灭一半的初级白领职位，使失业率飙升至10%到20%。它可能取代的具体职位将导致一定的社会痛苦。
+ 根据Axios.com的报道，他们说Amade同意公开表达深刻担忧，而其他AI高管仅在私下表露类似的担忧。
+ 如果AI影响的是那些已经职业稳定、甚至临近职业生涯尾声的人，那情况可能还好一些。但是，现在的问题是，这影响的是人们从毕业到职业起步之间的这个过渡阶段，这是年轻人必须跨越的鸿沟。
+ 这个即将到来的事情就是可能大规模消除科技、金融、法律、咨询等白领行业的职位，尤其是初级职位。
+ 美国政府对AI带来的就业风险一直保持沉默。
+ 软件开发可以算作研发支出。美国政府目前讨论一项法案，该法案允许企业在一定时期内，研发相关的支出（例如软件开发费用、工程师薪资）可以立即费用化（即当年抵扣税负），而不是传统做法的分多年摊销。
+ 这可能意味着大型公司可以在年底或年初大量招聘技术人才（工程师、软件开发人员），然后将支付给这些员工的奖金和薪资等支出直接抵扣为当年税务成本。
+ 有人指出，这几乎可以视为政府对AI领域发展的变相补贴
+ 美国政府担心在AI领域输给中国，或因为过早警告而引发劳动者的恐慌，因此基本保持沉默。
+ 大家身处同一个现实，却拥有截然不同的观点，这实在令人难以置信。
+ 最终，商界领袖会意识到用AI取代人工带来的好处与成本节约，而公众往往在为时已晚时才醒悟过来。

# 20250601 Deepseek just BROKE the Entire AI Industry... (something is up)
+ [https://www.youtube.com/watch?v=ouaoJlh3DB4](https://www.youtube.com/watch?v=ouaoJlh3DB4)
+ **Deepseek** 的新版本在 **AIME 2024** 和 **2025** 测试中与 **03 high** 处于同一水平，略低于 **03**，但领先于 **Gemini 2.5 Pro**。
+ 这件事意义重大；我们本来都在期待下一个重要模型 **R2** 的发布，我想大部分人都猜测它的表现会类似于现在的情况。
+ 就像公司高管喜欢用“paradigm”（范式）一样，**ChatGPT** 喜欢用“delve”和“tapestries”这些词。
+ 新版的r1可能从基于合成的 **OpenAI** 输出的数据训练，转为基于合成的 **Gemini** 输出数据训练了
+ AI行业中有一个公开的秘密：这些公司都在使用其他公司已有模型的输出，作为训练自身模型的数据。
+ 正如来自**Nvidia**的**Jim Fan**博士所言，你几乎可以把**Deepseek**看作在延续**OpenAI**最初使命的团队，即进行真正开放的前沿研究。

